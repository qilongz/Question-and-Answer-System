{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math,numpy,json,re,nltk,csv,time,re,os.path,sys,ast,itertools\n",
    "# from gensim import corpora\n",
    "from operator import add\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist, DictionaryProbDist\n",
    "\n",
    "#from sklearn import metrics\n",
    "#from math import sqrt\n",
    "#from collections import OrderedDict\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "import math,numpy,json,re,nltk\n",
    "import time,re,os.path,sys\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import defaultdict\n",
    "from numpy import multiply\n",
    "from math import sqrt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os.path as path\n",
    "from collections import OrderedDict\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk import FreqDist, DictionaryProbDist\n",
    "\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "import string\n",
    "path_to_jar = 'data/stanford-parser.jar'\n",
    "path_to_models_jar = 'data/stanford-parser-3.7.0-models.jar'\n",
    "st=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful \n",
      "There are totally 40 documents in this dataset\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dataset = []\n",
    "train_path = os.path.abspath('data/QA_dev.json')\n",
    "\n",
    "dataset = []\n",
    "with open(train_path) as f:\n",
    "    for line in f:\n",
    "        dataset+=(json.loads(line))\n",
    "print \"Import Successful \"\n",
    "print \"There are totally\", len(dataset),'documents in this dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build tf-idf model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    terms = set()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+(?:[-./]\\w+)?')\n",
    "    for token in tokenizer.tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return list(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tf_idf_Model:\n",
    "    def __init__(self, collection):\n",
    "        self.vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, use_idf=True,norm='l2',stop_words=None, tokenizer=my_tokenizer)\n",
    "        self.collection_matrix = self.vectorizer.fit_transform(document_collections)\n",
    "        feature_array = self.vectorizer.get_feature_names()\n",
    "        self.features = dict()\n",
    "        for index in range(len(feature_array)):\n",
    "            term = feature_array[index]\n",
    "            self.features[term] = index\n",
    "\n",
    "    def predict(self, queies,limit=3):\n",
    "        predictions = [self.inverted_index_score(i,limit) for i in  queies]\n",
    "        return predictions\n",
    "\n",
    "    def inverted_index_score(self, query_sent,limit=1):\n",
    "        \"\"\"\n",
    "        now we implement inverted index to handle query\n",
    "        \n",
    "        :param query_sent: \n",
    "        :return: \n",
    "        \n",
    "        \"\"\"\n",
    "        query_words = my_tokenizer(query_sent)\n",
    "        score = defaultdict(float)\n",
    "\n",
    "        for w in query_words:\n",
    "            try:\n",
    "                col_i = self.features[w]\n",
    "                inverted_ix = self.collection_matrix[:, col_i]\n",
    "                for di in range(inverted_ix.shape[0]):\n",
    "                    score[di] += inverted_ix[di, 0]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        index_score = sorted(score.items(), key=lambda (k, v): v, reverse=True)\n",
    "\n",
    "        if index_score:\n",
    "            doc_indexs = [i[0] for i in index_score[:limit]]\n",
    "            return doc_indexs\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25_Model(object): \n",
    "    def __init__(self, document_collection, k1=1.5, b=0.75, k3=1.0, EPS=0.25, tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.document_collection_length = len(document_collection) \n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length \n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection] \n",
    "        self.tf = [] \n",
    "        self.df = defaultdict(int) \n",
    "        self.bm25_idf = defaultdict(float) \n",
    "        self.average_idf = -1 \n",
    "        self.k1 = k1 \n",
    "        self.k3 = k3 \n",
    "        self.EPSILON = EPS \n",
    "        self.b = b \n",
    "        self.inverted_index = defaultdict(list) \n",
    "        self.initialize() \n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection): \n",
    "            doc_term_freq = FreqDist(document) \n",
    "            self.tf.append(doc_term_freq) \n",
    "            \n",
    "            for word, freq in doc_term_freq.items(): \n",
    "                self.df[word] += 1 \n",
    "                self.inverted_index[word].append(index) \n",
    "        for word, freq in self.df.items(): \n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5)  \n",
    "            self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys()) \n",
    "    \n",
    "    def predict(self, queryX, limit=1): \n",
    "        q_prediction = [] \n",
    "        for query in queryX: \n",
    "            answers = self.bm25_get_most_relevant(query)[:limit] \n",
    "            if len(answers) == 0:\n",
    "                q_prediction.append([]) \n",
    "            else:\n",
    "                q_prediction.append([i[0] for i in answers]) \n",
    "        return q_prediction \n",
    "\n",
    "    def bm25_get_most_relevant(self, query): \n",
    "        query_tks = self.tokenizer(query) \n",
    "        scores = defaultdict(float) \n",
    "        for q_token in query_tks: \n",
    "            for doc_index in self.inverted_index[q_token]: \n",
    "                idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf \n",
    "                top = self.tf[doc_index][q_token] * (self.k1 + 1) \n",
    "                below = self.tf[doc_index][q_token] + self.k1 * (1 - self.b + self.b * self.document_collection_length / self.avg_doc_length) \n",
    "                scores[doc_index] += idf * top / below \n",
    "        prels = scores.items() \n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True) \n",
    "        return sorted_socres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LM_Model:\n",
    "    def __init__(self,documents,a = 0.5 ,tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer  \n",
    "        self.alpha = a\n",
    "        self.document_collection = [self.tokenizer(doc) for doc in documents] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_term_prob = {}\n",
    "        self.corpus_term_freq = FreqDist(self.document_corpus)\n",
    "        self.vocabulary = self.corpus_term_freq.keys()\n",
    "        self.lmp = []\n",
    "        self.initialize()\n",
    "    def initialize(self): \n",
    "        document_freq = [FreqDist(doc) for doc in self.document_collection]\n",
    "        length_corpus = len(self.document_corpus)\n",
    "        for term,occurs in self.corpus_term_freq.items():\n",
    "            self.corpus_term_prob[term] = float(occurs)/float(length_corpus)\n",
    "        for sent_freq in document_freq:\n",
    "            tempDict = {}\n",
    "            for term in self.vocabulary:\n",
    "                upper = sent_freq.get(term,0) + self.alpha*self.corpus_term_prob.get(term,0)\n",
    "                below = self.corpus_term_freq.get(term,0) + self.alpha\n",
    "                tempDict[term] = float(upper)/float(below)\n",
    "            self.lmp.append(tempDict)\n",
    "    def get_lm_socres(self,Query):\n",
    "        doc_socres = []\n",
    "        Query = my_tokenizer(Query)\n",
    "        for doc_prob in  self.lmp:\n",
    "            term_score = []\n",
    "            for term in Query:\n",
    "                if term in self.vocabulary:\n",
    "                    term_score.append(doc_prob[term])\n",
    "            query_score = numpy.product(term_score)\n",
    "            doc_socres.append(query_score)\n",
    "        sorted_score = sorted(list(enumerate(doc_socres)), key=lambda (k,v): v, reverse=True)\n",
    "        doc_indexs = [i for i in sorted_score]\n",
    "        return doc_indexs\n",
    "    def predict(self,questions,limit = 3):\n",
    "        predictions = [] \n",
    "        for query in questions: \n",
    "            answers = self.get_lm_socres(query)[:limit] \n",
    "            predictions.append([i[0] for i in answers]) \n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Various Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_csv(csv_name,dataset,limit=1):\n",
    "    csv_file = open(csv_name, mode='w',)\n",
    "    fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, )\n",
    "    writer.writeheader()\n",
    "\n",
    "    ddi = 0\n",
    "    for document in dataset:\n",
    "        evaluate_row = []\n",
    "        questions = [i['question'] for i in document['qa']]\n",
    "        model = document['model']\n",
    "        predictions = model.predict(questions,limit)\n",
    "        quest_index = 0\n",
    "        for pred_index in predictions:\n",
    "            drow = dict()\n",
    "            drow['question_ID'] = quest_index\n",
    "            drow['prediction_ID'] = pred_index\n",
    "            evaluate_row.append(drow)\n",
    "            quest_index += 1\n",
    "        doc_sents = document['sentences']\n",
    "        for r in evaluate_row:\n",
    "            r['document_ID'] = ddi\n",
    "            r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "            if len(r['prediction_ID']) != 0:\n",
    "                r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "            else:\n",
    "                print 'error prediction',ddi,r['question_ID'],r['question']\n",
    "            writer.writerow(r)\n",
    "        ddi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 25 245 When was Chanakya alive?\n"
     ]
    }
   ],
   "source": [
    "#build model for each document collaction\n",
    "# for document in dataset:\n",
    "#     document_collections = document['sentences']\n",
    "#     document['model'] = tf_idf_Model(document_collections)\n",
    "# write_csv('data/tf_idf_dev_results.csv',dataset,1)\n",
    "\n",
    "for document in dataset:\n",
    "    document_collections_sents = document['sentences']\n",
    "    document['model'] = BM25_Model(document_collections_sents)\n",
    "write_csv('data/bm25_dev_predictions.csv',dataset,1)\n",
    "\n",
    "# for document in dataset:\n",
    "#     document_collections_sents = document['sentences']\n",
    "#     document['model'] = LM_Model(document_collections_sents)\n",
    "# write_csv('data/lm_dev_predictions.csv',dataset,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import different result and compare predictions from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def give_prediction_values(filename,devdata = False):\n",
    "    preds = []\n",
    "    with open(filename) as csvfile:\n",
    "        readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            tempDict = {}\n",
    "            tempDict['DocID'] = int(row['document_ID'])\n",
    "            tempDict['Predictions'] = ast.literal_eval(row['prediction_ID'])\n",
    "            tempDict['QuestionIndex'] = int(row['question_ID'])\n",
    "            tempDict['Question'] = row['question']\n",
    "            preds.append(tempDict)\n",
    "    count = 0\n",
    "    bingo = 0\n",
    "    if devdata:\n",
    "        for d in preds:\n",
    "            guess = d['Predictions'] \n",
    "            doc_i = d['DocID']\n",
    "            qus_i = d['QuestionIndex']\n",
    "            act_i = dataset[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "            if act_i in guess:\n",
    "                bingo += 1\n",
    "            count += 1\n",
    "        print \"correctness results :\",float(bingo)/float(count)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness results : 0.651305683564\n"
     ]
    }
   ],
   "source": [
    "p = give_prediction_values('data/bm25_dev_predictions.csv',devdata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_load_byteified(file_handle):\n",
    "    return _byteify(json.load(file_handle, object_hook=_byteify),ignore_dicts=True)\n",
    "\n",
    "def _byteify(data, ignore_dicts = False):\n",
    "    # if this is a unicode string, return its string representation\n",
    "    if isinstance(data, unicode):\n",
    "        return data.encode('utf-8')\n",
    "    # if this is a list of values, return list of byteified values\n",
    "    if isinstance(data, list):\n",
    "        return [ _byteify(item, ignore_dicts=True) for item in data ]\n",
    "    # if this is a dictionary, return dictionary of byteified keys and values\n",
    "    # but only if we haven't already byteified it\n",
    "    if isinstance(data, dict) and not ignore_dicts:\n",
    "        return {\n",
    "            _byteify(key, ignore_dicts=True): _byteify(value, ignore_dicts=True)\n",
    "            for key, value in data.iteritems()\n",
    "        }\n",
    "    # if it's anything else, return it in its original form\n",
    "    return data\n",
    "\n",
    "with open(\"data/QA_test.json\") as json_file:\n",
    "    json_data = json_load_byteified(json_file)\n",
    "print 'import success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "st = StanfordNERTagger(cwd+'\\data\\english.all.3class.distsim.crf.ser.gz',cwd+'\\data\\stanford-ner.jar')\n",
    "\n",
    "if not os.path.isfile(\"NERtest.json\"):    \n",
    "    start = time.time()\n",
    "    progressT = len(json_data)    \n",
    "    listOfDocument=[]\n",
    "    i=0\n",
    "    for jd in json_data:\n",
    "        aList=[]        \n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))\n",
    "        #remove the below file if running on test set\n",
    "        #aList.extend([st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['answer']))) for x in jd['qa']])])\n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "        listOfDocument.append(aList)\n",
    "        i+=1\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"%d%%\" % (i*100/progressT))\n",
    "        sys.stdout.flush()    \n",
    "    for document in range(0,len(listOfDocument)):\n",
    "        #change [2] to [1] if test set\n",
    "        for sentence in range(0,len(listOfDocument[document][1])):\n",
    "            for word in range(0,len(listOfDocument[document][1][sentence])):   \n",
    "                listOfDocument[document][1][sentence][word]= (listOfDocument[document][1][sentence][word][0],listOfDocument[document][1][sentence][word][1] if not listOfDocument[document][1][sentence][word][0].isdigit() else u'NUMBER')\n",
    "    with open('NERtest.json', 'w') as outfile:\n",
    "        json.dump(listOfDocument, outfile)\n",
    "    end = time.time()\n",
    "    print '\\nTime spending:',end - start    \n",
    "else:    \n",
    "    print 'there is a file'\n",
    "with open(\"NERtest.json\") as json_file:\n",
    "        json_dataNER = json_load_byteified(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design answer rank system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class answer_rank():\n",
    "    def __init__(self,NER,POS,QA):\n",
    "        self.NER = NER\n",
    "        self.POS = POS\n",
    "        self.QA = QA\n",
    "        self.json_data = None\n",
    "        self.json_dataOrg = None\n",
    "        self.json_dataPOS = None\n",
    "        self.json_dataPOSOrg = None\n",
    "        self.dictDoc = {}\n",
    "        self.dictDocOrg = {}\n",
    "        self.initialize()\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        with open(self.NER) as json_file:\n",
    "            self.json_data = self.json_load_byteified(json_file)    \n",
    "        with open(self.POS) as json_file:\n",
    "            self.json_dataPOS = self.json_load_byteified(json_file)\n",
    "        with open(self.QA) as json_file:\n",
    "            self.json_dataOrg = self.json_load_byteified(json_file)\n",
    "        with open(self.POS) as json_file:\n",
    "            self.json_dataPOSOrg = self.json_load_byteified(json_file)\n",
    "        for document in range(len(self.json_data)):\n",
    "            for thing in range(len(self.json_data[document])):\n",
    "                for sentence in range(len(self.json_data[document][thing])):\n",
    "                    for word in range(len(self.json_data[document][thing][sentence])):\n",
    "                        self.json_data[document][thing][sentence][word][1] = 'U'\\\n",
    "                        if word!=0 and self.json_data[document][thing][sentence][word][0][0].isupper()\\\n",
    "                        and self.json_data[document][thing][sentence][word][1]=='O'\\\n",
    "                        else self.json_data[document][thing][sentence][word][1]\n",
    "                        \n",
    "                        self.json_data[document][thing][sentence][word][0] = self.json_data[document][thing][sentence][word][0].lower()\n",
    "        \n",
    "                        if self.json_data[document][thing][sentence][word][0] \\\n",
    "                        in ['one','two','three','four','five','six','seven','eight','nine','ten','zero']\\\n",
    "                        or self.isfloat(self.json_data[document][thing][sentence][word][0]):\n",
    "                            self.json_data[document][thing][sentence][word][1] = 'NUMBER'\n",
    "        for document in range(len(self.json_dataPOS)):\n",
    "            for thing in range(len(self.json_dataPOS[document])):\n",
    "                for sentence in range(len(self.json_dataPOS[document][thing])):\n",
    "                    for word in range(len(self.json_dataPOS[document][thing][sentence])):\n",
    "                        self.json_dataPOS[document][thing][sentence][word][0] = self.json_dataPOS[document][thing][sentence][word][0].lower()\n",
    "                        if self.have_number(self.json_dataPOS[document][thing][sentence][word][0]):\n",
    "                            self.json_dataPOS[document][thing][sentence][word][1] = 'CD'\n",
    "        print 'NER json file import successful'\n",
    "        \n",
    "        \n",
    "    def json_load_byteified(self,file_handle):\n",
    "        return self._byteify(json.load(file_handle, object_hook=self._byteify),ignore_dicts=True)\n",
    "\n",
    "    def _byteify(self,data, ignore_dicts = False):\n",
    "        # if this is a unicode string, return its string representation\n",
    "        if isinstance(data, unicode):\n",
    "            return data.encode('utf-8')\n",
    "        # if this is a list of values, return list of byteified values\n",
    "        if isinstance(data, list):\n",
    "            return [self._byteify(item, ignore_dicts=True) for item in data ]\n",
    "        # if this is a dictionary, return dictionary of byteified keys and values\n",
    "        # but only if we haven't already byteified it\n",
    "        if isinstance(data, dict) and not ignore_dicts:\n",
    "            return {self._byteify(key, ignore_dicts=True): self._byteify(value, ignore_dicts=True)\n",
    "                    for key, value in data.iteritems()}\n",
    "        # if it's anything else, return it in its original form\n",
    "        return data\n",
    "    \n",
    "    def have_number(self,s):\n",
    "        return any(i.isdigit() for i in s)\n",
    "\n",
    "    def isfloat(self,value):\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def readCSV(self,filename,l):\n",
    "        csv_file = filename\n",
    "        i=0\n",
    "        answerSecondFilter = []\n",
    "        dictDocOrg={}\n",
    "        #question={}\n",
    "        with open(csv_file, 'rb') as csvfile:\n",
    "            readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "            for row in readCSV:\n",
    "                document_i = int(row['document_ID'])\n",
    "                question_i = int(row['question_ID'])\n",
    "                filteredlistOfNERSentence = []\n",
    "                question_type = []\n",
    "                predictionList = ast.literal_eval(row['prediction_ID'])\n",
    "                prediction_index = predictionList[0] if len(predictionList)!=0 else 1\n",
    "                try:\n",
    "                    question_type = self.detectQuestion(document_i,question_i,prediction_index,l) \n",
    "\n",
    "                except: \n",
    "                    print \"Question type error for\",document_i,question_i,prediction_index\n",
    "                #question[document_i,question_i] = row['question']        \n",
    "                self.dictDoc[document_i,question_i]=prediction_index,question_type                                              \n",
    "\n",
    "\n",
    "    \n",
    "    def detectQuestion(self,d_i, q_i, p_i, l):\n",
    "        openclassword=[]\n",
    "        kindOfAnswer = []\n",
    "        questionPOS = self.json_dataPOS[d_i][0][q_i]\n",
    "        questionNER = self.json_data[d_i][0][q_i]        \n",
    "        answerPOS = self.json_dataPOS[d_i][l][p_i]\n",
    "        answerNER = self.json_data[d_i][l][p_i]\n",
    "        specialcommand=[]\n",
    "        #print questionNER, answerPOS\n",
    "        originalWithout = [x[0] for x in questionPOS]\n",
    "        originalWithoutA = [x[0] for x in answerPOS]\n",
    "        getIndexOfWH = [y for y,x in enumerate(questionPOS) if 'W' in x[1]]\n",
    "        #if wh\n",
    "        if len(getIndexOfWH) != 0:\n",
    "            getPOS = ''.join([x[1][0] for x in questionPOS])        \n",
    "\n",
    "            #not at last word\n",
    "            if getIndexOfWH[0]+1!=len(questionPOS):            \n",
    "                searchWordAfterWh1 = re.search('W(.*?)V', getPOS, re.IGNORECASE)\n",
    "                if searchWordAfterWh1:\n",
    "                    if len(searchWordAfterWh1.group(1))!=0:\n",
    "                        openclassword.append([searchWordAfterWh1.start()+len(searchWordAfterWh1.group(1))])\n",
    "                    else:\n",
    "                        openclassword.append([])\n",
    "                else:\n",
    "                    searchWordAfterWh1 = re.search('W(.*?)', getPOS, re.IGNORECASE)\n",
    "                    openclassword.append([searchWordAfterWh1.start()+len(searchWordAfterWh1.group(0))])            \n",
    "                frontPart = range(0,searchWordAfterWh1.start())\n",
    "                backPart = range(searchWordAfterWh1.start()+len(searchWordAfterWh1.group(0)),len(getPOS))                        \n",
    "                openclassword.append(frontPart+backPart)      \n",
    "                #print openclassword\n",
    "            else:\n",
    "                openclassword=[[],[y for y,x in enumerate(questionPOS)][:-1]]\n",
    "            #remove Stopwords        \n",
    "            openclassword[1] = [x for x in openclassword[1] if questionPOS[x][0] not in stopwords]\n",
    "            if len(openclassword[0])!=0:            \n",
    "                numberIndicator = ['year','length','percentage', 'many','much']            \n",
    "                for x in numberIndicator: \n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['NUMBER'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                            specialcommand.append(x)\n",
    "                personIndicator = ['name']\n",
    "                for x in personIndicator:\n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['PERSON','ORGANIZATION'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                placeIndicator = ['location','place','country','city','area']\n",
    "                for x in placeIndicator:\n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['LOCATION'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                            specialcommand=['location']\n",
    "                if len(kindOfAnswer)==0:    \n",
    "                    kindOfAnswer = ['O','U']\n",
    "            else:\n",
    "                numberIndicator = ['when']            \n",
    "                personIndicator = ['who','whom','whose','name']\n",
    "                placeIndicator = ['where','located']\n",
    "                if originalWithout[getIndexOfWH[0]] in numberIndicator:\n",
    "                    kindOfAnswer = ['NUMBER']\n",
    "                    specialcommand = ['year']\n",
    "                elif originalWithout[getIndexOfWH[0]] in personIndicator:\n",
    "                    kindOfAnswer = ['PERSON','ORGANIZATION']\n",
    "                elif originalWithout[getIndexOfWH[0]] in placeIndicator:\n",
    "                    kindOfAnswer = ['LOCATION']\n",
    "                    specialcommand=['location']\n",
    "                else:\n",
    "                    if len([y for y in openclassword[1] if questionPOS[y][0] in personIndicator])!=0:\n",
    "                        kindOfAnswer = ['PERSON','ORGANIZATION']\n",
    "                    elif len([y for y in openclassword[1] if questionPOS[y][0] in placeIndicator])!=0:\n",
    "                        kindOfAnswer = ['LOCATION']\n",
    "                        specialcommand=['location']\n",
    "                    else: \n",
    "                        kindOfAnswer = ['O','U']\n",
    "        else:\n",
    "            openclassword = [[],[x for x,y in enumerate(originalWithout) if y not in stop]]    \n",
    "            kindOfAnswer = ['O','U']\n",
    "        #determine whether it requires number entity\n",
    "        newList1=[]\n",
    "        newList2=[]\n",
    "        for x in range(len(openclassword[0])):\n",
    "            if originalWithout[openclassword[0][x]] in originalWithoutA:            \n",
    "                newList1.extend([f for f,h in enumerate(originalWithoutA) if originalWithout[openclassword[0][x]] == h])\n",
    "        for x in range(len(openclassword[1])):\n",
    "            if originalWithout[openclassword[1][x]] in originalWithoutA:\n",
    "                newList2.extend([f for f,h in enumerate(originalWithoutA) if originalWithout[openclassword[1][x]] == h])\n",
    "        openclassword[0]=newList1\n",
    "        openclassword[1]=newList2\n",
    "        return openclassword, kindOfAnswer,specialcommand   \n",
    "\n",
    "    \n",
    "    def createNP(self,answerToReturn,answerPOS,specialcommand,q,j):\n",
    "        newAnswer = answerPOS[answerToReturn[1]][0]\n",
    "        if (newAnswer.isdigit() and 'year' not in specialcommand):\n",
    "            newAnswer = \"{:,}\".format(int(answerPOS[answerToReturn[1]][0]))\n",
    "        for i in range(answerToReturn[1],0,-1):\n",
    "            if answerToReturn[1] != 0:\n",
    "                if answerPOS[i][1] =='NNP':\n",
    "                    if 'NNP' in answerPOS[i-1][1] or (('DT' in answerPOS[i-1][1] or 'IN' in answerPOS[i-1][1]) \\\n",
    "                                                      and answerPOS[i-1][0] !='at' and 'location' in specialcommand):\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NN':\n",
    "                    if 'JJ' in answerPOS[i-1][1] or 'DT' in answerPOS[i-1][1] or 'CD' in answerPOS[i-1][1] or answerPOS[i-1][1] =='NN':\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='JJ':\n",
    "                    if answerPOS[i-1][1] =='JJ':\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NNS':\n",
    "                    if 'JJ' in answerPOS[i-1][1]:\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif 'DT' in answerPOS[i][1] and 'location' not in specialcommand:\n",
    "                    if 'TO' in answerPOS[i-1][1]:\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if answerPOS[i][1] !='CD' and answerPOS[i][1] !='RB':\n",
    "                        if 'NN' in answerPOS[i-1][1] or 'JJ' in answerPOS[i-1][1] or 'RB' in answerPOS[i-1][1]:\n",
    "                            newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        for i in range(answerToReturn[1],len(answerPOS)-1):\n",
    "            if answerToReturn[1] != len(answerPOS)-1:\n",
    "                if answerPOS[i][1] =='NNP':                \n",
    "                    if 'NNP' in answerPOS[i+1][1] or 'CC' in answerPOS[i+1][1] \\\n",
    "                    or 'IN' in answerPOS[i+1][1] or 'TO' in answerPOS[i+1][1] \\\n",
    "                    or ('NN' in answerPOS[i+1][1] and 'location' in specialcommand):                                        \n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='CC':\n",
    "                    if 'NNP' in answerPOS[i+1][1] or 'NN' == answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='CD':\n",
    "                    if 'CD' in answerPOS[i+1][1]:                 \n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='TO' or answerPOS[i][1] =='DT':\n",
    "                    if 'NN' in answerPOS[i+1][1] or 'RB' in answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break \n",
    "                elif answerPOS[i][1] =='IN' and 'location' not in specialcommand:\n",
    "                    if answerPOS[i+1][1] =='DT' or answerPOS[i+1][1] =='NNP' or answerPOS[i+1][1] =='TO' or answerPOS[i+1][1] =='CD':\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='JJ':\n",
    "                    if 'NNS' in answerPOS[i+1][1] or 'NN' == answerPOS[i+1][1] or answerPOS[i+1][1] =='JJ':\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NN':\n",
    "                    if 'NN' in answerPOS[i+1][1] or 'JJ' in answerPOS[i+1][1] or 'IN' in answerPOS[i+1][1] or 'CC' in answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NNS':\n",
    "                    if 'IN' in answerPOS[i+1][1] and 'ORG' in specialcommand:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if newAnswer.split()[-1]=='along':\n",
    "            newAnswer=newAnswer[:-6]\n",
    "        if newAnswer.split()[-1]=='and':\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if newAnswer.split()[-1]=='for':\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if newAnswer.split()[-1]=='but':\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if newAnswer.split()[-1]=='in':\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if newAnswer.split()[-1]=='as':\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if newAnswer.split()[-1]=='at':\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if newAnswer.split()[-1]=='on':\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if newAnswer.split()[-1]=='to':\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if newAnswer.split()[-1]=='while':\n",
    "            newAnswer=newAnswer[:-6]\n",
    "        if newAnswer.split()[-1]=='despite':\n",
    "            newAnswer=newAnswer[:-8]\n",
    "        if newAnswer.split()[-1]=='because':\n",
    "            newAnswer=newAnswer[:-8]\n",
    "        if newAnswer.split()[-1].isdigit() and len(newAnswer.split()[-1])==4 and 'year' in specialcommand:\n",
    "            newAnswer=newAnswer.split()[-1]\n",
    "        if newAnswer.split()[-1].isdigit() and 'location' in specialcommand:\n",
    "            newAnswer=newAnswer.split()[-1]\n",
    "        if len(specialcommand)!=0 and 'percentage' in specialcommand[0]:\n",
    "            newAnswer+='%'\n",
    "        if 'location' in specialcommand:\n",
    "            bufferNewAnswer=[]        \n",
    "            for x in newAnswer.split():\n",
    "                for y in self.json_dataOrg[q]['sentences'][j].split():\n",
    "                    if x in y:\n",
    "                        bufferNewAnswer.append(y)\n",
    "                        break\n",
    "            newAnswer=' '.join(bufferNewAnswer)\n",
    "            newAnswer = newAnswer[:-1] if not (newAnswer[-1].isalnum() or newAnswer[-1]=='%') else newAnswer\n",
    "        newAnswer = newAnswer.replace(',','-COMMA-')\n",
    "        return newAnswer\n",
    "    \n",
    "    def returnAnswer(self,i,j,l):\n",
    "        answerListPOS =self.json_dataPOS[i][l][self.dictDoc[i,j][0]]    \n",
    "        answerListNER = self.json_data[i][l][self.dictDoc[i,j][0]]    \n",
    "        question_type = self.dictDoc[i,j][1][1]\n",
    "        #print answerListPOS,question_type\n",
    "        question_typeLocationInAnswer = [x for x,y in enumerate(answerListNER) \\\n",
    "                                         if ('CD' in answerListPOS[answerListNER.index(y)][1] \\\n",
    "                                             or 'NN' in answerListPOS[answerListNER.index(y)][1]) \\\n",
    "                                         and y[1] in question_type and y[0] not in stopwords \\\n",
    "                                         and x not in self.dictDoc[i,j][1][0][0] \\\n",
    "                                         and x not in self.dictDoc[i,j][1][0][1]]\n",
    "        #print question_typeLocationInAnswer\n",
    "        if len(question_typeLocationInAnswer) ==0:\n",
    "            question_typeLocationInAnswer = [x for x,y in enumerate(answerListNER) \\\n",
    "                                             if ('CD' in answerListPOS[answerListNER.index(y)][1] \\\n",
    "                                                 or 'NN' in answerListPOS[answerListNER.index(y)][1]) \\\n",
    "                                             and y[0] not in stopwords and x not in self.dictDoc[i,j][1][0][0] \\\n",
    "                                             and x not in self.dictDoc[i,j][1][0][1]]\n",
    "        #print question_typeLocationInAnswer,dictDoc[i,j][1][0][1], dictDoc[i,j][1][0][0]\n",
    "        scoreList = 0\n",
    "        if len(self.dictDoc[i,j][1][0][0])!=0:\n",
    "            maxScore = sys.maxint        \n",
    "            for x in question_typeLocationInAnswer:\n",
    "                score = sum([math.fabs(z-x) for z in self.dictDoc[i,j][1][0][0]])\n",
    "                if maxScore>score:\n",
    "                    maxScore=score\n",
    "                    scoreList = x\n",
    "        else:\n",
    "            maxScore = sys.maxint\n",
    "            scoreList = 0\n",
    "            for x in question_typeLocationInAnswer:\n",
    "                score = sum([math.fabs(z-x) for z in self.dictDoc[i,j][1][0][1]])\n",
    "                if maxScore>score:\n",
    "                    maxScore=score\n",
    "                    scoreList = x\n",
    "        answerToReturn = (self.json_dataPOSOrg[i][l][self.dictDoc[i,j][0]][scoreList][0],scoreList)\n",
    "        answerPOS = self.json_dataPOSOrg[i][l][self.dictDoc[i,j][0]]\n",
    "        specialcommand = self.dictDoc[i,j][1][2]\n",
    "        question_index = self.dictDoc[i,j][0]\n",
    "        document_index = i\n",
    "        answer = self.createNP(answerToReturn,answerPOS,specialcommand,document_index,question_index)\n",
    "        return answer\n",
    "\n",
    "    def writeToFile(self,filename,l):\n",
    "        with open(filename, mode='wb',) as csv_file:\n",
    "            if l == 2 :\n",
    "                fieldnames = ['document_id','question_id','answer_predict',\"answer_actual\",'tag','sentence','predict','question_type','question']\n",
    "            elif l == 1:\n",
    "                fieldnames = ['id','answer']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames,delimiter=',')\n",
    "            writer.writeheader()\n",
    "            k = 0\n",
    "            doc_size=len(self.json_data)\n",
    "            for i in range(0, doc_size):\n",
    "                for j in range(0,len(self.json_data[i][0])):\n",
    "                    k+=1            \n",
    "                    dictToCSV={}\n",
    "                    if l == 2:\n",
    "                        dictToCSV['document_id'] = i\n",
    "                        dictToCSV['question_id'] = j\n",
    "                        try:\n",
    "                            dictToCSV['answer_predict']= self.returnAnswer(i,j,l)\n",
    "                        except:\n",
    "                            print '\\nErrors on dev set return answers',i,j\n",
    "                        dictToCSV['answer_actual'] = self.json_dataOrg[i]['qa'][j]['answer']\n",
    "                        dictToCSV['tag'] =self.dictDoc[i,j]\n",
    "                        dictToCSV['question_type'] = self.dictDoc[i,j][1][1]\n",
    "                        dictToCSV['question'] = self.json_dataOrg[i]['qa'][j]['question']\n",
    "                        dictToCSV['sentence'] = self.json_dataOrg[i]['qa'][j]['answer_sentence']\n",
    "                        dictToCSV['predict'] = self.dictDoc[i,j][0]\n",
    "                    elif l == 1:\n",
    "                        dictToCSV={}\n",
    "                        dictToCSV['id'] = k\n",
    "                        try:\n",
    "                            dictToCSV['answer'] = self.returnAnswer(i,j,l)\n",
    "                        except:\n",
    "                            print '\\nError on test return answers',i,j,l\n",
    "                    writer.writerow(dictToCSV)    \n",
    "                    csv_file.flush()\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write(str(k))\n",
    "                sys.stdout.flush()\n",
    "        csv_file.close()\n",
    "        print '\\nsuccess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER json file import successful\n",
      "1194\n",
      "Errors on dev set return answers 6 98\n",
      "1482\n",
      "Errors on dev set return answers 8 4\n",
      "1796\n",
      "Errors on dev set return answers 11 598\n",
      "3456\n",
      "Errors on dev set return answers 18 159\n",
      "\n",
      "Errors on dev set return answers 18 180\n",
      "5573\n",
      "Errors on dev set return answers 29 242\n",
      "\n",
      "Errors on dev set return answers 29 443\n",
      "7266\n",
      "Errors on dev set return answers 34 259\n",
      "8463\n",
      "succes\n"
     ]
    }
   ],
   "source": [
    "qa = answer_rank(\"data/NERdev.json\",\"data/POSdev.json\",\"data/QA_dev.json\")\n",
    "qa.readCSV(\"data/bm25_dev_predictions.csv\",2)\n",
    "qa.writeToFile(\"data/bm25_dev_result.csv\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancement for Q&A systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25_S_Model(object): \n",
    "    def __init__(self, document_collection, k1=1.5, b=0.75, k3=1.0, EPS=0.25, tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.document_collection_length = len(document_collection) \n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length \n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection] \n",
    "        self.tf = [] \n",
    "        self.df = defaultdict(int) \n",
    "        self.bm25_idf = defaultdict(float) \n",
    "        self.average_idf = -1 \n",
    "        self.k1 = k1 \n",
    "        self.k3 = k3 \n",
    "        self.EPSILON = EPS \n",
    "        self.b = b \n",
    "        self.inverted_index = defaultdict(list) \n",
    "        self.initialize() \n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection): \n",
    "            doc_term_freq = FreqDist(document) \n",
    "            self.tf.append(doc_term_freq) \n",
    "            \n",
    "            for word, freq in doc_term_freq.items(): \n",
    "                self.df[word] += 1 \n",
    "                self.inverted_index[word].append(index) \n",
    "        for word, freq in self.df.items(): \n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5)  \n",
    "            self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys()) \n",
    "    \n",
    "    def predict(self, queryX, limit=1): \n",
    "        q_prediction = [] \n",
    "        for query in queryX: \n",
    "            answers = self.bm25_get_most_relevant(query)[:limit] \n",
    "            if len(answers) == 0:\n",
    "                q_prediction.append([]) \n",
    "            else:\n",
    "                q_prediction.append([i[0] for i in answers]) \n",
    "        return q_prediction \n",
    "\n",
    "    def bm25_get_most_relevant(self, query): \n",
    "        query_tks = self.tokenizer(query) \n",
    "        scores = defaultdict(float) \n",
    "        for q_token in query_tks: \n",
    "            for doc_index in self.inverted_index[q_token]: \n",
    "                idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf \n",
    "                top = self.tf[doc_index][q_token] * (self.k1 + 1) \n",
    "                below = self.tf[doc_index][q_token] + self.k1 * (1 - self.b + self.b * self.document_collection_length / self.avg_doc_length) \n",
    "                scores[doc_index] += idf * top / below \n",
    "        prels = scores.items() \n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True) \n",
    "        score1 = score2 = score3 = 0\n",
    "        new = []\n",
    "        if len(sorted_socres)  > 3:\n",
    "            if  sorted_socres[0][0] != 0 and sorted_socres[1][0] != 0 and sorted_socres[2][0] != 0:\n",
    "                left1,right1 =  - 1, sorted_socres[0][0] +1\n",
    "                left2,right2 = sorted_socres[1][0] - 1, sorted_socres[1][0] +1\n",
    "                left3,right3 = sorted_socres[2][0] - 1, sorted_socres[2][0] +1\n",
    "                score1= 0.8*scores[left1]+0.2*scores[right1]\n",
    "                score2 = 0.8*scores[left2]+0.2*scores[right2]\n",
    "                score3 = 0.8*scores[left3]+0.2*scores[right3]\n",
    "            new.append((sorted_socres[0][0],sorted_socres[0][1]+score1))\n",
    "            new.append((sorted_socres[1][0],sorted_socres[1][1]+score2))\n",
    "            new.append((sorted_socres[2][0],sorted_socres[2][1]+score3))\n",
    "        if new :\n",
    "            new_sort = sorted_socres = sorted(new, key=lambda (k, v): v, reverse=True) \n",
    "            return new_sort\n",
    "        else:\n",
    "            return sorted_socres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 25 245 When was Chanakya alive?\n"
     ]
    }
   ],
   "source": [
    "for document in dataset:\n",
    "    document_collections_sents = document['sentences']\n",
    "    document['model'] = BM25_S_Model(document_collections_sents)\n",
    "write_csv('data/bm25_dev_ss.csv',dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness results : 0.574618929458\n"
     ]
    }
   ],
   "source": [
    "u = give_prediction_values('data/bm25_dev_ss.csv',devdata = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
