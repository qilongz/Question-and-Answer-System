{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math,numpy,json,re,nltk\n",
    "import time,re,os.path,sys\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import defaultdict\n",
    "from numpy import multiply\n",
    "from math import sqrt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os.path as path\n",
    "from collections import OrderedDict\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk import FreqDist, DictionaryProbDist\n",
    "from operator import add\n",
    "import csv, ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful \n",
      "There are totally 42 documents in this dataset\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "filename_ls = ['QA_test.json']\n",
    "dataset = []\n",
    "train_path = path.abspath('data/QA_test.json')\n",
    "\n",
    "dataset = []\n",
    "with open(train_path) as f:\n",
    "    for line in f:\n",
    "        dataset+=(json.loads(line))\n",
    "print \"Import Successful \"\n",
    "print \"There are totally\", len(dataset),'documents in this dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build modle and evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return list(terms)\n",
    "\n",
    "\n",
    "class MostRelevantSentenceModel(object):\n",
    "    def __init__(self, vectorizer, collection_matrix):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.collection_matrix = collection_matrix\n",
    "        feature_array = vectorizer.get_feature_names()\n",
    "        self.features = dict()\n",
    "        for index in range(len(feature_array)):\n",
    "            term = feature_array[index]\n",
    "            self.features[term] = index\n",
    "\n",
    "    def predict(self, queies):\n",
    "        predictions = [self.inverted_index_score(i) for i in  queies]\n",
    "        return predictions\n",
    "\n",
    "    def inverted_index_score(self, query_sent):\n",
    "        \"\"\"\n",
    "        now we implement inverted index to handle query\n",
    "        \n",
    "        :param query_sent: \n",
    "        :return: \n",
    "        \n",
    "        \"\"\"\n",
    "        query_words = my_tokenizer(query_sent)\n",
    "        score = defaultdict(float)\n",
    "\n",
    "        for w in query_words:\n",
    "            try:\n",
    "                col_i = self.features[w]\n",
    "                inverted_ix = self.collection_matrix[:, col_i]\n",
    "                for di in range(inverted_ix.shape[0]):\n",
    "                    score[di] += inverted_ix[di, 0]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        index_score = sorted(score.items(), key=lambda (k, v): v, reverse=True)\n",
    "\n",
    "        if index_score:\n",
    "            top10_doc_index = [i[0] for i in index_score[:10]]\n",
    "            return top10_doc_index\n",
    "        else:\n",
    "            print 'error occured' ,query_sent\n",
    "            return -1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_and_evaluate(model, query ,report=False):\n",
    "    evaluate_row = []\n",
    "    pred = model.predict(query)\n",
    "    quest_index = 0\n",
    "    for pred_index in pred:\n",
    "        drow = dict()\n",
    "        if report:\n",
    "            print pred_index\n",
    "        drow['question_ID'] = quest_index\n",
    "        drow['prediction_ID'] = pred_index\n",
    "        evaluate_row.append(drow)\n",
    "        quest_index += 1\n",
    "    return evaluate_row\n",
    "\n",
    "\n",
    "# #build model for each document collaction\n",
    "\n",
    "for document in dataset:\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, use_idf=True,norm='l1',stop_words=None, tokenizer=my_tokenizer)\n",
    "    document_collections_sents = document['sentences']\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(document_collections_sents)\n",
    "    document['model'] = MostRelevantSentenceModel(vectorizer=tfidf_vectorizer,collection_matrix=tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output with predtion and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occured What was destroyed in the fire?\n",
      "error occured What is the Barengraben?\n",
      "error occured Who were the “intelligentia?” \n",
      "error occured What are phrynges?\n",
      "error occured When was the shooting?\n",
      "Running time count: 206.378999949\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_file = open('evaluatin_test_results.csv', mode='w',)\n",
    "fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "writer = csv.DictWriter(csv_file, fieldnames=fieldnames, )\n",
    "writer.writeheader()\n",
    "\n",
    "ddi = 0\n",
    "for document in dataset:\n",
    "    questions = [i['question'] for i in document['qa']]\n",
    "    model = document['model']\n",
    "    result_row = build_model_and_evaluate(model, questions)\n",
    "    doc_sents = document['sentences']\n",
    "    for r in result_row:\n",
    "        r['document_ID'] = ddi\n",
    "        r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "        r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "        writer.writerow(r)\n",
    "    ddi += 1\n",
    "print 'Running time count:', time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detectQuestion(argument):\n",
    "    argument = tokenizeUnicode(argument.lower())\n",
    "    if 'what' in argument and len(set(argument) & set(['year','time','rankings']))!=0:\n",
    "        return 'NUMBER'\n",
    "    elif 'who' in argument or 'whom' in argument or 'whose' in argument or \\\n",
    "            ('what' in argument and len(set(argument) & set(['name']))!=0):\n",
    "        return 'PERSON'\n",
    "    elif 'when' in argument:\n",
    "        return 'NUMBER'\n",
    "    elif 'where' in argument:\n",
    "        return 'LOCATION'\n",
    "    elif 'how' in argument and len(set(argument) & set(['many','much','long','far']))!=0:\n",
    "        return 'NUMBER'        \n",
    "    elif 'why' in argument:\n",
    "        return 'O'\n",
    "    elif 'which' in argument and len(set(argument) & set(['year',\"years\"]))!=0:\n",
    "        return 'NUMBER'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PERSON'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizeUnicode(aUnicode):\n",
    "    return word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',aUnicode)))\n",
    "\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "cwd = os.getcwd()\n",
    "stPOS = StanfordPOSTagger(cwd+'\\\\data\\\\wsj-0-18-left3words-distsim.tagger',cwd+'\\data\\stanford-postagger.jar')\n",
    "def posTagger(aString):\n",
    "    return stPOS.tag_sents([tokenizeUnicode(aString)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n"
     ]
    }
   ],
   "source": [
    "with open(\"NERtest.json\") as json_file:\n",
    "    json_data = json_load_byteified(json_file)\n",
    "print 'import success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8974\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'evaluatin_test_results.csv'\n",
    "i=0\n",
    "answerSecondFilter = []\n",
    "dictDoc={}\n",
    "question={}\n",
    "with open(csv_file) as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:        \n",
    "        document_i = int(row['document_ID'])\n",
    "        question_i = int(row['question_ID'])\n",
    "        filteredlistOfNERSentence = []\n",
    "        question_type = detectQuestion(row['question'])\n",
    "        question[document_i,question_i] = row['question']\n",
    "        for x in ast.literal_eval(row['prediction_ID']):\n",
    "            listOfNERSentence = json_data[document_i][1][x]                      \n",
    "            if question_type in [k[1] for k in listOfNERSentence]:\n",
    "                filteredlistOfNERSentence.append(x)\n",
    "                i+=1\n",
    "        dictDoc[document_i,question_i,question_type]=filteredlistOfNERSentence        \n",
    "print len(dictDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'what', u'WP'), (u'is', u'VBZ')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictDoc2 = {}\n",
    "for i,j,qtype in dictDoc:\n",
    "    if qtype!= u'O':\n",
    "        for x in dictDoc[i,j,qtype]:\n",
    "            dictDoc2[i,j]=[x for x in json_data[i][0][j] if x[1]==qtype]\n",
    "    else:\n",
    "        for x in dictDoc[i,j,qtype]:\n",
    "            dictDoc2[i,j]=json_data[i][0][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[353, 39] []\n",
      "[['What', 'O'], ['year', 'O'], ['did', 'O'], ['the', 'O'], ['Crimean', 'O'], ['War', 'O'], ['begin', 'O']]\n",
      "[['The', 'O'], ['Crimean', 'O'], ['War', 'O'], ['marked', 'O'], ['the', 'O'], ['ascendancy', 'O'], ['of', 'O'], ['France', 'LOCATION'], ['to', 'O'], ['the', 'O'], ['position', 'O'], ['of', 'O'], ['pre-eminent', 'O'], ['power', 'O'], ['on', 'O'], ['the', 'O'], ['Continent', 'O'], ['411', 'NUMBER'], ['the', 'O'], ['continued', 'O'], ['decline', 'O'], ['of', 'O'], ['the', 'O'], ['Ottoman', 'O'], ['Empire', 'O'], ['and', 'O'], ['the', 'O'], ['beginning', 'O'], ['of', 'O'], ['a', 'O'], ['decline', 'O'], ['for', 'O'], ['Tsarist', 'O'], ['Russia', 'LOCATION'], ['.', 'O']]\n",
      "[['In', 'O'], ['1830', 'NUMBER'], ['Greece', 'LOCATION'], ['becomes', 'O'], ['an', 'O'], ['independent', 'O'], ['state', 'O'], ['after', 'O'], ['10', 'NUMBER'], ['years', 'O'], ['of', 'O'], ['independence', 'O'], ['war', 'O'], ['and', 'O'], ['the', 'O'], ['Russo-Turkish', 'O'], ['War', 'O'], ['of', 'O'], ['1828', 'NUMBER'], ['1829', 'NUMBER'], ['.', 'O']]\n",
      "[(u'What', u'WP'), (u'year', u'NN'), (u'did', u'VBD'), (u'the', u'DT'), (u'Crimean', u'NNP'), (u'War', u'NNP'), (u'begin', u'VB')]\n",
      "[(u'Crimean', u'NNP'), (u'War', u'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print dictDoc[0,0,u'NUMBER'],dictDoc2[0,0]\n",
    "print json_data[0][0][0]\n",
    "print json_data[0][1][353]\n",
    "print json_data[0][1][39]\n",
    "print posTagger(question[0,0])\n",
    "print [x for x in posTagger(question[0,0]) if x[1]=='NNP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n"
     ]
    }
   ],
   "source": [
    "def json_load_byteified(file_handle):\n",
    "    return _byteify(json.load(file_handle, object_hook=_byteify),ignore_dicts=True)\n",
    "\n",
    "def _byteify(data, ignore_dicts = False):\n",
    "    # if this is a unicode string, return its string representation\n",
    "    if isinstance(data, unicode):\n",
    "        return data.encode('utf-8')\n",
    "    # if this is a list of values, return list of byteified values\n",
    "    if isinstance(data, list):\n",
    "        return [ _byteify(item, ignore_dicts=True) for item in data ]\n",
    "    # if this is a dictionary, return dictionary of byteified keys and values\n",
    "    # but only if we haven't already byteified it\n",
    "    if isinstance(data, dict) and not ignore_dicts:\n",
    "        return {\n",
    "            _byteify(key, ignore_dicts=True): _byteify(value, ignore_dicts=True)\n",
    "            for key, value in data.iteritems()\n",
    "        }\n",
    "    # if it's anything else, return it in its original form\n",
    "    return data\n",
    "\n",
    "with open(\"data/QA_train.json\") as json_file:\n",
    "    json_data = json_load_byteified(json_file)\n",
    "print 'import success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = \"C:/Program Files/Java\" # replace this\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip uninstall nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%\n",
      "Time spending: 2928.25099993\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "st = StanfordNERTagger(cwd+'\\data\\english.all.3class.distsim.crf.ser.gz',cwd+'\\data\\stanford-ner.jar')\n",
    "\n",
    "if not os.path.isfile(\"NERtrain.json\"):    \n",
    "    start = time.time()\n",
    "    progressT = len(json_data)    \n",
    "    listOfDocument=[]\n",
    "    i=0\n",
    "    for jd in json_data:\n",
    "        aList=[]        \n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))\n",
    "        #remove the below file if running on test set\n",
    "        aList.extend([st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['answer']))) for x in jd['qa']])])\n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "        listOfDocument.append(aList)\n",
    "        i+=1\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"%d%%\" % (i*100/progressT))\n",
    "        sys.stdout.flush()    \n",
    "    for document in range(0,len(listOfDocument)):\n",
    "        #change [2] to [1] if test set\n",
    "        for sentence in range(0,len(listOfDocument[document][2])):\n",
    "            for word in range(0,len(listOfDocument[document][2][sentence])):   \n",
    "                listOfDocument[document][2][sentence][word]= (listOfDocument[document][2][sentence][word][0]\n",
    "                                                              ,listOfDocument[document][2][sentence][word][1] if not listOfDocument[document][2][sentence][word][0].isdigit() else u'NUMBER')\n",
    "    with open('NERtrain.json', 'w') as outfile:\n",
    "        json.dump(listOfDocument, outfile)\n",
    "    end = time.time()\n",
    "    print '\\nTime spending:',end - start    \n",
    "else:    \n",
    "    print 'there is a file'\n",
    "with open(\"NERtrain.json\") as json_file:\n",
    "        json_dataNER = json_load_byteified(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'What', u'O'),\n",
       " (u'year', u'O'),\n",
       " (u'did', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'Crimean', u'O'),\n",
       " (u'War', u'O'),\n",
       " (u'begin', u'O')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfDocument[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for document in range(0,len(listOfDocument)):\n",
    "        for sentence in range(0,len(listOfDocument[document])):\n",
    "            for word in range(0,len(listOfDocument[document][2][sentence])):   \n",
    "                listOfDocument[document][2][sentence][word]= (listOfDocument[document][2][sentence][word][0],listOfDocument[document][2][sentence][word][1] if not listOfDocument[document][2][sentence][word][0].isdigit() else u'NUMBER')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
