{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Module 1. Prepare BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from six import iteritems\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "stop_english = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# N_GRAM = (1, 3)\n",
    "\n",
    "# def get_n_gram(ls, n):\n",
    "#     l = len(ls)\n",
    "#     nlist = []\n",
    "#     if n > len(ls):\n",
    "#         return nlist\n",
    "#     for i in range(l-n+1):\n",
    "#         nlist.append(tuple(ls[i: i+n]))\n",
    "#     return nlist\n",
    "\n",
    "# lbbbs = [0.6, 0.20, 0.20]\n",
    "\n",
    "# def my_tokenize(sentence):\n",
    "#     \"\"\" This the is tokenize function, part of the feature engineering \"\"\"\n",
    "#     sentence = sentence.lower()\n",
    "#     tokens = word_tokenize(sentence)\n",
    "\n",
    "#     lls = [stemmer.stem(i) for i in tokens if i not in stop_english]  # re.search(r'[a-z0-9]+', ii)\n",
    "#     lo, hi = (1, 3)\n",
    "#     nglist = []\n",
    "#     assert lo <= hi\n",
    "#     while lo <= hi:\n",
    "#         if lo == 1:\n",
    "#             nglist += lls\n",
    "#         else:\n",
    "#             nglist += get_n_gram(lls, lo)\n",
    "#         lo += 1\n",
    "\n",
    "#     return nglist\n",
    "\n",
    "class BM25_Model(object):\n",
    "\n",
    "    def __init__(self, document_collection, K1=0.3, B=0.0, K3=1.0, EPS=0.000001, tokenizer=None):\n",
    "       \n",
    "        self.tokenizer =my_tokenize \n",
    "        self.document_collection_length = len(document_collection)\n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length\n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection]\n",
    "        self.f = []\n",
    "        self.df = defaultdict(int)\n",
    "        self.bm25_idf = defaultdict(float)\n",
    "        self.idf_1 = defaultdict(float)\n",
    "        self.average_idf = -1\n",
    "        self.K1 = K1\n",
    "        self.K3 = K3\n",
    "        self.EPSILON = EPS\n",
    "        self.B = B\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection):\n",
    "            frequencies = FreqDist(document)\n",
    "            self.f.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                self.df[word] += 1\n",
    "                self.inverted_index[word].append(index)\n",
    "\n",
    "        for word, freq in self.df.items():\n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5)\n",
    "            # self.idf_1 = math.log((self.document_collection_length - freq))\n",
    "        self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys())\n",
    "\n",
    "    def my_tokenize(self,sentence):\n",
    "        \"\"\" This the is tokenize function, part of the feature engineering \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        tokens = word_tokenize(sentence)\n",
    "        lls = [stemmer.stem(i) for i in tokens if i not in stop_english]  # re.search(r'[a-z0-9]+', ii)\n",
    "        lo, hi = N_GRAM\n",
    "        nglist = []\n",
    "        assert lo <= hi\n",
    "        while lo <= hi:\n",
    "            if lo == 1:\n",
    "                nglist += lls\n",
    "            else:\n",
    "                nglist += get_n_gram(lls, lo)\n",
    "            lo += 1\n",
    "\n",
    "        return nglist      \n",
    "        \n",
    "    def get_n_gram(self,ls, n):\n",
    "        l = len(ls)\n",
    "        nlist = []\n",
    "        if n > len(ls):\n",
    "            return nlist\n",
    "        for i in range(l-n+1):\n",
    "            nlist.append(tuple(ls[i: i+n]))\n",
    "        return nlist\n",
    "\n",
    "        def predict(self, queryX, limit=1):\n",
    "            q_prediction = []\n",
    "            i = 0\n",
    "            for query in queryX:\n",
    "                i += 1\n",
    "                if i == 192:\n",
    "                    lll = 0\n",
    "                ls = self.bm25_get_most_relevant(query)[:limit]\n",
    "                if ls:\n",
    "                    q_prediction.append([a for a, b in ls])\n",
    "                else:\n",
    "                    q_prediction.append([])\n",
    "        return q_prediction\n",
    "\n",
    "    def bm25_get_most_relevant(self, query):\n",
    "        query_tks = self.tokenizer(query)\n",
    "        scores = defaultdict(float)\n",
    "        lbbbs = [0.6, 0.20, 0.20]\n",
    "        for q_token in query_tks:\n",
    "            if self.df[q_token] / float(self.document_collection_length) < 0.80:\n",
    "                for doc_index in self.inverted_index[q_token]:\n",
    "                    idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf\n",
    "                    top = self.f[doc_index][q_token] * (self.K1 + 1)\n",
    "                    below = self.f[doc_index][q_token] + self.K1 * (\n",
    "                        1 - self.B + self.B * self.document_collection_length / self.avg_doc_length)\n",
    "                    vvs = idf * (top / below)\n",
    "                    if isinstance(q_token, tuple):\n",
    "                        vvs *= lbbbs[len(q_token)-1]\n",
    "                    else:\n",
    "                        vvs *= lbbbs[0]\n",
    "                    scores[doc_index] += vvs\n",
    "        prels = scores.items()\n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True)\n",
    "\n",
    "        return sorted_socres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Module 2. Prepare for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json,time,sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import StanfordNERTagger, StanfordPOSTagger\n",
    "from nltk.internals import find_jars_within_path\n",
    "\n",
    "#Genrate the path for tagger files\n",
    "cwd=os.getcwd()+'//'\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(cwd+'wsj-0-18-left3words-distsim.tagger',\n",
    "                                            cwd+'stanford-postagger.jar', encoding='utf-8')\n",
    "\n",
    "class Prepare:\n",
    "    def __init__(self, source, classification='3class'):\n",
    "        self.source = source\n",
    "        self.stop_english = set(stopwords.words('english'))\n",
    "        self.dataset = json.loads(open(source).readline())\n",
    "\n",
    "        if classification == '7class':\n",
    "            model_file = 'english.muc.7class.distsim.crf.ser.gz'\n",
    "        elif classification == '3class':\n",
    "            model_file = 'english.all.3class.distsim.crf.ser.gz'\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        self.ner_tagger = StanfordNERTagger(cwd+model_file, cwd+'stanford-ner.jar', encoding='utf-8')\n",
    "    \n",
    "        self.pos_tagger = StanfordPOSTagger(cwd+'wsj-0-18-left3words-distsim.tagger',\n",
    "                                            cwd+'stanford-postagger.jar', encoding='utf-8')\n",
    " \n",
    "\n",
    "        self.prog_total = len(self.dataset)\n",
    "\n",
    "    pass\n",
    "\n",
    "    def dmerge(self, ner, pos):\n",
    "        if ner[1] == 'O':\n",
    "            if pos and re.search(r'[0-9]+', ner[0]):\n",
    "                return ner[0], 'NUMBER'\n",
    "            elif pos and pos[1] == 'CD':\n",
    "                return ner[0], 'NUMBER'\n",
    "            return pos\n",
    "        else:\n",
    "            return ner\n",
    "\n",
    "    def _merge_tag(self, ners, poss):\n",
    "        return map(self.dmerge, ners, poss)\n",
    "\n",
    "    def putd(self, dic, key, value):\n",
    "        dic[key] = value\n",
    "\n",
    "    def launch(self, simplified=False, purpose='dev', file_name='xdev_cb7_standard.json'):\n",
    "        print ('Souce Using:', self.source, 'Simplified:', simplified, 'purpose', purpose)\n",
    "        ts = time.time()\n",
    "\n",
    "        assert purpose in ['dev', 'test']\n",
    "        if simplified:\n",
    "            assert purpose == 'dev'\n",
    "\n",
    "        jfile = open(file_name, 'w')\n",
    "        prog_i = 0.0\n",
    "        for col in self.dataset:\n",
    "            document_collection = col['sentences']\n",
    "\n",
    "            if purpose == 'test':\n",
    "                bm25_query_model = BM25_Model(document_collection)\n",
    "\n",
    "                def bm25_tag(qa):\n",
    "                    if bm25_query_model.predict([qa['question']])[0]:\n",
    "                        qa['answer_sentence'] = bm25_query_model.predict([qa['question']])[0][0]\n",
    "                    else:\n",
    "                        qa['answer_sentence'] = 0\n",
    "\n",
    "                map(lambda m: bm25_tag(m), col['qa'])\n",
    "                map(lambda m: self.putd(m, 'question_tks', word_tokenize(m['question'])), col['qa'])\n",
    "                map(lambda m: self.putd(m, 'ans_sent',\n",
    "                                        col['sentences'][m['answer_sentence']].\n",
    "                                        encode('utf-8').decode('utf-8')), col['qa'])\n",
    "                map(lambda m: self.putd(m, 'ans_sent_tks', word_tokenize(m['ans_sent'])), col['qa'])\n",
    "\n",
    "                try:\n",
    "                    map(lambda m: self.putd(m, 'answer_tks', word_tokenize(m['answer'])), col['qa'])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif purpose == 'dev':\n",
    "\n",
    "                if not simplified:\n",
    "                    map(lambda m: self.putd(m, 'question_tks', word_tokenize(m['question'])), col['qa'])\n",
    "                    map(lambda m: self.putd(m, 'ans_sent',\n",
    "                                            col['sentences'][m['answer_sentence']].\n",
    "                                            encode('utf-8').decode('utf-8')), col['qa'])\n",
    "                    map(lambda m: self.putd(m, 'ans_sent_tks', word_tokenize(m['ans_sent'])), col['qa'])\n",
    "                map(lambda m: self.putd(m, 'answer_tks', word_tokenize(m['answer'])), col['qa'])\n",
    "\n",
    "            try:\n",
    "                if not simplified:\n",
    "                    questions_ner = self.ner_tagger.tag_sents([q['question_tks'] for q in col['qa']])\n",
    "                    questions_pos = self.pos_tagger.tag_sents([q['question_tks'] for q in col['qa']])\n",
    "                    q_tags = map(self._merge_tag, questions_ner, questions_pos)\n",
    "                    map(lambda m, val: self.putd(m, 'question_tks', val),  col['qa'], q_tags)\n",
    "\n",
    "                    asent_ner = self.ner_tagger.tag_sents([q['ans_sent_tks'] for q in col['qa']])\n",
    "                    asent_pos = self.pos_tagger.tag_sents([q['ans_sent_tks'] for q in col['qa']])\n",
    "                    asent_tags = map(self._merge_tag, asent_ner, asent_pos)\n",
    "                    map(lambda m, val: self.putd(m, 'ans_sent_tks', val), col['qa'], asent_tags)\n",
    "\n",
    "                if 'answer_tks' in col['qa'][0]:\n",
    "                    answer_ner = self.ner_tagger.tag_sents([q['answer_tks'] for q in col['qa']])\n",
    "                    answer_pos = self.pos_tagger.tag_sents([q['answer_tks'] for q in col['qa']])\n",
    "                    a_tags = map(self._merge_tag, answer_ner, answer_pos)\n",
    "                    map(lambda m, val: self.putd(m, 'answer_tks', val), col['qa'], a_tags)\n",
    "\n",
    "            except ImportError:\n",
    "                col['qa'] = None\n",
    "                col = None\n",
    "                print('Error:', prog_i, '#####')\n",
    "\n",
    "            if col:\n",
    "                if simplified:\n",
    "                    del col['sentences']\n",
    "                jfile.write(json.dumps(col) + '\\n')\n",
    "            prog_i += 1\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"%f%%\" % (prog_i * 100.0 / self.prog_total))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print '\\n',file_name, 'is generatred'\n",
    "        print'EXEC: ', time.time() - ts\n",
    "        jfile.close()\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re,json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class ClassificationBuild:\n",
    "    def __init__(self, train_data=None, test_data=None, nclass='7class'):\n",
    "        print ('building dataset..')\n",
    "        tt0 = time.time()\n",
    "\n",
    "        self.nclass = nclass\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.stop_english = set(stopwords.words('english'))\n",
    "\n",
    "        self.test_Xo, self.test_y, self.test_by_cat = self._extract(test_data)\n",
    "        self.train_Xo, self.train_y, self.train_by_cat = self._extract(train_data)\n",
    "        self.ml = LogisticRegression(C=1.0)\n",
    "        self.cv = CountVectorizer(stop_words='english', tokenizer=self.my_tokenize)\n",
    "        self.combined = True\n",
    "        print ('Finish initialization:', time.time()-tt0)\n",
    "        pass\n",
    "\n",
    "    def q_qtag_c7(self, qa):\n",
    "        qsent, atg = qa['question'], qa['answer_tks']\n",
    "\n",
    "        atg = map(lambda x: x[1], atg)\n",
    "        tag = 'O'\n",
    "\n",
    "        if 'DATE' in atg:\n",
    "            tag = 'DATE'\n",
    "        elif 'TIME' in atg:\n",
    "            tag = 'DATE'\n",
    "        elif 'MONEY' in atg:\n",
    "            tag = 'MONEY'\n",
    "        elif 'PERCENT' in atg:\n",
    "            tag = 'PERCENT'\n",
    "        elif 'NUMBER' in atg:\n",
    "            tag = 'NUMBER'\n",
    "        elif 'PERSON' in atg:\n",
    "            tag = 'PERSON'\n",
    "        elif 'LOCATION' in atg:\n",
    "            tag = 'LOCATION'\n",
    "        elif 'ORGANIZATION' in atg:\n",
    "            tag = 'ORGANIZATION'\n",
    "\n",
    "        d = defaultdict(list)\n",
    "        q = dict()\n",
    "        q['q'] = qsent\n",
    "        q['a'] = qa['answer']\n",
    "        d[tag].append(q)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def q_qtag_c3(self, qa):\n",
    "        qsent, atg = qa['question'], qa['answer_tks']\n",
    "\n",
    "        atg = map(lambda x: x[1], atg)\n",
    "        tag = 'O'\n",
    "        if 'DATE' in atg:\n",
    "            tag = 'NUMBER'\n",
    "        elif 'TIME' in atg:\n",
    "            tag = 'NUMBER'\n",
    "        elif 'MONEY' in atg:\n",
    "            tag = 'NUMBER'\n",
    "        elif 'PERCENT' in atg:\n",
    "            tag = 'NUMBER'\n",
    "        elif 'NUMBER' in atg:\n",
    "            tag = 'NUMBER'\n",
    "        elif 'PERSON' in atg:\n",
    "            tag = 'PERSON'\n",
    "        elif 'LOCATION' in atg:\n",
    "            tag = 'LOCATION'\n",
    "        elif 'ORGANIZATION' in atg:\n",
    "            tag = 'ORGANIZATION'\n",
    "\n",
    "        d = defaultdict(list)\n",
    "        q = dict()\n",
    "        q['q'] = qsent\n",
    "        q['a'] = qa['answer']\n",
    "        d[tag].append(q)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def _extract(self, dataset):\n",
    "        all_qa = []\n",
    "        for col in dataset:\n",
    "            all_qa.extend(col['qa'])\n",
    "\n",
    "        if self.nclass == '7class':\n",
    "            extract_Xy = map(self.q_qtag_c7, all_qa)\n",
    "        elif self.nclass == '3class':\n",
    "            extract_Xy = map(self.q_qtag_c3, all_qa)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        def group_by_tag(a, b):\n",
    "            d = defaultdict(list)\n",
    "            ts = set(a.keys() + b.keys())\n",
    "            for t in ts:\n",
    "                d[t].extend(a[t])\n",
    "                d[t].extend(b[t])\n",
    "            return d\n",
    "\n",
    "        train_set_by_cat = reduce(group_by_tag, extract_Xy)\n",
    "        _Xy = []\n",
    "        for cat in train_set_by_cat:\n",
    "            _Xy.extend(map(lambda qa: (qa['q'], cat), train_set_by_cat[cat]))\n",
    "        _X = map(lambda tt: tt[0], _Xy)\n",
    "        _y = map(lambda tt: tt[1], _Xy)\n",
    "        return _X, _y, train_set_by_cat\n",
    "\n",
    "    def my_tokenize(self, sentence):\n",
    "        \"\"\" This the is tokenize function, part of the feature engineering \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        ll = word_tokenize(sentence)\n",
    "        lls = [self.stemmer.stem(ii) for ii in ll if re.search(r'[a-z0-9]+', ii) and ii not in self.stop_english]\n",
    "\n",
    "        return lls\n",
    "\n",
    "    def _fit(self, tXo, ty):\n",
    "        tX = self.cv.fit_transform(tXo)\n",
    "        self.ml.fit(tX, ty)\n",
    "        return\n",
    "\n",
    "    def _pred3_(self, x, xm=None):\n",
    "        if 'where' in x.lower():\n",
    "            return 'LOCATION'\n",
    "        elif 'what city' in x.lower():\n",
    "            return 'LOCATION'\n",
    "        elif 'which city' in x.lower():\n",
    "            return 'LOCATION'\n",
    "        elif 'when' in x.lower():\n",
    "            return 'NUMBER'\n",
    "        elif 'how much' in x.lower():\n",
    "            return 'NUMBER'\n",
    "        elif 'how many' in x.lower():\n",
    "            return 'NUMBER'\n",
    "        elif 'what rate' in x.lower():\n",
    "            return 'NUMBER'\n",
    "        elif 'whom' in x.lower() or 'who' in x.lower():\n",
    "            return 'PERSON'\n",
    "        elif 'which' in x.lower():\n",
    "            if 'team' in x.lower():\n",
    "                return 'ORGANIZATION'\n",
    "            else:\n",
    "                return 'O'\n",
    "        else:\n",
    "            return 'O'\n",
    "\n",
    "    def _pred7_(self, x, xm=None):\n",
    "        if 'where' in x.lower():\n",
    "            return 'LOCATION'\n",
    "        if 'what city' in x.lower():\n",
    "            return 'LOCATION'\n",
    "        if 'which city' in x.lower():\n",
    "            return 'LOCATION'\n",
    "        elif 'when' in x.lower():\n",
    "            return 'DATE'\n",
    "        elif 'how much' in x.lower():\n",
    "            return 'MONEY'\n",
    "        elif 'what rate' in x.lower():\n",
    "            return 'NUMBER'\n",
    "        elif 'whom' in x.lower() or 'who' in x.lower():\n",
    "            return 'PERSON'\n",
    "        elif 'which' in x.lower():\n",
    "            if 'team' in x.lower():\n",
    "                return 'ORGANIZATION'\n",
    "            else:\n",
    "                return 'O'\n",
    "        else:\n",
    "            return 'O'\n",
    "\n",
    "    def predict(self, Xo):\n",
    "        Xmat = self.cv.transform(Xo)\n",
    "        assert Xo and Xmat is not None and self.ml\n",
    "        if self.nclass == '7class':\n",
    "            fun_pred = self._pred7_\n",
    "        elif self.nclass == '3class':\n",
    "            fun_pred = self._pred3_\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if Xo and Xmat is None:\n",
    "            pl = []\n",
    "            for x in Xo:\n",
    "                pl.append(fun_pred(x))\n",
    "            return pl\n",
    "        elif self.ml and Xmat is not None and not Xo:\n",
    "            return self.ml.predict(Xo)\n",
    "        elif self.ml and Xmat is not None and Xo:\n",
    "            rule_prediction = []\n",
    "            for x in Xo:\n",
    "                rule_prediction.append(fun_pred(x))\n",
    "            ml_prediction = self.ml.predict(Xmat)\n",
    "\n",
    "            def overwrite(rl_p, ml_p):\n",
    "                if rl_p != ml_p:\n",
    "                    if rl_p == 'O':\n",
    "                        return ml_p\n",
    "                    elif ml_p == 'ORGANIZATION':\n",
    "                        return ml_p\n",
    "                    else:\n",
    "                        return rl_p\n",
    "                return ml_p\n",
    "\n",
    "            return map(overwrite, rule_prediction, ml_prediction)\n",
    "\n",
    "    def build_model_and_evaluate(self, report=True):\n",
    "        print ('starting..')\n",
    "        tt0 = time.time()\n",
    "        # training\n",
    "        self._fit(self.train_Xo, self.train_y)\n",
    "        pred = self.predict(Xo=self.test_Xo)\n",
    "        # score\n",
    "        accuracy = metrics.accuracy_score(self.test_y, pred)\n",
    "        if report:\n",
    "            print('-' * 100)\n",
    "            print(\"macro f1 score:   %0.3f\" % metrics.f1_score(self.test_y, pred, average='macro'))\n",
    "            print\"accuracy:   %0.3f\" % accuracy, '\\n\\n'\n",
    "            print(metrics.classification_report(self.test_y, pred))\n",
    "            print()\n",
    "            print(metrics.confusion_matrix)\n",
    "        print()\n",
    "        print ('EXEC:', time.time() - tt0)\n",
    "        return accuracy\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.combined and self.ml:\n",
    "            return 'Combined ########\\n' + str(self.ml)\n",
    "        else:\n",
    "            return 'Damaged!'\n",
    "\n",
    "    def _tagup(self, dcol):\n",
    "        def upd(q):\n",
    "            q['tag'] = self.predict([q['question']])[0]\n",
    "        map(lambda q: upd(q), dcol['qa'])\n",
    "        return dcol\n",
    "\n",
    "    def tag_prediction(self, dataset, out_file_name='bm25_qtag_test.json'):\n",
    "        data_set_for_tag = map(self._tagup, dataset)\n",
    "        output_file = open(out_file_name, 'w')\n",
    "        for col in data_set_for_tag:\n",
    "            output_file.write(json.dumps(col) + '\\n')\n",
    "        return data_set_for_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define load json file function\n",
    "def get_dataset(filen):\n",
    "    dataset = []\n",
    "    try:\n",
    "        dataset = json.load(filen)\n",
    "    except:\n",
    "        texts = open(filen).readlines()\n",
    "        for t in texts:\n",
    "            dataset.append(json.loads(t))\n",
    "        if len(dataset) == 1:\n",
    "            return dataset[0]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Module 4. Build word2vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('W2V EXEC:', 74.23995399475098)\n",
      "0.0384568036719 people\n",
      "-0.101977420037 animal\n",
      "0.208270916039 device\n",
      "0.208270916039 devices\n",
      "0.121197863105 active\n",
      "0.0387065507259 near-infrared\n",
      "-0.0131857431604 illumination\n",
      "0.110321016465\n"
     ]
    }
   ],
   "source": [
    "import os,time\n",
    "from nltk import word_tokenize\n",
    "import gensim, logging\n",
    "from nltk.stem import WordNetLemmatizer as WNL\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "PICKLE = \"averaged_perceptron_tagger.pickle\"\n",
    "AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.load(AP_MODEL_LOC)\n",
    "pos_tag = tagger.tag\n",
    "\n",
    "class W2V_Model:\n",
    "    def __init__(self, model_file='word_siml_lem_s.model', dataset=None):\n",
    "        t0 = time.time()\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                            level=logging.INFO)\n",
    "\n",
    "        if os.path.exists(model_file):\n",
    "            self.model = gensim.models.Word2Vec.load(model_file)\n",
    "        else:\n",
    "            wnl = WNL()\n",
    "            corpus = reduce(lambda a, b: a + b, map(lambda d: d['sentences'], dataset))\n",
    "\n",
    "            qqs = map(lambda dd: map(lambda d: d['question'], dd['qa']), dataset)\n",
    "            corpus += reduce(lambda a, b: a + b, qqs)\n",
    "\n",
    "            def idtp(tp):\n",
    "                if re.search(r'V.*', tp[1]):\n",
    "                    return 'v'\n",
    "                return 'n'\n",
    "\n",
    "            corpus = map(\n",
    "                lambda sent: map(lambda tk: wnl.lemmatize(tk[0], idtp(tk)), pos_tag(word_tokenize(sent.lower()))),\n",
    "                corpus)\n",
    "            # corpus = map(lambda sent: word_tokenize(sent), corpus)\n",
    "\n",
    "            self.model = gensim.models.Word2Vec(corpus, size=500, min_count=0, workers=8, iter=50)\n",
    "            self.model.save(model_file)\n",
    "            print 'Word2vector running time:', time.time()-t0\n",
    "\n",
    "#     def small_test(self):\n",
    "#         print self.model.similarity('technology', 'people'), 'people'\n",
    "#         print self.model.similarity('technology', 'animal'), 'animal'\n",
    "#         print self.model.similarity('technology', 'device'), 'device'\n",
    "#         print self.model.similarity('technology', 'device'), 'devices'\n",
    "#         print self.model.similarity('technology', 'active'), 'active'\n",
    "#         print self.model.similarity('technology', 'near-infrared'), 'near-infrared'\n",
    "#         print self.model.similarity('technology', 'illumination'), 'illumination'\n",
    "#         print self.model.n_similarity(['technology'], ('active', 'near-infrared', 'illumination',))\n",
    "\n",
    "    def similarity(self, w1, w2):\n",
    "        try:\n",
    "            return w2v_model.model.similarity(w1, w2)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def ws_similarity(self, wls1, wls2):\n",
    "        try:\n",
    "            return w2v_model.model.n_similarity(wls1, wls2)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "w2v_train = get_dataset('QA_dev.json') + get_dataset('QA_test.json')  \n",
    "w2v_model = W2V_Model(dataset=w2v_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Module 5.1 Prepare QA data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys,json\n",
    "\n",
    "class QASS:\n",
    "    def __init__(self, dataset):\n",
    "        map(lambda (did, col): self.mark(did, col), enumerate(dataset))\n",
    "        self.all_qa = []\n",
    "        for col in dataset:\n",
    "            self.all_qa.extend(col['qa'])\n",
    "\n",
    "    def mark(self, did, col):\n",
    "        def mk(qa, did, qid):\n",
    "            qa['doc_id'] = did\n",
    "            qa['qa_id'] = qid\n",
    "\n",
    "        map(lambda (qid, qa): mk(qa, did, qid), enumerate(col['qa']))\n",
    "\n",
    "    def actual_qtype_tag_7(self, qa):\n",
    "        try:\n",
    "            atg = qa['answer_tks']\n",
    "            atg = map(lambda x: x[1], atg)\n",
    "            tag = 'O'\n",
    "\n",
    "            if 'DATE' in atg:\n",
    "                tag = 'DATE'\n",
    "            elif 'TIME' in atg:\n",
    "                tag = 'DATE'\n",
    "            elif 'MONEY' in atg:\n",
    "                tag = 'MONEY'\n",
    "            elif 'PERCENT' in atg:\n",
    "                tag = 'PERCENT'\n",
    "            elif 'NUMBER' in atg:\n",
    "                tag = 'NUMBER'\n",
    "            elif 'PERSON' in atg:\n",
    "                tag = 'PERSON'\n",
    "            elif 'LOCATION' in atg:\n",
    "                tag = 'LOCATION'\n",
    "            elif 'ORGANIZATION' in atg:\n",
    "                tag = 'ORGANIZATION'\n",
    "\n",
    "            qa['tag'] = tag\n",
    "        except:\n",
    "            pass\n",
    "        return qa\n",
    "\n",
    "    def get_qass(self, enbble_actag=False):\n",
    "        if enbble_actag:\n",
    "            qass = map(lambda xt: self.actual_qtype_tag_7(xt), self.all_qa)\n",
    "        else:\n",
    "            qass = self.all_qa\n",
    "        return qass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Module 5.2. Define Segement grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser, Tree\n",
    "from nltk.stem import WordNetLemmatizer as WNL\n",
    "\n",
    "wnl = WNL()\n",
    "\n",
    "grammar = \"\"\"\n",
    "    # NNP:\n",
    "    # {<DT><NNP>+}\n",
    "\n",
    "    CORE:\n",
    "    <W.*>{<N.*>+}<V.*>\n",
    "\n",
    "    ETY_ORG:\n",
    "    {<DT><ORGANIZATION>+<OF><LOCATION>+}\n",
    "    {<THE><ORGANIZATION>+<BRA><.*>+<BRB>}\n",
    "    {<THE><ORGANIZATION>+}\n",
    "    {<ORGANIZATION>+<CC><NNP>}\n",
    "    {<ORGANIZATION>+}\n",
    "    {<NNP><NN><OF><LOCATION>}\n",
    "    {<DT><NNP|CC|OF>+<NNP>}\n",
    "\n",
    "\n",
    "    # JJ:\n",
    "    # {<AN><NUMBER>}\n",
    "\n",
    "    DATE:\n",
    "    {<DATE>+<NUMBER|,>+<NUMBER|DATE>}\n",
    "    DATE:\n",
    "    {<DATE>+<NUMBER>?}\n",
    "    DATE:\n",
    "    {<DATE><NUMBER>+}\n",
    "    DATE:\n",
    "    {<NUMBER>+<DATE>}\n",
    "\n",
    "\n",
    "    ETY_DATE:\n",
    "    {<DATE>+<,>+<DATE>+}\n",
    "    {<DATE>+<,>?<DATE>+}\n",
    "    {<DATE>+}\n",
    "\n",
    "    ETY_TIME:\n",
    "    {<TIME>+}    \n",
    "\n",
    "    ETY_MNY:\n",
    "    {<MONEY><MONEY|NUMBER>+}\n",
    "    {<MONEY|NUMBER>+<MONEY>}\n",
    "    {<MONEY>+}\n",
    "\n",
    "    ETY_NUM:\n",
    "    {<NUMBER>+}\n",
    "\n",
    "    ETY_PSN:\n",
    "    {<PERSON>+}\n",
    "\n",
    "    ETY_LOC:\n",
    "    {<DT>?<LOCATION>+}\n",
    "\n",
    "    ETY_PRCT:\n",
    "    {<PERCENT>+}\n",
    "\n",
    "    ETY_STRS:\n",
    "    <``>{<.*>+}<''>\n",
    "\n",
    "    STRESS:\n",
    "    <BRA>{<ETY.*>}<BRB>\n",
    "\n",
    "\n",
    "    MNE:\n",
    "    {<THE>?<NNP><JJ>+<NN>+}\n",
    "    {<THE>?<NNP><NNP|DT|OF>+}\n",
    "\n",
    "    MNE2:\n",
    "    {<DT><N.*>+<OF><J.*><N.*>+}\n",
    "    {<J.*>+<N.*>+}\n",
    "\n",
    "    MNE4:\n",
    "    {<AN><N.*>+}\n",
    "    {<J.*>?<N.*>+}\n",
    "\n",
    "    MNE5:\n",
    "    {<J.*>+}\n",
    "\n",
    "    # NX:\n",
    "    # {<NE>+<OF>?<DT>?<NE>+}\n",
    "\n",
    "    # BULLSHIT:\n",
    "    # {<N.*>+<OF>?<DT>?<J.*>?<N.*>+}\n",
    "    # {<N.*>+<IN>?<DT>?<J.*>+<N.*>+}\n",
    "    # {<NNP>+<IN>?<DT>?<J.*>?<NNP>+}\n",
    "    # {<N.*>+<CC>?<DT>?<J.*>+<N.*>+}\n",
    "    # {<N.*>+<CC>?<DT>?<J.*>?<N.*>+}\n",
    "    # <BRA>{<.*>+}<BRB>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "grammar2 = \"\"\"\n",
    "    PSV:\n",
    "    {<BEV><IN|RB>?<VBN|VBD>+<IN|RB>?}\n",
    "\n",
    "    PRED:\n",
    "    {<V.*>+<IN|RB>?}\n",
    "\n",
    "    PX:\n",
    "    {<IN>+}\n",
    "\n",
    "    # SUBP:\n",
    "    # {(?!<VV>)<.*>+(?!<VV>)}<PRED|PSV>\n",
    "    # \n",
    "    # OBJP:\n",
    "    # <PSV|PRED>{(?!<VV>)<.*>+(?!<VV>)}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "rg_parser = RegexpParser(grammar=grammar)\n",
    "rg_parser2 = RegexpParser(grammar=grammar2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Module 5.3. Answer extraction and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import *\n",
    "import re\n",
    "\n",
    "\n",
    "class QA:\n",
    "    def __init__(red, qa):\n",
    "        red.qa = qa\n",
    "        red.qmask = map(lambda l: OrderedDict({'seq': l[0]}), enumerate(qa['question_tks']))\n",
    "        map(lambda dic, l: red.put(dic, 'ori', l[0]), red.qmask, qa['question_tks'])\n",
    "        map(lambda dic, l: red.put(dic, 'tag', l[1]), red.qmask, qa['question_tks'])\n",
    "        map(lambda dic, xt: red.put(dic, 'lem', wnl.lemmatize(xt[0].lower(), red.vb(xt[1]))),\n",
    "            red.qmask, qa['question_tks'])\n",
    "\n",
    "        _qety = map(red._ety_expand, rg_parser.parse(map(lambda m: (m['lem'], m['tag']), red.qmask)))\n",
    "        red.core = red.identify_q_core(_qety)\n",
    "\n",
    "        red.q_type = [t[0] for t in qa['question_tks'] if re.search(r'^W.*', t[1])]\n",
    "        red.highlight = red.highlightq()\n",
    "\n",
    "        red.a_length = len(qa['ans_sent_tks'])\n",
    "\n",
    "        red.a_tokens_only = map(lambda x: x[0], qa['ans_sent_tks'])\n",
    "        red.qa['ans_sent_tks'] = map(lambda qt: (qt[1][0], red.preprocess(qt)), enumerate(qa['ans_sent_tks']))\n",
    "        red.amask = map(lambda l: OrderedDict({'seq': l[0]}), enumerate(qa['ans_sent_tks']))\n",
    "\n",
    "        map(lambda dic, l: red.put(dic, 'ori', l[0]), red.amask, red.qa['ans_sent_tks'])\n",
    "        map(lambda dic, l: red.put(dic, 'tag', l[1]), red.amask, red.qa['ans_sent_tks'])\n",
    "\n",
    "        red.build_mask()\n",
    "\n",
    "        red.scoring()\n",
    "\n",
    "    def highlightq(self):\n",
    "        qtks = map(lambda m: (m['ori'], m['tag']), self.qmask)\n",
    "        grm_temp = map(self._ety_expand, rg_parser2.parse(qtks))\n",
    "        predicates = []\n",
    "        sbe = {'is', 'are', 'was', 'were', 'am', 'been', 'be', 'being'}\n",
    "        for grm in grm_temp:\n",
    "            if grm and grm[0][1] in {'PRED', 'PSV'}:\n",
    "                d = dict()\n",
    "                d['type'] = grm[0][1]\n",
    "                d['voc'] = map(lambda p: wnl.lemmatize(p, 'v'),\n",
    "                               list(set(map(lambda g: g[0], grm)) - sbe))\n",
    "                predicates.append(d)\n",
    "\n",
    "        return predicates\n",
    "\n",
    "    def put(self, dic, cat, content):\n",
    "        dic[cat] = content\n",
    "\n",
    "    def adds(self, dic, cat, floatv):\n",
    "        dic[cat] = dic.get(cat, 0.0) + floatv\n",
    "\n",
    "    def preprocess(self, etg):\n",
    "        seq, tg = etg\n",
    "        if tg[0] == 'The' and seq == 0:\n",
    "            return 'FUK'\n",
    "        if tg[0] == '(':\n",
    "            return 'BRA'\n",
    "        if tg[0] == ')':\n",
    "            return 'BRB'\n",
    "        if tg[0] == 'of' and tg[1] == 'IN':\n",
    "            return 'OF'\n",
    "        if tg[0] == 'The' and tg[1] == 'DT':\n",
    "            return 'THE'\n",
    "        if tg[0] in ['an', 'a'] and tg[1] == 'DT':\n",
    "            return 'AN'\n",
    "        if re.search(r'[0-9]+', tg[0]):\n",
    "            return 'NUMBER'\n",
    "        if tg[0] in ['nm', 'mi', 'km', 'mile', 'miles', 'per', '%']:\n",
    "            return 'NUMBER'\n",
    "        if tg[0] == 'over':\n",
    "            return 'OV'\n",
    "        if tg[0] in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
    "                     'September', 'October', 'November', 'December']:\n",
    "            return 'DATE'\n",
    "        if tg[0] in ['is', 'are', 'was', 'were', 'am', 'been', 'be', 'being']:\n",
    "            return 'BEV'\n",
    "        if tg[0] in ['such']:\n",
    "            return 'INX'\n",
    "        if tg[0] in ['for', 'to']:\n",
    "            return 'FT'\n",
    "        if tg[0] in ['[', ']']:\n",
    "            return 'O'\n",
    "        return tg[1]\n",
    "\n",
    "    def _semi(self, a, b):\n",
    "        simi_rate = 1 - edit_distance(a, b) / float(len(a + b)) * 2\n",
    "        return simi_rate > 0.8\n",
    "\n",
    "    def _ctx(self, a):\n",
    "        q_lem = map(lambda m: m['lem'], self.qmask) + ['such', 'as']\n",
    "        if self.core and any(map(lambda qt: self._semi(qt, a), self.core)):\n",
    "            return '**'\n",
    "        if any(map(lambda qt: self._semi(qt, a), q_lem)):\n",
    "            return '*'\n",
    "        return '_'\n",
    "\n",
    "    def vb(self, tk):\n",
    "        if re.search(r'^V.*', tk):\n",
    "            return 'v'\n",
    "        return 'n'\n",
    "\n",
    "    def build_mask(self):\n",
    "\n",
    "        map(lambda dic, xt: self.put(dic, 'lem', wnl.lemmatize(xt[0].lower(), self.vb(xt[1]))),\n",
    "            self.amask, self.qa['ans_sent_tks'])\n",
    "        # enu_tks = map(lambda m: m['lem'], self.mask)\n",
    "\n",
    "        map(lambda dic, xt: self.put(dic, 'ctx', self._ctx(xt)),\n",
    "            self.amask, map(lambda m: m['lem'], self.amask))\n",
    "\n",
    "        ety_temp = map(self._ety_expand, rg_parser.parse(map(lambda m: (m['lem'], m['tag']), self.amask)))\n",
    "        aet = []\n",
    "        for a in ety_temp:\n",
    "            aet.extend(a)\n",
    "\n",
    "        map(lambda dic, xt: (self.put(dic, 'ety1', xt[1]), self.put(dic, 'enum', xt[2])),\n",
    "            self.amask, aet)\n",
    "\n",
    "        grm_temp = map(self._ety_expand, rg_parser2.parse(map(lambda m: (m['ori'], m['tag']), self.amask)))\n",
    "        grm = []\n",
    "        for a in grm_temp:\n",
    "            grm.extend(a)\n",
    "\n",
    "        map(lambda dic, xt: (self.put(dic, 'grm', xt[1]), self.put(dic, 'gnum', xt[2])),\n",
    "            self.amask, grm)\n",
    "\n",
    "    def score_ctx_dup(self, tt):\n",
    "        if tt == '*':\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def _ety_expand(self, tn):\n",
    "        if isinstance(tn, Tree):\n",
    "            tnflat = tn.flatten()\n",
    "            return map(lambda c: (c[0], tn.label(), len(tnflat)), tnflat)\n",
    "        else:\n",
    "            return [(tn[0], '%', 1)]\n",
    "\n",
    "    def identify_q_core(self, qls):\n",
    "        for i in range(len(qls)):\n",
    "            if qls[i] and qls[i][0][1] == 'CORE':\n",
    "                return map(lambda c: c[0], qls[i])\n",
    "        return None\n",
    "\n",
    "    def scoring(self):\n",
    "\n",
    "        # score on duplication\n",
    "        map(lambda dic, x: self.put(dic, 'score1', self.score_ctx_dup(x)),\n",
    "            self.amask, map(lambda m: m['ctx'], self.amask))\n",
    "\n",
    "        # score on distance\n",
    "        ctxx = map(lambda m: (m['seq'], m['lem'], m['ctx']), self.amask)\n",
    "        ssq = []\n",
    "        extr = 0.0\n",
    "        for seq, lem, ctx in ctxx:\n",
    "            # if self.core and lem in\n",
    "            if ctx == '*':\n",
    "                ssq.append(seq)\n",
    "            if ctx == '**':\n",
    "                ssq.append(seq)\n",
    "                extr += 1.0\n",
    "            elif ctx != '*' and ssq:\n",
    "                map(lambda dic, xt: self.adds(dic, 'score2', self.score_exp_distrb(ssq, xt, extr)),\n",
    "                    self.amask, ctxx)\n",
    "                ssq = []\n",
    "                extr = 0.0\n",
    "\n",
    "        # score on w2v\n",
    "        lem_ls = map(lambda m: (m['lem']), self.amask)\n",
    "        map(lambda dic, xt: self.adds(dic, 'score3', w2v_model.ws_similarity([xt], self.core) * 3.5),\n",
    "            self.amask, lem_ls)\n",
    "\n",
    "        map(lambda m: self.adds(m, 'score5', self.score_type(m) * 1.0), self.amask)\n",
    "\n",
    "    def score_type(self, m):\n",
    "        if self.qa['tag'] == 'NUMBER' and m['ety1'] in ['ETY_PRCT', 'ETY_NUM', 'ETY_DATE', 'ETY_MNY']:\n",
    "            return 1.0\n",
    "        if self.qa['tag'] in ['PERSON', 'ORGANIZATION'] and m['ety1'] in ['ETY_PSN', 'ETY_ORG', 'MNE']:\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "\n",
    "    def summary(self):\n",
    "        s = 0\n",
    "        end = len(self.amask)\n",
    "        wcol = []\n",
    "        try:\n",
    "            answer = map(lambda x: x[0], self.qa['answer_tks'])\n",
    "        except:\n",
    "            answer = []\n",
    "\n",
    "        while s < end:\n",
    "\n",
    "            k = dict()\n",
    "            k['ori'] = []\n",
    "            k['ety'] = None\n",
    "            k['length'] = 0\n",
    "            k['dup'] = 0\n",
    "            k['score'] = 0.0\n",
    "\n",
    "            for i in range(s, end):\n",
    "\n",
    "                if self.amask[i]['ety1'] != '%':\n",
    "                    if not k['ety']:\n",
    "                        k['ety'] = self.amask[i]['ety1']\n",
    "                        k['length'] = self.amask[i]['enum']\n",
    "                    k['ori'].append(self.amask[i]['ori'])\n",
    "\n",
    "                    assert k['ety'] == self.amask[i]['ety1']\n",
    "                    k['score'] += self.amask[i]['score1'] + self.amask[i].get('score2', 0.0) \\\n",
    "                                  + self.amask[i]['score3'] + self.amask[i]['score5']\n",
    "\n",
    "                if self.amask[i]['ctx'] == \"*\" or self.amask[i]['ctx'] == \"**\":\n",
    "                    k['dup'] += 1\n",
    "\n",
    "                try:\n",
    "                    assert len(k['ori']) < k['length']\n",
    "                    assert self.amask[i]['ety1'] != '%'\n",
    "                except:\n",
    "                    if k['ori'] and not (k['dup'] == k['length'] == 1):\n",
    "                        k['score'] /= (k['length'] + 1.0)\n",
    "                        wcol.append(k)\n",
    "                    s = i + 1\n",
    "                    k = dict()\n",
    "                    k['ori'] = []\n",
    "                    k['ety'] = None\n",
    "                    k['length'] = 0\n",
    "                    k['dup'] = 0\n",
    "                    k['score'] = 0\n",
    "\n",
    "        wcol = sorted(wcol, key=lambda x: x['score'], reverse=True)\n",
    "        ans_pool = map(lambda x: x['ori'], wcol)\n",
    "        # prepare for csv results\n",
    "        results = dict()\n",
    "        pred_ans = []\n",
    "        if ans_pool:\n",
    "            for j in ans_pool[0]:\n",
    "                if j == ',':\n",
    "                    pred_ans.append('-COMMA-')\n",
    "                elif j == '\\\"':\n",
    "                    pass\n",
    "                else:\n",
    "                    pred_ans.append(j)\n",
    "\n",
    "        try:\n",
    "            results['id'] = str(self.qa['id'])\n",
    "            results['answer'] = ' '.join(pred_ans)\n",
    "        except:\n",
    "            results['q'] = self.qa['question']\n",
    "            results['a'] = str(self.qa['answer_tks'])\n",
    "            results['apool'] = str(ans_pool)\n",
    "            results['cort'] = str(int(answer in ans_pool))\n",
    "\n",
    "        if ans_pool:\n",
    "            if answer in ans_pool and answer != ans_pool[0]:\n",
    "                llllllllll = 0\n",
    "            if answer not in ans_pool:\n",
    "                kkkkkkkkkkk = 0\n",
    "            return answer == ans_pool[0], answer in ans_pool, results\n",
    "        else:\n",
    "            return False, False, results\n",
    "\n",
    "    def hlt(self, sec_half, bp, xt, aa=2.0):\n",
    "        seq = xt['seq']\n",
    "\n",
    "        if sec_half and seq > bp:\n",
    "            x = seq - bp - 1\n",
    "        elif (not sec_half) and seq < bp:\n",
    "            x = bp - seq - 1\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "        og = 1.0\n",
    "\n",
    "        f = aa ** 1.0 / math.sqrt(2.0 * og ** 2.0 * math.pi) * math.pow(math.e, -x ** 2 / (2 * og ** 2))\n",
    "        return f\n",
    "\n",
    "    def score_exp_distrb(self, range, xt, extra=0.0):\n",
    "        lo, hi = range[0], range[-1]\n",
    "        seq, _, ctx = xt\n",
    "        # pps = math.fabs(self.a_length/2.0 - seq) / math.sqrt(self.a_length)\n",
    "\n",
    "        if seq > hi:\n",
    "            x = seq - hi - 1\n",
    "        elif seq < lo:\n",
    "            x = lo - seq - 1\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "        a = float(hi - lo) + extra\n",
    "        og = 1.0\n",
    "        # print og, a\n",
    "\n",
    "        f = a ** 1.0 / math.sqrt(2.0 * og ** 2.0 * math.pi) * math.pow(math.e, -x ** 2 / (2 * og ** 2))\n",
    "        return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Module 5.4. Report on QA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class QAReport:\n",
    "    def __init__(self, dev, qass, report_file='answer.csv'):\n",
    "        self.correct = 0\n",
    "        self.prog_i = 0.0\n",
    "        self.prog_total = len(qass)\n",
    "\n",
    "        file_csv = open(report_file, 'w')\n",
    "        if dev:\n",
    "            field_names = ['cort', 'a', 'apool', 'q']\n",
    "        else:\n",
    "            field_names = ['id', 'answer']\n",
    "        self.writer = csv.DictWriter(file_csv, fieldnames=field_names)\n",
    "        self.writer.writeheader()\n",
    "\n",
    "    def guess(self, qa, ):\n",
    "\n",
    "        qa_process = QA(qa)\n",
    "        a, b, c = qa_process.summary()\n",
    "\n",
    "        self.prog_i += 1\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"%f%%\" % (self.prog_i * 100.0 / self.prog_total))\n",
    "        sys.stdout.flush()\n",
    "        self.writer.writerow({k: v.encode('utf8') for k, v in c.items()})\n",
    "\n",
    "        return a, b\n",
    "\n",
    "    def report(self):\n",
    "        pre = map(self.guess, qass[0:])\n",
    "\n",
    "        print ()\n",
    "\n",
    "        print sum(map(lambda x: x[0], pre)), 'correct'\n",
    "\n",
    "        print sum(map(lambda x: x[1], pre)), 'in pool'\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Prepare data and build for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/ source:', 'QA_train.json', '/ simplified:', True, '/ purpose', 'dev')\n",
      "100.000000%('/Users/qilong/OneDrive - The University of Melbourne/COMP90042 Web Search and Text Analysis/Project/Final Version//xtrain_simple_cb3.dev.json', 'is now prepared')\n",
      "('EXEC: ', 1179.382751941681)\n",
      "('/ source:', 'QA_dev.json', '/ simplified:', False, '/ purpose', 'dev')\n",
      "100.000000%('/Users/qilong/OneDrive - The University of Melbourne/COMP90042 Web Search and Text Analysis/Project/Final Version//xdev_standard_cb7.dev.json', 'is now prepared')\n",
      "('EXEC: ', 344.3170700073242)\n",
      "building dataset..\n",
      "('Finish initialization:', 73.77411007881165)\n",
      "starting..\n",
      "----------------------------------------------------------------------------------------------------\n",
      "classifier:\n",
      "Combined ########\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "macro f1 score:   0.497\n",
      "accuracy:   0.633 \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    LOCATION       0.32      0.38      0.35       470\n",
      "      NUMBER       0.71      0.78      0.75      2037\n",
      "           O       0.70      0.76      0.73      4007\n",
      "ORGANIZATION       0.48      0.13      0.20      1365\n",
      "      PERSON       0.38      0.60      0.46       584\n",
      "\n",
      " avg / total       0.62      0.63      0.61      8463\n",
      "\n",
      "()\n",
      "<function confusion_matrix at 0x10a674500>\n",
      "()\n",
      "('EXEC:', 23.7292001247406)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63287250384024574"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainning set: query + answer ner tag\n",
    "train_cb3_simple_dev = 'xtrain_simple_cb3.dev.json'\n",
    "if os.path.exists(train_cb3_simple_dev):\n",
    "    train_cb3_simple_dev = get_dataset(train_cb3_simple_dev)\n",
    "    print('xtrain_simple_cb3.dev.json', 'ready!')\n",
    "else:\n",
    "    qA_train_cb3 = Prepare('QA_train.json', classification='3class')\n",
    "    train_cb3_simple_dev = qA_train_cb3.launch(simplified=True, purpose='dev', file_name=cwd+'xtrain_simple_cb3.dev.json')\n",
    "\n",
    "# ner processed dev set, contains 7class ner tag (question sent / ans sent)\n",
    "dev_cb7_standard_dev = 'xdev_standard_cb7.dev.json'\n",
    "if os.path.exists(dev_cb7_standard_dev):\n",
    "    dev_cb7_standard_dev = get_dataset(dev_cb7_standard_dev)\n",
    "    print('xdev_standard_cb7.dev.json', 'ready!')\n",
    "else:\n",
    "    qA_dev_cb7 = Prepare('QA_dev.json', classification='7class')\n",
    "    dev_cb7_standard_dev = qA_dev_cb7.launch(simplified=False, purpose='dev', file_name=cwd+'xdev_standard_cb7.dev.json')\n",
    "\n",
    "classification3 = ClassificationBuild(train_data=train_cb3_simple_dev, test_data=dev_cb7_standard_dev,\n",
    "                                          nclass='3class')\n",
    "classification3.build_model_and_evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Output QA results on dev set (BM25 tagged sent, ML classifier tagged qtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/ source:', 'QA_dev.json', '/ simplified:', False, '/ purpose', 'test')\n",
      "100.000000%('xdev_standard_cb7.test.json', 'is now prepared')\n",
      "('EXEC: ', 365.89133286476135)\n",
      "100.000000%()\n",
      "1460 correct\n",
      "2896 in pool\n"
     ]
    }
   ],
   "source": [
    "# Given the BM25 prediction, ML tagged qtype; run answer extraction on dev set\n",
    "dev_bm25_qtag_c3 = 'bm25_qtag_c3_dev.json'\n",
    "if os.path.exists(dev_bm25_qtag_c3):\n",
    "    dev_bm25_qtag_c3 = get_dataset(dev_bm25_qtag_c3)\n",
    "    print('bm25_qtag_c3_dev.json', 'ready!')\n",
    "else:\n",
    "    dev_cb7_standard_test = 'xdev_standard_cb7.test.json'\n",
    "    if os.path.exists(dev_cb7_standard_test):\n",
    "        dev_cb7_standard_test =get_dataset(dev_cb7_standard_test)\n",
    "    else:\n",
    "        qA_dev_cb7 = Prepare('QA_dev.json', classification='7class')\n",
    "        dev_cb7_standard_test = qA_dev_cb7.launch(simplified=False, purpose='test',\n",
    "                                                  file_name='xdev_standard_cb7.test.json')\n",
    "    dev_bm25_qtag_c3 = classification3.tag_prediction(dataset=dev_cb7_standard_test,\n",
    "                                                      out_file_name='bm25_qtag_c3_dev.json')\n",
    "\n",
    "qass = QASS(dataset=dev_bm25_qtag_c3).get_qass(enbble_actag=True)\n",
    "QAReport(dev=True, qass=qass).report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Output QA results on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('xtest_standard_cb7.test.json', 'ready!')\n",
      "('bm25_qtag_c3_test.json', 'ready!')\n",
      "100.000000%()\n",
      "0 correct\n",
      "0 in pool\n"
     ]
    }
   ],
   "source": [
    "# Given the BM25 prediction, ML tagged qtype; run answer extraction on test set\n",
    "# This is what submitted to kaggle\n",
    "\n",
    "# ner and BM25 processed dev set, contains 7class ner tag (question sent / ans sent)\n",
    "test_cb7_standard = 'xtest_standard_cb7.test.json'\n",
    "if os.path.exists(test_cb7_standard):\n",
    "    test_cb7_standard = get_dataset(test_cb7_standard)\n",
    "    print'xtest_standard_cb7.test.json', ' is ready!'\n",
    "else:\n",
    "    qA_test_cb7 = Prepare('QA_test.json', classification='7class')\n",
    "    test_cb7_standard = qA_test_cb7.launch(simplified=False, purpose='test', file_name='xtest_standard_cb7.test.json')\n",
    "\n",
    "#\n",
    "test_bm25_qtag_c3 = 'bm25_qtag_c3_test.json'\n",
    "if os.path.exists(test_bm25_qtag_c3):\n",
    "    test_bm25_qtag_c3 = get_dataset(test_bm25_qtag_c3)\n",
    "    print'bm25_qtag_c3_test.json', ' is ready!'\n",
    "else:\n",
    "    test_bm25_qtag_c3 = classification3.tag_prediction(dataset=test_cb7_standard, \n",
    "                                                       out_file_name='bm25_qtag_c3_test.json')\n",
    "qass = QASS(dataset=test_bm25_qtag_c3).get_qass(enbble_actag=False)\n",
    "QAReport(dev=False, qass=qass).report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
