{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math,numpy,json,re,nltk,csv,time,re,os.path,sys,ast,itertools,string\n",
    "from operator import add\n",
    "from math import sqrt\n",
    "from numpy import multiply\n",
    "from nltk import FreqDist, DictionaryProbDist\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "from nltk.tag import StanfordNERTagger,StanfordPOSTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict,OrderedDict\n",
    "from nltk import RegexpParser\n",
    "\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful \n",
      "There are totally 40 documents in this dev dataset\n",
      "There are totally 42 documents in this test dataset\n",
      "There are totally 360 documents in this  train dataset\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_path = os.path.abspath('QA_test.json')\n",
    "dev_path = os.path.abspath('QA_dev.json')\n",
    "train_path = os.path.abspath('QA_train.json')\n",
    "\n",
    "def load_jsonfile(filepath):\n",
    "    dataset = []\n",
    "    with open(filepath) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            dataset += (json.loads(line.encode('utf-8')))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_dev = load_jsonfile(dev_path)\n",
    "dataset_test = load_jsonfile(test_path)\n",
    "dataset_train = load_jsonfile(train_path)\n",
    "print \"Import Successful \"\n",
    "print \"There are totally\", len(dataset_dev),'documents in this dev dataset'\n",
    "print \"There are totally\", len(dataset_test),'documents in this test dataset'\n",
    "print \"There are totally\", len(dataset_train),'documents in this  train dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build tf-idf model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer() \n",
    "from nltk import word_tokenize\n",
    "def my_tokenizer(doc):\n",
    "    terms = set()\n",
    "    for token in word_tokenize(doc):\n",
    "        if token not in stopwords and token not in string.punctuation: \n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return list(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tf_idf_Model:\n",
    "    \"\"\"\n",
    "    This model use tf_idf socoring basis,\n",
    "    the parameter  tf_idf vevtorizer are \n",
    "    ‣ max_df=0.95, min_df=1, log of the terms range \n",
    "    ‣ norm='l2' chosee the normalisation\n",
    "    ‣ tokenizer: use own build tokenier\n",
    "    \"\"\"\n",
    "    def __init__(self, collection):\n",
    "        self.vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, use_idf=True,norm='l2',stop_words=None, tokenizer=my_tokenizer)\n",
    "        self.collection_matrix = self.vectorizer.fit_transform(document_collections)\n",
    "        feature_array = self.vectorizer.get_feature_names()\n",
    "        self.features = dict()\n",
    "        for index in range(len(feature_array)):\n",
    "            term = feature_array[index]\n",
    "            self.features[term] = index\n",
    "\n",
    "    def predict(self, queryX,limit=3):\n",
    "        predictions = [self.inverted_index_score(i,limit) for i in  queryX]\n",
    "        return predictions\n",
    "\n",
    "    def inverted_index_score(self, query_sent,limit=3):\n",
    "        query_words = my_tokenizer(query_sent)\n",
    "        score = defaultdict(float)\n",
    "\n",
    "        for w in query_words:\n",
    "            try:\n",
    "                col_i = self.features[w]\n",
    "                inverted_ix = self.collection_matrix[:, col_i]\n",
    "                for doc_i in range(inverted_ix.shape[0]):\n",
    "                    score[doc_i] += inverted_ix[doc_i, 0]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        index_score = sorted(score.items(), key=lambda (k, v): v, reverse=True)\n",
    "\n",
    "        if index_score:\n",
    "            doc_indexs = [i[0] for i in index_score[:limit]]\n",
    "            return doc_indexs\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25_Model(object): \n",
    "    \"\"\"\n",
    "    this mode use Okapi BM25 calculation basis,retrival from IDF\n",
    "    ‣ k1  = controls the weight of the length document\n",
    "    ‣ b = controls length based term to reward high frequency terms in shorter documents\n",
    "    ‣ k3 = modulates between binary occurrence and query frequency count\n",
    "    \"\"\"\n",
    "    def __init__(self, document_collection, k1=1.5, b=0.75, k3=0.0, EPS=0.25, tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.document_collection_length = len(document_collection) \n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length \n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_freq = FreqDist(self.document_corpus)\n",
    "        self.tf = [] \n",
    "        self.df = defaultdict(int) \n",
    "        self.bm25_idf = defaultdict(float) \n",
    "        self.average_idf = -1 \n",
    "        self.k1 = k1 \n",
    "        self.k3 = k3 \n",
    "        self.EPSILON = EPS \n",
    "        self.b = b \n",
    "        self.inverted_index = defaultdict(list) \n",
    "        self.initialize() \n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection): \n",
    "            doc_term_freq = FreqDist(document) \n",
    "            self.tf.append(doc_term_freq) \n",
    "            \n",
    "            for word, freq in doc_term_freq.items(): \n",
    "                self.df[word] += 1 \n",
    "                self.inverted_index[word].append(index) \n",
    "        for word, freq in self.df.items(): \n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5) \n",
    "            \n",
    "            self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys()) \n",
    "    \n",
    "    def predict(self, queryX, limit=1): \n",
    "        q_prediction = [] \n",
    "        for query in queryX: \n",
    "            answers = self.bm25_get_most_relevant(query)[:limit] \n",
    "            if len(answers) == 0:\n",
    "                q_prediction.append([]) \n",
    "            else:\n",
    "                q_prediction.append([i[0] for i in answers]) \n",
    "        return q_prediction \n",
    "\n",
    "    def bm25_get_most_relevant(self, query): \n",
    "        query_tks = self.tokenizer(query) \n",
    "        scores = defaultdict(float) \n",
    "        for q_token in query_tks: \n",
    "            for doc_index in self.inverted_index[q_token]: \n",
    "                idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf \n",
    "                top = self.tf[doc_index][q_token] * (self.k1 + 1) \n",
    "                below = self.tf[doc_index][q_token] + self.k1 * (1 - self.b + self.b * self.document_collection_length / self.avg_doc_length) \n",
    "                frq_q_t = self.corpus_freq[q_token]\n",
    "                scores[doc_index] += idf * top / below *(self.k3 +1)*frq_q_t/(self.k3+frq_q_t)\n",
    "        prels = scores.items() \n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True) \n",
    "        return sorted_socres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LM_Model:\n",
    "    \"\"\"\n",
    "    Language Model \n",
    "    Smoothing Way:Dirichlet smoothing\n",
    "    ‣ Alpha\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,documents,a = 0.5 ,tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer  \n",
    "        self.alpha = a\n",
    "        self.document_collection = [self.tokenizer(doc) for doc in documents] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_term_prob = {}\n",
    "        self.corpus_term_freq = FreqDist(self.document_corpus)\n",
    "        self.vocabulary = self.corpus_term_freq.keys()\n",
    "        self.lmp = []\n",
    "        self.initialize()\n",
    "    def initialize(self): \n",
    "        document_freq = [FreqDist(doc) for doc in self.document_collection]\n",
    "        length_corpus = len(self.document_corpus)\n",
    "        for term,occurs in self.corpus_term_freq.items():\n",
    "            self.corpus_term_prob[term] = float(occurs)/float(length_corpus)\n",
    "        for sent_freq in document_freq:\n",
    "            tempDict = {}\n",
    "            for term in self.vocabulary:\n",
    "                upper = sent_freq.get(term,0) + self.alpha*self.corpus_term_prob.get(term,0)\n",
    "                below = self.corpus_term_freq.get(term,0) + self.alpha\n",
    "                tempDict[term] = float(upper)/float(below)\n",
    "            self.lmp.append(tempDict)\n",
    "    def get_lm_socres(self,Query):\n",
    "        doc_socres = []\n",
    "        Query = my_tokenizer(Query)\n",
    "        for doc_prob in  self.lmp:\n",
    "            term_score = []\n",
    "            for term in Query:\n",
    "                if term in self.vocabulary:\n",
    "                    term_score.append(doc_prob[term])\n",
    "            query_score = numpy.product(term_score)\n",
    "            doc_socres.append(query_score)\n",
    "        sorted_score = sorted(list(enumerate(doc_socres)), key=lambda (k,v): v, reverse=True)\n",
    "        doc_indexs = [i for i in sorted_score]\n",
    "        return doc_indexs\n",
    "    def predict(self,questions,limit = 3):\n",
    "        predictions = [] \n",
    "        for query in questions: \n",
    "            answers = self.get_lm_socres(query)[:limit] \n",
    "            predictions.append([i[0] for i in answers]) \n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Various Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_csv(csv_name,model_name,dataset,limit=1):\n",
    "    csv_file = open(csv_name, mode='w',)\n",
    "    fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, )\n",
    "    writer.writeheader()\n",
    "\n",
    "    ddi = 0\n",
    "    for document in dataset:\n",
    "        evaluate_row = []\n",
    "        questions = [i['question'] for i in document['qa']]\n",
    "        model = document[model_name]\n",
    "        predictions = model.predict(questions,limit)\n",
    "        quest_index = 0\n",
    "        for pred_index in predictions:\n",
    "            drow = dict()\n",
    "            drow['question_ID'] = quest_index\n",
    "            drow['prediction_ID'] = pred_index\n",
    "            evaluate_row.append(drow)\n",
    "            quest_index += 1\n",
    "        doc_sents = document['sentences']\n",
    "        for r in evaluate_row:\n",
    "            r['document_ID'] = ddi\n",
    "            r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "            if len(r['prediction_ID']) != 0:\n",
    "                r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "            else:\n",
    "                print 'error prediction',ddi,r['question_ID'],r['question']\n",
    "            writer.writerow(r)\n",
    "        ddi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are errors made by different models,doucnment Index,Query Index, Query\n",
      "\n",
      "error prediction 11 317 Who was the runner up?\n",
      "error prediction 11 520 Who was the runner up?\n",
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 22 94 What is an Etsudiantinas? \n",
      "error prediction 22 181 What is crosspicking?\n",
      "error prediction 25 245 When was Chanakya alive?\n",
      "----------------------------------------------------\n",
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 25 245 When was Chanakya alive?\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#build model for each document collaction\n",
    "for document in dataset_dev:\n",
    "    document_collections = document['sentences']\n",
    "    document['tf_idf_model'] = tf_idf_Model(document_collections)\n",
    "    document['bm25_model'] = BM25_Model(document_collections)\n",
    "    document['lm_model'] = LM_Model(document_collections)\n",
    "print 'The following are errors made by different models,doucnment Index,Query Index, Query'\n",
    "print ''\n",
    "write_csv('tf_idf_dev_predictions.csv','tf_idf_model',dataset_dev,1)\n",
    "print '----------------------------------------------------'\n",
    "write_csv('bm25_dev_predictions.csv','bm25_model',dataset_dev,1)\n",
    "print '----------------------------------------------------'\n",
    "write_csv('lm_dev_predictions.csv','lm_model',dataset_dev,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import different result and compare predictions from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_prediction_accuracy(filename):\n",
    "    preds = []\n",
    "    with open(filename) as csvfile:\n",
    "        readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            tempDict = {}\n",
    "            tempDict['DocID'] = int(row['document_ID'])\n",
    "            tempDict['Predictions'] = ast.literal_eval(row['prediction_ID'])\n",
    "            tempDict['QuestionIndex'] = int(row['question_ID'])\n",
    "            tempDict['Question'] = row['question']\n",
    "            preds.append(tempDict)\n",
    "    count = 0\n",
    "    bingo = 0\n",
    "    for d in preds:\n",
    "        guess = d['Predictions'] \n",
    "        doc_i = d['DocID']\n",
    "        qus_i = d['QuestionIndex']\n",
    "        act_i = dataset_dev[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "        if act_i in guess:\n",
    "            bingo += 1\n",
    "        count += 1\n",
    "    print \"Model correctness results :\",float(bingo)/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model correctness results : 0.584308164953\n",
      "Model correctness results : 0.647879002718\n",
      "Model correctness results : 0.638071605814\n"
     ]
    }
   ],
   "source": [
    "check_prediction_accuracy('tf_idf_dev_predictions.csv')\n",
    "check_prediction_accuracy('bm25_dev_predictions.csv')\n",
    "check_prediction_accuracy('lm_dev_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose BM25  Model and predict sentence  for test set\n",
    "* in this case BM25 will be used for sentence retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error prediction 4 95 What does ICRISTAT stand for?\n",
      "error prediction 9 75 What is okanye?\n",
      "error prediction 10 46 What was destroyed in the fire?\n",
      "error prediction 10 118 What is the Barengraben?\n",
      "error prediction 17 67 What are phrynges?\n",
      "error prediction 19 74 What happeded between 1927-1934?\n",
      "error prediction 20 106 What is 'gorg'?\n",
      "error prediction 36 107 When was the overbudget amount discovered?\n"
     ]
    }
   ],
   "source": [
    "#Build models for test data set\n",
    "for document in dataset_test:\n",
    "    document_collections = document['sentences']\n",
    "    document['bm25_model'] = BM25_Model(document_collections)\n",
    "# write to a CSV file for test data predictions\n",
    "write_csv('bm25_test_predictions.csv','bm25_model',dataset_test,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "NERtagger = StanfordNERTagger(cwd+'/english.all.3class.distsim.crf.ser.gz',cwd+'/stanford-ner.jar')\n",
    "POStagger = StanfordPOSTagger(cwd+'/wsj-0-18-left3words-distsim.tagger',cwd+'/stanford-postagger-3.7.0.jar') \n",
    "\n",
    "def NERandPOS_writer(filenameNER,filenamePOS,dataset,l):\n",
    "    if not os.path.isfile(filenameNER):    \n",
    "        start = time.time()\n",
    "        progressT = len(dataset)    \n",
    "        listOfDocumentNER=[]\n",
    "        listOfDocumentPOS=[]\n",
    "        i=0\n",
    "        for jd in dataset:\n",
    "            aListNER=[]\n",
    "            aListPOS=[]\n",
    "            aListNER.append(NERtagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))            \n",
    "            aListPOS.append(POStagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))\n",
    "            if l!=1:\n",
    "                aListNER.append(NERtagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['answer']))) for x in jd['qa']]))\n",
    "                aListPOS.append(POStagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['answer']))) for x in jd['qa']]))                         \n",
    "            aListPOS.append(POStagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "            aListNER.append(NERtagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "            listOfDocumentNER.append(aListNER)\n",
    "            listOfDocumentPOS.append(aListPOS)\n",
    "            i+=1\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"Processing: %d%%\" % (i*100/progressT))\n",
    "            sys.stdout.flush()    \n",
    "        if l!=1:\n",
    "            for document in range(0,len(listOfDocumentNER)):\n",
    "                for sentence in range(0,len(listOfDocumentNER[document][1])):\n",
    "                    for word in range(0,len(listOfDocumentNER[document][1][sentence])):   \n",
    "                        listOfDocumentNER[document][1][sentence][word]= \\\n",
    "                        (listOfDocumentNER[document][1][sentence][word][0],\\\n",
    "                         listOfDocumentNER[document][1][sentence][word][1] \\\n",
    "                         if not listOfDocumentNER[document][1][sentence][word][0].isdigit() else u'NUMBER')\n",
    "        with open(filenameNER, 'w') as outfile:\n",
    "            json.dump(listOfDocumentNER, outfile)\n",
    "        with open(filenamePOS, 'w') as outfile:\n",
    "            json.dump(listOfDocumentPOS, outfile)\n",
    "        end = time.time()\n",
    "        print '\\nTime spending:',end - start    \n",
    "    else:    \n",
    "        print filename,'is alrady exist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 100%\n",
      "Time spending: 353.942639112\n"
     ]
    }
   ],
   "source": [
    "NERandPOS_writer('NERtest.json','POStest.json',dataset_test,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Answer rank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Ranking Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class answer_rank():\n",
    "    def __init__(self,json_data,json_dataOrg,json_dataPOS,json_dataPOSOrg):\n",
    "        self.json_data = json_data\n",
    "        self.json_dataOrg = json_dataOrg\n",
    "        self.json_dataPOS = json_dataPOS\n",
    "        self.json_dataPOSOrg = json_dataPOSOrg\n",
    "        self.dictDoc = {}\n",
    "        self.dictDocOrg = {}\n",
    "        self.initialize()\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "\n",
    "        for document in range(len(self.json_data)):\n",
    "            for thing in range(len(self.json_data[document])):\n",
    "                for sentence in range(len(self.json_data[document][thing])):\n",
    "                    for word in range(len(self.json_data[document][thing][sentence])):\n",
    "                        self.json_data[document][thing][sentence][word][1] = 'U'\\\n",
    "                        if word!=0 and self.json_data[document][thing][sentence][word][0][0].isupper()\\\n",
    "                        and self.json_data[document][thing][sentence][word][1]=='O'\\\n",
    "                        else self.json_data[document][thing][sentence][word][1]\n",
    "                        \n",
    "                        self.json_data[document][thing][sentence][word][0] = self.json_data[document][thing][sentence][word][0].lower()\n",
    "        \n",
    "                        if self.json_data[document][thing][sentence][word][0] \\\n",
    "                        in ['one','two','three','four','five','six','seven','eight','nine','ten','zero']\\\n",
    "                        or self.isfloat(self.json_data[document][thing][sentence][word][0]):\n",
    "                            self.json_data[document][thing][sentence][word][1] = 'NUMBER'\n",
    "        for document in range(len(self.json_dataPOS)):\n",
    "            for thing in range(len(self.json_dataPOS[document])):\n",
    "                for sentence in range(len(self.json_dataPOS[document][thing])):\n",
    "                    for word in range(len(self.json_dataPOS[document][thing][sentence])):\n",
    "                        self.json_dataPOS[document][thing][sentence][word][0] = self.json_dataPOS[document][thing][sentence][word][0].lower()\n",
    "                        if self.have_number(self.json_dataPOS[document][thing][sentence][word][0]):\n",
    "                            self.json_dataPOS[document][thing][sentence][word][1] = 'CD'\n",
    "        print 'NER json file import to system successful'\n",
    "        \n",
    "    \n",
    "    def have_number(self,s):\n",
    "        return any(i.isdigit() for i in s)\n",
    "\n",
    "    def isfloat(self,value):\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def readCSV(self,filename,devdata = False):\n",
    "        csv_file = filename\n",
    "        i=0\n",
    "        answerSecondFilter = []\n",
    "        dictDocOrg={}\n",
    "        t_i = 1\n",
    "        if devdata:\n",
    "            t_i = 2            \n",
    "        with open(csv_file, 'rb') as csvfile:\n",
    "            readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "            for row in readCSV:\n",
    "                document_i = int(row['document_ID'])\n",
    "                question_i = int(row['question_ID'])\n",
    "                filteredlistOfNERSentence = []\n",
    "                question_type = []\n",
    "                predictionList = ast.literal_eval(row['prediction_ID'])\n",
    "                prediction_index = predictionList[0] if len(predictionList)!=0 else 1\n",
    "                question_type = self.detectQuestion(document_i,question_i,prediction_index,t_i)\n",
    "                self.dictDoc[document_i,question_i]=prediction_index,question_type                                              \n",
    "\n",
    "\n",
    "\n",
    "    def detectQuestion(self,i, j, k, l):\n",
    "        openclassword=[]\n",
    "        kindOfAnswer = []\n",
    "        questionPOS = self.json_dataPOS[i][0][j]\n",
    "        questionNER = self.json_data[i][0][j]        \n",
    "        answerPOS = self.json_dataPOS[i][l][k]\n",
    "        answerNER = self.json_data[i][l][k]\n",
    "        specialcommand=[]\n",
    "        #print questionNER, answerPOS\n",
    "        originalWithout = [x[0] for x in questionPOS]\n",
    "        originalWithoutA = [x[0] for x in answerPOS]\n",
    "        getIndexOfWH = [y for y,x in enumerate(questionPOS) if 'W' in x[1]]\n",
    "        #if wh\n",
    "        if len(getIndexOfWH) != 0:\n",
    "            getPOS = ''.join([x[1][0] for x in questionPOS])        \n",
    "\n",
    "            #not at last word\n",
    "            if getIndexOfWH[0]+1!=len(questionPOS):            \n",
    "                searchWordAfterWh1 = re.search('W(.*?)V', getPOS, re.IGNORECASE)\n",
    "                if searchWordAfterWh1:\n",
    "                    if len(searchWordAfterWh1.group(1))!=0:\n",
    "                        openclassword.append([searchWordAfterWh1.start()+len(searchWordAfterWh1.group(1))])                    \n",
    "                    else:\n",
    "                        openclassword.append([])\n",
    "                else:\n",
    "                    searchWordAfterWh1 = re.search('W(.*?)', getPOS, re.IGNORECASE)\n",
    "                    if searchWordAfterWh1:\n",
    "                        openclassword.append([searchWordAfterWh1.start()+len(searchWordAfterWh1.group(0))-1])\n",
    "                    else:\n",
    "                        openclassword.append([])\n",
    "                frontPart = range(0,searchWordAfterWh1.start()) if searchWordAfterWh1 else range(0,len(getPOS)/2)\n",
    "                backPart = range(searchWordAfterWh1.start()+len(searchWordAfterWh1.group(0)),len(getPOS)) if searchWordAfterWh1 else range(len(getPOS)/2,len(getPOS))                      \n",
    "                openclassword.append(frontPart+backPart)      \n",
    "                #print openclassword\n",
    "            else:\n",
    "                openclassword=[[],[y for y,x in enumerate(questionPOS)][:-1]]                        \n",
    "            #remove Stopwords        \n",
    "            openclassword[1] = [x for x in openclassword[1] if questionPOS[x][0] not in stopwords]\n",
    "            if len(openclassword[0])!=0:            \n",
    "                numberIndicator = ['year','length','percentage', 'many','much']            \n",
    "                for x in numberIndicator: \n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['NUMBER'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                            specialcommand.append(x)\n",
    "                personIndicator = ['name']\n",
    "                for x in personIndicator:\n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['PERSON','ORGANIZATION'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                placeIndicator = ['location','place','country','city','area']\n",
    "                for x in placeIndicator:\n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['LOCATION'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                            specialcommand=['location']\n",
    "                if len(kindOfAnswer)==0:    \n",
    "                    kindOfAnswer = ['O','U']\n",
    "            else:\n",
    "                numberIndicator = ['when']            \n",
    "                personIndicator = ['who','whom','whose','name']\n",
    "                placeIndicator = ['where','located']\n",
    "                if originalWithout[getIndexOfWH[0]] in numberIndicator:\n",
    "                    kindOfAnswer = ['NUMBER']\n",
    "                    specialcommand = ['year']\n",
    "                elif originalWithout[getIndexOfWH[0]] in personIndicator:\n",
    "                    kindOfAnswer = ['PERSON','ORGANIZATION']\n",
    "                elif originalWithout[getIndexOfWH[0]] in placeIndicator:\n",
    "                    kindOfAnswer = ['LOCATION']\n",
    "                    specialcommand=['location']\n",
    "                else:\n",
    "                    if len([y for y in openclassword[1] if questionPOS[y][0] in personIndicator])!=0:\n",
    "                        kindOfAnswer = ['PERSON','ORGANIZATION']\n",
    "                    elif len([y for y in openclassword[1] if questionPOS[y][0] in placeIndicator])!=0:\n",
    "                        kindOfAnswer = ['LOCATION']\n",
    "                        specialcommand=['location']\n",
    "                    else: \n",
    "                        kindOfAnswer = ['O','U']\n",
    "        else:\n",
    "            openclassword = [[],[x for x,y in enumerate(originalWithout) if y not in stopwords]]\n",
    "            kindOfAnswer = ['O','U']\n",
    "        #determine whether it requires number entity\n",
    "        newList1=[]\n",
    "        newList2=[]\n",
    "        for x in range(len(openclassword[0])):\n",
    "            if originalWithout[openclassword[0][x]] in originalWithoutA:            \n",
    "                newList1.extend([f for f,h in enumerate(originalWithoutA) if originalWithout[openclassword[0][x]] == h])        \n",
    "        for x in range(len(openclassword[1])):\n",
    "            if originalWithout[openclassword[1][x]] in originalWithoutA:\n",
    "                newList2.extend([f for f,h in enumerate(originalWithoutA) if originalWithout[openclassword[1][x]] == h])\n",
    "        openclassword[0]=newList1\n",
    "        openclassword[1]=newList2\n",
    "        return openclassword, kindOfAnswer,specialcommand     \n",
    "\n",
    "    def createNP(self,answerToReturn,answerPOS,specialcommand,q,j):\n",
    "        newAnswer = answerPOS[answerToReturn[1]][0]\n",
    "        if (newAnswer.isdigit() and 'year' not in specialcommand):\n",
    "            newAnswer = \"{:,}\".format(int(answerPOS[answerToReturn[1]][0]))\n",
    "        for i in range(answerToReturn[1],0,-1):\n",
    "            if answerToReturn[1] != 0:\n",
    "                if answerPOS[i][1] =='NNP':\n",
    "                    if 'NNP' in answerPOS[i-1][1] or (('DT' in answerPOS[i-1][1] or 'IN' in answerPOS[i-1][1]) \\\n",
    "                                                      and answerPOS[i-1][0] !='at' \\\n",
    "                                                      and 'location' in specialcommand):\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NN':\n",
    "                    if 'JJ' in answerPOS[i-1][1] or 'DT' in answerPOS[i-1][1] \\\n",
    "                    or 'CD' in answerPOS[i-1][1] or answerPOS[i-1][1] =='NN':\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='JJ':\n",
    "                    if answerPOS[i-1][1] =='JJ':\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NNS':\n",
    "                    if 'JJ' in answerPOS[i-1][1]:\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif 'DT' in answerPOS[i][1] and 'location' not in specialcommand:\n",
    "                    if 'TO' in answerPOS[i-1][1]:\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if answerPOS[i][1] !='CD' and answerPOS[i][1] !='RB':\n",
    "                        if 'NN' in answerPOS[i-1][1] or 'JJ' in answerPOS[i-1][1] or 'RB' in answerPOS[i-1][1]:\n",
    "                            newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        for i in range(answerToReturn[1],len(answerPOS)-1):\n",
    "            if answerToReturn[1] != len(answerPOS)-1:\n",
    "                if answerPOS[i][1] =='NNP':                \n",
    "                    if 'NNP' in answerPOS[i+1][1] or 'CC' in answerPOS[i+1][1] or 'IN' in answerPOS[i+1][1] or 'TO' in answerPOS[i+1][1] or ('NN' in answerPOS[i+1][1] and 'location' in specialcommand):                                        \n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='CC':\n",
    "                    if 'NNP' in answerPOS[i+1][1] or 'NN' == answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='CD':\n",
    "                    if 'CD' in answerPOS[i+1][1]:                 \n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='TO' or answerPOS[i][1] =='DT':\n",
    "                    if 'NN' in answerPOS[i+1][1] or 'RB' in answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break \n",
    "                elif answerPOS[i][1] =='IN' and 'location' not in specialcommand:\n",
    "                    if answerPOS[i+1][1] =='DT' or answerPOS[i+1][1] =='NNP' or answerPOS[i+1][1] =='TO' or answerPOS[i+1][1] =='CD':\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='JJ':\n",
    "                    if 'NNS' in answerPOS[i+1][1] or 'NN' == answerPOS[i+1][1] or answerPOS[i+1][1] =='JJ':\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NN':\n",
    "                    if 'NN' in answerPOS[i+1][1] or 'JJ' in answerPOS[i+1][1] or 'IN' in answerPOS[i+1][1] or 'CC' in answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NNS':\n",
    "                    if 'IN' in answerPOS[i+1][1] and 'ORG' in specialcommand:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        buffernewAnswer1 = newAnswer.split()\n",
    "        if buffernewAnswer1[-1]=='along' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-6]\n",
    "        if buffernewAnswer1[-1]=='and' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if buffernewAnswer1[-1]=='for' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if buffernewAnswer1[-1]=='but' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if buffernewAnswer1[-1]=='in' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='as' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='at' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='on' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='to' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='while' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-6]\n",
    "        if buffernewAnswer1[-1]=='despite' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-8]\n",
    "        if buffernewAnswer1[-1]=='because' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-8]\n",
    "        if buffernewAnswer1[-1].isdigit() and len(buffernewAnswer1[-1])==4 and 'year' in specialcommand:\n",
    "            newAnswer=buffernewAnswer1[-1]\n",
    "        if buffernewAnswer1[-1].isdigit() and 'location' in specialcommand:\n",
    "            newAnswer=buffernewAnswer1[-1]\n",
    "        if len(specialcommand)!=0 and 'percentage' in specialcommand[0]:\n",
    "            newAnswer+='%'\n",
    "        if 'location' in specialcommand:\n",
    "            bufferNewAnswer=[]        \n",
    "            for x in newAnswer.split():\n",
    "                for y in self.json_dataOrg[q]['sentences'][j].split():\n",
    "                    if x in y:\n",
    "                        bufferNewAnswer.append(y)\n",
    "                        break\n",
    "            newAnswer=' '.join(bufferNewAnswer)\n",
    "            newAnswer = newAnswer[:-1] if (len(newAnswer)>1 and not (newAnswer[-1].isalnum() or newAnswer[-1]=='%')) else newAnswer\n",
    "        newAnswer = newAnswer.replace(',','-COMMA-')\n",
    "        return newAnswer   \n",
    "\n",
    "    \n",
    "    def returnAnswer(self,i,j,l):\n",
    "        answerListPOS = self.json_dataPOS[i][l][self.dictDoc[i,j][0]]    \n",
    "        answerListNER = self.json_data[i][l][self.dictDoc[i,j][0]]    \n",
    "        question_type = self.dictDoc[i,j][1][1]\n",
    "        #print answerListPOS,question_type\n",
    "        question_typeLocationInAnswer = [x for x,y in enumerate(answerListNER) \\\n",
    "                                         if ('CD' in answerListPOS[answerListNER.index(y)][1] \\\n",
    "                                             or 'NN' in answerListPOS[answerListNER.index(y)][1]) \\\n",
    "                                         and y[1] in question_type and y[0] not in stopwords \\\n",
    "                                         and x not in self.dictDoc[i,j][1][0][0] \\\n",
    "                                         and x not in self.dictDoc[i,j][1][0][1]]\n",
    "        #print question_typeLocationInAnswer\n",
    "        if len(question_typeLocationInAnswer) ==0:\n",
    "            question_typeLocationInAnswer = [x for x,y in enumerate(answerListNER) \\\n",
    "                                             if ('CD' in answerListPOS[answerListNER.index(y)][1] \\\n",
    "                                                 or 'NN' in answerListPOS[answerListNER.index(y)][1]) \\\n",
    "                                             and y[0] not in stopwords and x not in self.dictDoc[i,j][1][0][0] \\\n",
    "                                             and x not in self.dictDoc[i,j][1][0][1]]\n",
    "        scoreList = 0\n",
    "        if len(self.dictDoc[i,j][1][0][0])!=0:\n",
    "            maxScore = sys.maxint        \n",
    "            for x in question_typeLocationInAnswer:\n",
    "                score = sum([math.fabs(z-x) for z in self.dictDoc[i,j][1][0][0]])\n",
    "                if maxScore>score:\n",
    "                    maxScore=score\n",
    "                    scoreList = x\n",
    "        else:\n",
    "            maxScore = sys.maxint\n",
    "            scoreList = 0\n",
    "            for x in question_typeLocationInAnswer:\n",
    "                score = sum([math.fabs(z-x) for z in self.dictDoc[i,j][1][0][1]])\n",
    "                if maxScore>score:\n",
    "                    maxScore=score\n",
    "                    scoreList = x\n",
    "                    \n",
    "        answerToReturn = (self.json_dataPOSOrg[i][l][self.dictDoc[i,j][0]][scoreList][0],scoreList)\n",
    "        answerPOS = self.json_dataPOSOrg[i][l][self.dictDoc[i,j][0]]\n",
    "        specialcommand = self.dictDoc[i,j][1][2]\n",
    "        document_index = i\n",
    "        question_index = self.dictDoc[i,j][0]\n",
    "        \n",
    "        answer = self.createNP(answerToReturn,answerPOS,specialcommand,document_index,question_index)\n",
    "        \n",
    "        return answer.encode('utf-8')\n",
    "\n",
    "    def writeToFile(self,filename,devdata = False):\n",
    "        t_i = 1\n",
    "        if devdata:\n",
    "            t_i = 2\n",
    "        with open(filename, mode='wb',) as csv_file:\n",
    "            if devdata :\n",
    "                fieldnames = ['document_id','question_id','answer_predict',\"answer_actual\",'tag','sentence','predict','question_type','question']\n",
    "            else:\n",
    "                fieldnames = ['id','answer']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames,delimiter=',')\n",
    "            writer.writeheader()\n",
    "            k = 0\n",
    "            doc_size=len(self.json_data)\n",
    "            for i in range(0, doc_size):\n",
    "                for j in range(0,len(self.json_data[i][0])):\n",
    "                    k+=1            \n",
    "                    dictToCSV={}\n",
    "                    if devdata:\n",
    "                        dictToCSV['document_id'] = i\n",
    "                        dictToCSV['question_id'] = j\n",
    "                        try:\n",
    "                            dictToCSV['answer_predict']= self.returnAnswer(i,j,t_i)\n",
    "                        except:\n",
    "                            print '\\nErrors on dev set return answers',i,j,t_i\n",
    "                        dictToCSV['answer_actual'] = self.json_dataOrg[i]['qa'][j]['answer'].encode('utf-8')\n",
    "                        dictToCSV['tag'] =self.dictDoc[i,j]\n",
    "                        dictToCSV['question_type'] = self.dictDoc[i,j][1][1]\n",
    "                        dictToCSV['question'] = self.json_dataOrg[i]['qa'][j]['question'].encode('utf-8')\n",
    "                        dictToCSV['sentence'] = self.json_dataOrg[i]['qa'][j]['answer_sentence']\n",
    "                        dictToCSV['predict'] = self.dictDoc[i,j][0]\n",
    "                    else:\n",
    "                        dictToCSV={}\n",
    "                        dictToCSV['id'] = k\n",
    "                        try:\n",
    "                            dictToCSV['answer'] = self.returnAnswer(i,j,t_i)\n",
    "                        except:\n",
    "                            print '\\nError on test return answers',i,j,t_i\n",
    "                    writer.writerow(dictToCSV)    \n",
    "                    csv_file.flush()\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write(str(k))\n",
    "                sys.stdout.flush()\n",
    "        csv_file.close()\n",
    "        print '\\nsuccess'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading JSON files for dev and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# json_data_dev = load_jsonfile('NERdev.json')\n",
    "# json_dataPOS_dev = load_jsonfile('POSdev.json')\n",
    "# json_dataOrg_dev = load_jsonfile('QA_dev.json')\n",
    "# json_dataPOSOrg_dev = load_jsonfile('POSdev.json')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data_test = load_jsonfile('NERtest.json')\n",
    "json_dataPOS_test = load_jsonfile('POStest.json')\n",
    "json_dataOrg_test = load_jsonfile('QA_test.json')\n",
    "json_dataPOSOrg_test = load_jsonfile('POStest.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER json file import to system successful\n",
      "8974\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "# qa_dev = answer_rank(json_data_dev,json_dataOrg_dev,json_dataPOS_dev,json_dataPOSOrg_dev)\n",
    "# qa_dev.readCSV(\"bm25_dev_predictions.csv\",True)\n",
    "# qa_dev.writeToFile(\"bm25_dev_result.csv\",True)\n",
    "\n",
    "qa_test = answer_rank(json_data_test,json_dataOrg_test,json_dataPOS_test,json_dataPOSOrg_test)\n",
    "qa_test.readCSV(\"bm25_test_predictions.csv\",devdata = False)\n",
    "qa_test.writeToFile(\"bm25_test_result.csv\",devdata =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 1)** Sentence Retrival:<br >\n",
    "> **Errors Found**: Errors on prediciting sentences.so there are few sentence that do not have any predictions <br>\n",
    "\n",
    "> **Reasons**: from tf-idf model there are 6 sentences which is can not produce any relevent sentences to them, after tokenzie those query.For example 'who was then runner up' , the query will left token 'runner' in this query or ecen nothing if all tokens are belong to stopwords.but the document corpus dost not exit. therefore base on current algorithm,  tf-idf model wont predict any sentencs secore back<br> \n",
    "However, thery another resons its the Unicode , some the recouse sentences might contain Latin terms, which can not match with ASCII code example of 'chāṇakya' but the query term is 'chanakya', and lastly the typo errors also might occurs, example of term 'mielno' but query term is 'milno'therore it wont reusult answers, <br>\n",
    "\n",
    ">**Imporovement**: there are two possible solutions when those query can not extract relevant sentences. first one is change model to language model. after we smooth langurage model. the wont occur zero probabbblity to predict sentences. there fore there will least some sentences will be predict. second solutions is to use semantic ways. in term of semantic, this allow models to choose different similarity word to re-score sentences, simple means when if there words in query dosent exist in corpus we substitute the key words to re-score it.\n",
    "    \n",
    "**Task 2)** Entity Retrival: <br >\n",
    "> **Errors Found**: NER provide a entity extraction rules to parse entity from sentenes which can not extract right entity or assign an wrong entity <br>\n",
    "\n",
    ">**Reasons:** In this project, NER will provide 5 types of entity 'PERSON','NUMBER','ORGANZIATON','LOCATION','OTHER'many of the query. There are alot of date numbers for example 4000 NER will automatic consider as other rather than number. Some really unusall terms cant not be classify by NER. For Example , the term phrase 'Emilio Aguinaldo and Apolinario Mabini' is extreamlly unsuall too seen as \"PERSON\",but this term is actually belong to 'PERSON'. in term '0.9–14' the punctuation will also affect the entity extractions,moreover if the term collaps between entiries, a example of 'Christian Dior' seems should be 'PERSON' , but it will allocate to 'ORGANZIATION'.\n",
    "\n",
    "> **Improvemnt** Some entity can be process by rule based. coutiounsly implemanting by human design rule to preprocessing the rules and filter out  from entity 'OTHER', but it comes ineffiency and time consuming for human filter out entity<br>\n",
    "\n",
    "**Task 3)** Answer Ranking: <br >\n",
    "> **Errors Found** on reuturning answers the errors occurs when for the focus extraction,ususally rule base ia hard to produce the right part of answer sentene even if right found <br>\n",
    "\n",
    ">**Resons** Afterwards sentence retrival, base on the rule design, for example if three 'PERSON' tags were found in answer sentences, rule will only filter out the one closet to part based on position distance.The chanllange for answer ranking is the third step to chooose the closed-class word. the varity of of answer still complicated. as example '2000 mm' can be answer for numbers  which contains units, it will incorrectly '2000'. For 'Location' same errors will comes up when \n",
    "\n",
    "> **Improvement ** : Beside to implementing rules to selcet rules, it might also evaualtion two condadiate words againest questions,therefore more effiecy way might classify the segment of sentences first and base on segament. Use the supervised model to train which one is the best math sement to match the query entity<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time is  1130.45038509\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time() - t0\n",
    "print 'Running time is ',t1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
