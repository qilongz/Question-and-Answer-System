{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math,numpy,json,re,nltk,csv,time,re,os.path,sys,gensim,ast,itertools\n",
    "from gensim import corpora\n",
    "from operator import add\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist, DictionaryProbDist\n",
    "\n",
    "#from sklearn import metrics\n",
    "#from math import sqrt\n",
    "#from collections import OrderedDict\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful \n",
      "There are totally 40 documents in this dataset\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "filename_ls = ['QA_dev.json']\n",
    "dataset = []\n",
    "train_path = os.path.abspath('data/QA_dev.json')\n",
    "\n",
    "dataset = []\n",
    "with open(train_path) as f:\n",
    "    for line in f:\n",
    "        dataset+=(json.loads(line))\n",
    "print \"Import Successful \"\n",
    "print \"There are totally\", len(dataset),'documents in this dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build tf-idf model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords and token.isalpha(): # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return list(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25_Model(object): \n",
    "    def __init__(self, document_collection, k1=1.5, b=0.75, k3=1.0, EPS=0.25, tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.document_collection_length = len(document_collection) \n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length \n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection] \n",
    "        self.tf = [] \n",
    "        self.df = defaultdict(int) \n",
    "        self.bm25_idf = defaultdict(float) \n",
    "        self.average_idf = -1 \n",
    "        self.k1 = k1 \n",
    "        self.k3 = k3 \n",
    "        self.EPSILON = EPS \n",
    "        self.b = b \n",
    "        self.inverted_index = defaultdict(list) \n",
    "        self.initialize() \n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection): \n",
    "            doc_term_freq = FreqDist(document) \n",
    "            self.tf.append(doc_term_freq) \n",
    "            \n",
    "            for word, freq in doc_term_freq.items(): \n",
    "                self.df[word] += 1 \n",
    "                self.inverted_index[word].append(index) \n",
    "        for word, freq in self.df.items(): \n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5)  \n",
    "            self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys()) \n",
    "    \n",
    "    def predict(self, queryX, limit=1): \n",
    "        q_prediction = [] \n",
    "        for query in queryX: \n",
    "            answers = self.bm25_get_most_relevant(query)[:limit] \n",
    "            q_prediction.append([i[0] for i in answers]) \n",
    "        return q_prediction \n",
    " \n",
    "\n",
    "    def bm25_get_most_relevant(self, query): \n",
    "        query_tks = self.tokenizer(query) \n",
    "        scores = defaultdict(float) \n",
    "        for q_token in query_tks: \n",
    "            for doc_index in self.inverted_index[q_token]: \n",
    "                idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf \n",
    "                top = self.tf[doc_index][q_token] * (self.k1 + 1) \n",
    "                below = self.tf[doc_index][q_token] + self.k1 * (1 - self.b + self.b * self.document_collection_length / self.avg_doc_length) \n",
    "                scores[doc_index] += idf * top / below \n",
    "        prels = scores.items() \n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True) \n",
    "        return sorted_socres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LM_Model:\n",
    "    def __init__(self,documents,a = 0.5 ,tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer  \n",
    "        self.alpha = a\n",
    "        self.document_collection = [self.tokenizer(doc) for doc in documents] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_term_prob = {}\n",
    "        self.corpus_term_freq = FreqDist(self.document_corpus)\n",
    "        self.vocabulary = self.corpus_term_freq.keys()\n",
    "        self.lmp = []\n",
    "        self.initialize()\n",
    "    def initialize(self): \n",
    "        document_freq = [FreqDist(doc) for doc in self.document_collection]\n",
    "        length_corpus = len(self.document_corpus)\n",
    "        for term,occurs in self.corpus_term_freq.items():\n",
    "            self.corpus_term_prob[term] = float(occurs)/float(length_corpus)\n",
    "        for sent_freq in document_freq:\n",
    "            tempDict = {}\n",
    "            for term in self.vocabulary:\n",
    "                upper = sent_freq.get(term,0) + self.alpha*self.corpus_term_prob.get(term,0)\n",
    "                below = self.corpus_term_freq.get(term,0) + self.alpha\n",
    "                tempDict[term] = float(upper)/float(below)\n",
    "            self.lmp.append(tempDict)\n",
    "    def get_lm_socres(self,Query):\n",
    "        doc_socres = []\n",
    "        Query = my_tokenizer(Query)\n",
    "        for doc_prob in  self.lmp:\n",
    "            term_score = []\n",
    "            for term in Query:\n",
    "                if term in self.vocabulary:\n",
    "                    term_score.append(doc_prob[term])\n",
    "            query_score = numpy.product(term_score)\n",
    "            doc_socres.append(query_score)\n",
    "        sorted_score = sorted(list(enumerate(doc_socres)), key=lambda (k,v): v, reverse=True)\n",
    "        doc_indexs = [i for i in sorted_score]\n",
    "        return doc_indexs\n",
    "    def predict(self,questions,limit = 3):\n",
    "        predictions = [] \n",
    "        for query in questions: \n",
    "            answers = self.get_lm_socres(query)[:limit] \n",
    "            prediction.append([i[0] for i in answers]) \n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_csv(csv_name,dataset,limit=1):\n",
    "    csv_file = open(csv_name, mode='w',)\n",
    "    fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, )\n",
    "    writer.writeheader()\n",
    "\n",
    "    ddi = 0\n",
    "    for document in dataset:\n",
    "        evaluate_row = []\n",
    "        questions = [i['question'] for i in document['qa']]\n",
    "        model = document['model']\n",
    "        pred = model.predict(questions,limit)\n",
    "        quest_index = 0\n",
    "        for pred_index in pred:\n",
    "            drow = dict()\n",
    "            drow['question_ID'] = quest_index\n",
    "            drow['prediction_ID'] = pred_index\n",
    "            evaluate_row.append(drow)\n",
    "            quest_index += 1\n",
    "        doc_sents = document['sentences']\n",
    "        for r in evaluate_row:\n",
    "            r['document_ID'] = ddi\n",
    "            r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "            if len(r['prediction_ID']) != 0:\n",
    "                r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "            else:\n",
    "                print 'error prediction',ddi,r['question_ID'],r['prediction_ID']\n",
    "            writer.writerow(r)\n",
    "        ddi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occured What does ITU-R stand for?\n",
      "error occured What does D-VHS stand for?\n",
      "error occured How is The Bahre-Nagassi translated?\n",
      "error occured Who was the runner up?\n",
      "error occured Who was the runner up?\n",
      "error occured Where did it open?\n",
      "error occured What did the actof of milno do?\n",
      "error occured What is an Etsudiantinas? \n",
      "error occured What is crosspicking?\n",
      "error occured When was Chanakya alive?\n",
      "error occured What is q-glass?\n",
      "error occured What hardens glass-ceramics?\n"
     ]
    }
   ],
   "source": [
    "#build model for each document collaction\n",
    "for document in dataset:\n",
    "    document_collections = document['sentences']\n",
    "    document['model'] = tf_idf_Model(document_collections)\n",
    "write_csv('tf_idf.csv',dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error prediction 2 65 []\n",
      "error prediction 2 192 []\n",
      "error prediction 11 783 []\n",
      "error prediction 18 182 []\n",
      "error prediction 25 245 []\n",
      "error prediction 36 89 []\n",
      "error prediction 36 92 []\n"
     ]
    }
   ],
   "source": [
    "for document in dataset:\n",
    "    document_collections_sents = document['sentences']\n",
    "    document['model'] = BM25_Model(document_collections_sents)\n",
    "write_csv('test.csv',dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for document in dataset:\n",
    "    document_collections_sents = document['sentences']\n",
    "    document['model'] = LM_Model(document_collections_sents)\n",
    "write_csv('lm.csv',dataset,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Prdictions from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_csv_vales(filename):\n",
    "    preds = []\n",
    "    with open(filename) as csvfile:\n",
    "        readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            tempDict = {}\n",
    "            tempDict['DocID'] = int(row['document_ID'])\n",
    "            tempDict['Predictions'] = ast.literal_eval(row['prediction_ID'])\n",
    "            tempDict['QuestionIndex'] = int(row['question_ID'])\n",
    "            tempDict['Question'] = row['question']\n",
    "            preds.append(tempDict)\n",
    "    count = 0\n",
    "    bingo = 0\n",
    "    for d in preds:\n",
    "        guess = d['Predictions'] \n",
    "        doc_i = d['DocID']\n",
    "        qus_i = d['QuestionIndex']\n",
    "        act_i = dataset[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "        if act_i in guess:\n",
    "            bingo += 1\n",
    "        count += 1\n",
    "    print \"correctness results :\",float(bingo)/float(count)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness results : 0.567765567766\n"
     ]
    }
   ],
   "source": [
    "tf_idf = get_csv_vales('tf_idf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness results : 0.618574973414\n"
     ]
    }
   ],
   "source": [
    "lm = get_csv_vales('lm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness results : 0.625192012289\n"
     ]
    }
   ],
   "source": [
    "bm25 = get_csv_vales('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add surrending for top one results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ADDsurreding(g):\n",
    "    n = []\n",
    "    for i in g:\n",
    "        if i != 0:\n",
    "            n.append(i+1)\n",
    "            n.append(i-1)\n",
    "        else:\n",
    "            n.append(i+1)\n",
    "    return g+n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after add surrending: 0.697388632873\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "bingo = 0\n",
    "for d in bm25:\n",
    "    guess = d['Predictions'] \n",
    "    doc_i = d['DocID']\n",
    "    qus_i = d['QuestionIndex']\n",
    "    new_guess = ADDsurreding(guess)\n",
    "    act_i = dataset[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "    if act_i in new_guess:\n",
    "        bingo += 1\n",
    "    count += 1\n",
    "print \"results after add surrending:\",float(bingo)/float(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_load_byteified(file_handle):\n",
    "    return _byteify(json.load(file_handle, object_hook=_byteify),ignore_dicts=True)\n",
    "\n",
    "def _byteify(data, ignore_dicts = False):\n",
    "    # if this is a unicode string, return its string representation\n",
    "    if isinstance(data, unicode):\n",
    "        return data.encode('utf-8')\n",
    "    # if this is a list of values, return list of byteified values\n",
    "    if isinstance(data, list):\n",
    "        return [ _byteify(item, ignore_dicts=True) for item in data ]\n",
    "    # if this is a dictionary, return dictionary of byteified keys and values\n",
    "    # but only if we haven't already byteified it\n",
    "    if isinstance(data, dict) and not ignore_dicts:\n",
    "        return {\n",
    "            _byteify(key, ignore_dicts=True): _byteify(value, ignore_dicts=True)\n",
    "            for key, value in data.iteritems()\n",
    "        }\n",
    "    # if it's anything else, return it in its original form\n",
    "    return data\n",
    "\n",
    "with open(\"data/QA_test.json\") as json_file:\n",
    "    json_data = json_load_byteified(json_file)\n",
    "print 'import success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "st = StanfordNERTagger(cwd+'\\data\\english.all.3class.distsim.crf.ser.gz',cwd+'\\data\\stanford-ner.jar')\n",
    "\n",
    "if not os.path.isfile(\"NERtest.json\"):    \n",
    "    start = time.time()\n",
    "    progressT = len(json_data)    \n",
    "    listOfDocument=[]\n",
    "    i=0\n",
    "    for jd in json_data:\n",
    "        aList=[]        \n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))\n",
    "        #remove the below file if running on test set\n",
    "        #aList.extend([st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['answer']))) for x in jd['qa']])])\n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "        listOfDocument.append(aList)\n",
    "        i+=1\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"%d%%\" % (i*100/progressT))\n",
    "        sys.stdout.flush()    \n",
    "    for document in range(0,len(listOfDocument)):\n",
    "        #change [2] to [1] if test set\n",
    "        for sentence in range(0,len(listOfDocument[document][1])):\n",
    "            for word in range(0,len(listOfDocument[document][1][sentence])):   \n",
    "                listOfDocument[document][1][sentence][word]= (listOfDocument[document][1][sentence][word][0],listOfDocument[document][1][sentence][word][1] if not listOfDocument[document][1][sentence][word][0].isdigit() else u'NUMBER')\n",
    "    with open('NERtest.json', 'w') as outfile:\n",
    "        json.dump(listOfDocument, outfile)\n",
    "    end = time.time()\n",
    "    print '\\nTime spending:',end - start    \n",
    "else:    \n",
    "    print 'there is a file'\n",
    "with open(\"NERtest.json\") as json_file:\n",
    "        json_dataNER = json_load_byteified(json_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
