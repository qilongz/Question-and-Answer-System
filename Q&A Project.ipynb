{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import FreqDist\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from numpy import random\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords as sw\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self, loadsaved=True):\n",
    "        self.classifier = None\n",
    "        # if loadsaved:\n",
    "        #     try:\n",
    "        #         fs = open('sentimentAPI.model', mode='rb')\n",
    "        #         ss = fs.readlines()\n",
    "        #         self.classifier = pickle.loads(ss)\n",
    "        #         return\n",
    "        #     except Exception:\n",
    "        #         print('Failed')\n",
    "        #         pass\n",
    "        self.dataset = None\n",
    "        self.stopwords = sw.words('english')\n",
    "        self.dset_positive = []\n",
    "        self.dset_negative = []\n",
    "        self.dset_neutral = []\n",
    "        self.train_X, self.train_y = None, None\n",
    "        self.dev_X,  self.dev_y = None, None\n",
    "        self.test_X, self.test_y = None, None\n",
    "\n",
    "        # self.tokenizer_tw = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "        self.word_freq_vectorizer = DictVectorizer()\n",
    "\n",
    "        self.read_dataset()\n",
    "        self.process_make_matrix()\n",
    "        self.classifier = self.build_model_and_evaluate(RandomForestClassifier())\n",
    "\n",
    "    def read_dataset(self, filename_ls=('data/QA_train.json',)):\n",
    "        self.dataset = []\n",
    "        for ff in filename_ls:\n",
    "            file_strs = open(ff).readline()\n",
    "            self.dataset += json.loads(file_strs)\n",
    "\n",
    "\n",
    "    def tokenize_process(self, ls, tag=None):\n",
    "        new_lls = []\n",
    "        for lt in ls:\n",
    "            tokens = self.tokenizer_tw.tokenize(lt[1])\n",
    "            tokens = [i for i in tokens if not re.search(r'http://', i) and i not in self.stopwords]\n",
    "    \n",
    "            bag_of_word_dict = FreqDist(tokens)\n",
    "            new_lls.append((bag_of_word_dict, lt[0]))\n",
    "            assert lt[0] == tag\n",
    "        return new_lls\n",
    "    \n",
    "    def segmentation(self, data):\n",
    "        # print data[0], 'AA'\n",
    "        random.shuffle(data)\n",
    "        # print data[0], 'BB'\n",
    "        dlen = len(data)\n",
    "        train, dev, test = data[:int(0.8 * dlen)], data[int(0.8 * dlen): int(0.9 * dlen)], data[int(0.9 * dlen):]\n",
    "        return train, dev, test\n",
    "\n",
    "    def process_make_matrix(self):\n",
    "        self.dset_positive = self.tokenize_process(self.dset_positive, 'positive')\n",
    "        self.dset_negative = self.tokenize_process(self.dset_negative, 'negative')\n",
    "        self.dset_neutral = self.tokenize_process(self.dset_neutral, 'neutral')\n",
    "        \n",
    "        ptrain, pdev, ptest = self.segmentation(self.dset_positive)\n",
    "        ntrain, ndev, ntest = self.segmentation(self.dset_negative)\n",
    "        xtrain, xdev, xtest = self.segmentation(self.dset_neutral)\n",
    "\n",
    "        train_raw = ptrain + ntrain + xtrain\n",
    "        random.shuffle(train_raw)\n",
    "        train_x_raw = [x[0] for x in train_raw]\n",
    "        self.train_X = self.word_freq_vectorizer.fit_transform(train_x_raw)\n",
    "        self.train_y = [x[1] for x in train_raw]\n",
    "        \n",
    "        dev_raw = pdev + ndev + xdev\n",
    "        random.shuffle(dev_raw)\n",
    "        dev_x_raw = [x[0] for x in dev_raw]\n",
    "        self.dev_X = self.word_freq_vectorizer.transform(dev_x_raw)\n",
    "        self.dev_y = [x[1] for x in dev_raw]\n",
    "        \n",
    "        test_raw = ptest + ntest + xtest\n",
    "        random.shuffle(test_raw)\n",
    "        test_x_raw = [x[0] for x in test_raw]\n",
    "        self.test_X = self.word_freq_vectorizer.transform(test_x_raw)\n",
    "        self.test_y = [x[1] for x in test_raw]\n",
    "\n",
    "    def build_model_and_evaluate(self, clf, report=True):\n",
    "        # training\n",
    "        clf.fit(self.train_X, self.train_y)\n",
    "        # test\n",
    "        pred = clf.predict(self.test_X)\n",
    "        # score\n",
    "        accuracy = metrics.accuracy_score(self.test_y, pred)\n",
    "        if report:\n",
    "            print('-' * 100)\n",
    "            print('classifier:')\n",
    "            print(clf)\n",
    "\n",
    "            print(\"macro f1 score:   %0.3f\" % metrics.f1_score(self.test_y, pred, average='macro'))\n",
    "            print(\"accuracy:   %0.3f\" % accuracy, '\\n\\n')\n",
    "            print(metrics.classification_report(self.test_y, pred))\n",
    "            print()\n",
    "            print(metrics.confusion_matrix)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # save model:\n",
    "        # ss = pickle.dumps(clf)\n",
    "        # fs = open('sentimentAPI.model', mode='wb')\n",
    "        # fs.write(ss)\n",
    "        # fs.close()\n",
    "\n",
    "        return clf\n",
    "\n",
    "    def sentiment_analysis(self, tweet_text):\n",
    "        pred = None\n",
    "        if isinstance(tweet_text, str):\n",
    "            tokens = self.tokenizer_tw.tokenize(tweet_text)\n",
    "            tokens = [i for i in tokens if not re.search(r'http://', i) and i not in self.stopwords]\n",
    "            bag_of_word_dict = FreqDist(tokens)\n",
    "            tweet_vector = self.word_freq_vectorizer.transform(bag_of_word_dict)\n",
    "            pred = self.classifier.predict(tweet_vector)\n",
    "        elif isinstance(tweet_text, (list, tuple)):\n",
    "            pred = []\n",
    "            for tt in tweet_text:\n",
    "                tokens = self.tokenizer_tw.tokenize(tt)\n",
    "                tokens = [i for i in tokens if not re.search(r'http://', i) and i not in self.stopwords]\n",
    "                bag_of_word_dict = FreqDist(tokens)\n",
    "                tweet_vector = self.word_freq_vectorizer.transform(bag_of_word_dict)\n",
    "                pred += self.classifier.predict(tweet_vector)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
