{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math,numpy,json,re,nltk,csv,time,re,os.path,sys,gensim,ast\n",
    "from gensim import corpora\n",
    "from operator import add\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "#from sklearn import metrics\n",
    "#from math import sqrt\n",
    "#from collections import OrderedDict\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "#from nltk import FreqDist, DictionaryProbDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful \n",
      "There are totally 40 documents in this dataset\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "filename_ls = ['QA_dev.json']\n",
    "dataset = []\n",
    "train_path = os.path.abspath('data/QA_dev.json')\n",
    "\n",
    "dataset = []\n",
    "with open(train_path) as f:\n",
    "    for line in f:\n",
    "        dataset+=(json.loads(line))\n",
    "print \"Import Successful \"\n",
    "print \"There are totally\", len(dataset),'documents in this dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build modle and evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return list(terms)\n",
    "\n",
    "\n",
    "class MostRelevantSentenceModel:\n",
    "    def __init__(self, vectorizer, collection_matrix):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.collection_matrix = collection_matrix\n",
    "        feature_array = vectorizer.get_feature_names()\n",
    "        self.features = dict()\n",
    "        for index in range(len(feature_array)):\n",
    "            term = feature_array[index]\n",
    "            self.features[term] = index\n",
    "\n",
    "    def predictTop10(self, queies):\n",
    "        predictions = [self.inverted_index_score(i) for i in  queies]\n",
    "        return predictions\n",
    "\n",
    "    def inverted_index_score(self, query_sent):\n",
    "        \"\"\"\n",
    "        now we implement inverted index to handle query\n",
    "        \n",
    "        :param query_sent: \n",
    "        :return: \n",
    "        \n",
    "        \"\"\"\n",
    "        query_words = my_tokenizer(query_sent)\n",
    "        score = defaultdict(float)\n",
    "\n",
    "        for w in query_words:\n",
    "            try:\n",
    "                col_i = self.features[w]\n",
    "                inverted_ix = self.collection_matrix[:, col_i]\n",
    "                for di in range(inverted_ix.shape[0]):\n",
    "                    score[di] += inverted_ix[di, 0]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        index_score = sorted(score.items(), key=lambda (k, v): v, reverse=True)\n",
    "\n",
    "        if index_score:\n",
    "            top10_doc_index = [i[0] for i in index_score[:10]]\n",
    "            return top10_doc_index\n",
    "        else:\n",
    "            print 'error occured' ,query_sent\n",
    "            return -1, 0\n",
    "        \n",
    "#build model for each document collaction\n",
    "for document in dataset:\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, use_idf=True,norm='l1',stop_words=None, tokenizer=my_tokenizer)\n",
    "    document_collections_sents = document['sentences']\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(document_collections_sents)\n",
    "    document['model'] = MostRelevantSentenceModel(vectorizer=tfidf_vectorizer,collection_matrix=tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output with predtion and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occured Who was the runner up?\n",
      "error occured Who was the runner up?\n",
      "error occured Where did it open?\n",
      "error occured What did the actof of milno do?\n",
      "error occured What is an Etsudiantinas? \n",
      "error occured What is crosspicking?\n"
     ]
    }
   ],
   "source": [
    "def build_model_and_evaluate(model, query ,report=False):\n",
    "    evaluate_row = []\n",
    "    pred = model.predictTop10(query)\n",
    "    quest_index = 0\n",
    "    for pred_index in pred:\n",
    "        drow = dict()\n",
    "        if report:\n",
    "            print pred_index\n",
    "        drow['question_ID'] = quest_index\n",
    "        drow['prediction_ID'] = pred_index\n",
    "        evaluate_row.append(drow)\n",
    "        quest_index += 1\n",
    "    return evaluate_row\n",
    "\n",
    "csv_file = open('evaluatin_dev_results.csv', mode='w',)\n",
    "fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "writer = csv.DictWriter(csv_file, fieldnames=fieldnames, )\n",
    "writer.writeheader()\n",
    "\n",
    "ddi = 0\n",
    "for document in dataset:\n",
    "    questions = [i['question'] for i in document['qa']]\n",
    "    model = document['model']\n",
    "    result_row = build_model_and_evaluate(model, questions)\n",
    "    doc_sents = document['sentences']\n",
    "    for r in result_row:\n",
    "        r['document_ID'] = ddi\n",
    "        r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "        r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "        writer.writerow(r)\n",
    "    ddi += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, fn_docs, delimiter='|') :\n",
    "        self.dictionary = corpora.Dictionary()\n",
    "        self.DF = {}\n",
    "        self.delimiter = delimiter\n",
    "        self.DocTF = []\n",
    "        self.DocIDF = {}\n",
    "        self.N = 0\n",
    "        self.DocAvgLen = 0\n",
    "        self.fn_docs = fn_docs\n",
    "        self.DocLen = []\n",
    "        self.buildDictionary()\n",
    "        self.TFIDF_Generator()\n",
    "\n",
    "    def buildDictionary(self) :\n",
    "        raw_data = []\n",
    "        for line in self.fn_docs:\n",
    "            raw_data.append(line.strip().split(self.delimiter))\n",
    "        self.dictionary.add_documents(raw_data)\n",
    "\n",
    "    def TFIDF_Generator(self, base=math.e) :\n",
    "        docTotalLen = 0\n",
    "        for line in self.fn_docs:\n",
    "            doc = line.strip().split(self.delimiter)\n",
    "            docTotalLen += len(doc)\n",
    "            self.DocLen.append(len(doc))\n",
    "            bow = dict([(term, freq*1.0/len(doc)) for term, freq in self.dictionary.doc2bow(doc)])\n",
    "            for term, tf in bow.items() :\n",
    "                if term not in self.DF :\n",
    "                    self.DF[term] = 0\n",
    "                self.DF[term] += 1\n",
    "            self.DocTF.append(bow)\n",
    "            self.N = self.N + 1\n",
    "        for term in self.DF:\n",
    "            self.DocIDF[term] = math.log((self.N - self.DF[term] +0.5) / (self.DF[term] + 0.5), base)\n",
    "        self.DocAvgLen = docTotalLen / self.N\n",
    "\n",
    "    def BM25Score(self, Query=[], k1=1.5, b=0.75, k3 =0) :\n",
    "        query_bow = self.dictionary.doc2bow(Query)\n",
    "        query_freq =dict(query_bow)\n",
    "        scores = []\n",
    "        for idx, doc in enumerate(self.DocTF) :\n",
    "            commonTerms = set(dict(query_bow).keys()) & set(doc.keys())\n",
    "            tmp_score = []\n",
    "            doc_terms_len = self.DocLen[idx]\n",
    "            for term in commonTerms :\n",
    "                upper = (doc[term] * (k1+1))\n",
    "                below = ((doc[term]) + k1*(1 - b + b*doc_terms_len/self.DocAvgLen))\n",
    "                frq_q_t = query_freq[term]\n",
    "                tmp_score.append(self.DocIDF[term] * upper / below * (k3 +1)*frq_q_t/(k3+frq_q_t))\n",
    "            scores.append(sum(tmp_score))\n",
    "        sorted_scores =sorted(list(enumerate(scores)), key=lambda (k,v): v, reverse=True)\n",
    "        return  sorted_scores\n",
    "\n",
    "    def TFIDF(self) :\n",
    "        tfidf = []\n",
    "        for doc in self.DocTF :\n",
    "            doc_tfidf  = [(term, tf*self.DocIDF[term]) for term, tf in doc.items()]\n",
    "            doc_tfidf.sort()\n",
    "            tfidf.append(doc_tfidf)\n",
    "        return tfidf\n",
    "\n",
    "    def Items(self) :\n",
    "        # Return a list [(term_idx, term_desc),]\n",
    "        items = self.dictionary.items()\n",
    "        items.sort()\n",
    "        return items\n",
    "    \n",
    "    def predictTop10(self,query,k1=1.5, b=0.75, k3 =0):\n",
    "        query = query.split()\n",
    "        socres = self.BM25Score(query,k1,b,k3)\n",
    "        if socres:\n",
    "            socres = [i[0] for i in socres[:10]]\n",
    "            return socres\n",
    "        else:\n",
    "            print 'error occured' ,query\n",
    "            return -1, 0\n",
    "for document in dataset:\n",
    "    document_collections_sents = document['sentences']\n",
    "    document['model'] = BM25(document_collections_sents, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time counts for running models : 203.21600008\n"
     ]
    }
   ],
   "source": [
    "def model_evaluation_query(model, questions ,report=False):\n",
    "    evaluate_row = []\n",
    "    quest_index = 0\n",
    "    for index,query in enumerate(questions):\n",
    "        preditons = model.predictTop10(query)\n",
    "        drow = dict()\n",
    "        if report:\n",
    "            print preditons\n",
    "        drow['question_ID'] = quest_index\n",
    "        drow['prediction_ID'] = preditons\n",
    "        evaluate_row.append(drow)\n",
    "        quest_index += 1\n",
    "    return evaluate_row\n",
    "csv_file = open('BM25_dev_results.csv', mode='w',)\n",
    "fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "\n",
    "ddi = 0\n",
    "for document in dataset:\n",
    "    questions = [i['question'] for i in document['qa']]\n",
    "    model = document['model']\n",
    "    result_row = model_evaluation_query(model,questions)\n",
    "    doc_sents = document['sentences']\n",
    "    for r in result_row:\n",
    "        r['document_ID'] = ddi\n",
    "        r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "        r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "        writer.writerow(r)\n",
    "    ddi += 1\n",
    "    \n",
    "print 'Time counts for running models :', time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Prdictions from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_file = 'evaluatin_dev_results.csv'\n",
    "TF_IDF_preds = []\n",
    "with open(csv_file) as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        tempDict = {}\n",
    "        tempDict['DocID'] = int(row['document_ID'])\n",
    "        tempDict['Predictions'] = ast.literal_eval(row['prediction_ID'])\n",
    "        tempDict['QuestionIndex'] = int(row['question_ID'])\n",
    "        tempDict['Question'] = row['question']\n",
    "        TF_IDF_preds.append(tempDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = 'BM25_dev_results.csv'\n",
    "sentence_BM25_predictons = []\n",
    "BM25_preds = []\n",
    "with open(csv_file) as csvfile:\n",
    "    readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        tempDict = {}\n",
    "        tempDict['DocID'] = int(row['document_ID'])\n",
    "        tempDict['Predictions'] = ast.literal_eval(row['prediction_ID'])\n",
    "        tempDict['QuestionIndex'] = int(row['question_ID'])\n",
    "        tempDict['Question'] = row['question']\n",
    "        BM25_preds.append(tempDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparsion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816613494033\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "bingo = 0\n",
    "for d in TF_IDF_preds:\n",
    "    guess = d['Predictions'] \n",
    "    doc_i = d['DocID']\n",
    "    qus_i = d['QuestionIndex']\n",
    "    act_i = dataset[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "    if act_i in guess:\n",
    "        bingo += 1\n",
    "    count += 1\n",
    "print float(bingo)/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.600520340587\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "bingo = 0\n",
    "for d in BM25_preds:\n",
    "    guess = d['Predictions'] \n",
    "    doc_i = d['DocID']\n",
    "    qus_i = d['QuestionIndex']\n",
    "    act_i = dataset[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "    if act_i in guess:\n",
    "        bingo += 1\n",
    "    count += 1\n",
    "print float(bingo)/float(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_load_byteified(file_handle):\n",
    "    return _byteify(json.load(file_handle, object_hook=_byteify),ignore_dicts=True)\n",
    "\n",
    "def _byteify(data, ignore_dicts = False):\n",
    "    # if this is a unicode string, return its string representation\n",
    "    if isinstance(data, unicode):\n",
    "        return data.encode('utf-8')\n",
    "    # if this is a list of values, return list of byteified values\n",
    "    if isinstance(data, list):\n",
    "        return [ _byteify(item, ignore_dicts=True) for item in data ]\n",
    "    # if this is a dictionary, return dictionary of byteified keys and values\n",
    "    # but only if we haven't already byteified it\n",
    "    if isinstance(data, dict) and not ignore_dicts:\n",
    "        return {\n",
    "            _byteify(key, ignore_dicts=True): _byteify(value, ignore_dicts=True)\n",
    "            for key, value in data.iteritems()\n",
    "        }\n",
    "    # if it's anything else, return it in its original form\n",
    "    return data\n",
    "\n",
    "with open(\"data/QA_test.json\") as json_file:\n",
    "    json_data = json_load_byteified(json_file)\n",
    "print 'import success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "st = StanfordNERTagger(cwd+'\\data\\english.all.3class.distsim.crf.ser.gz',cwd+'\\data\\stanford-ner.jar')\n",
    "\n",
    "if not os.path.isfile(\"NERtest.json\"):    \n",
    "    start = time.time()\n",
    "    progressT = len(json_data)    \n",
    "    listOfDocument=[]\n",
    "    i=0\n",
    "    for jd in json_data:\n",
    "        aList=[]        \n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))\n",
    "        #remove the below file if running on test set\n",
    "        #aList.extend([st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['answer']))) for x in jd['qa']])])\n",
    "        aList.append(st.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "        listOfDocument.append(aList)\n",
    "        i+=1\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"%d%%\" % (i*100/progressT))\n",
    "        sys.stdout.flush()    \n",
    "    for document in range(0,len(listOfDocument)):\n",
    "        #change [2] to [1] if test set\n",
    "        for sentence in range(0,len(listOfDocument[document][1])):\n",
    "            for word in range(0,len(listOfDocument[document][1][sentence])):   \n",
    "                listOfDocument[document][1][sentence][word]= (listOfDocument[document][1][sentence][word][0],listOfDocument[document][1][sentence][word][1] if not listOfDocument[document][1][sentence][word][0].isdigit() else u'NUMBER')\n",
    "    with open('NERtest.json', 'w') as outfile:\n",
    "        json.dump(listOfDocument, outfile)\n",
    "    end = time.time()\n",
    "    print '\\nTime spending:',end - start    \n",
    "else:    \n",
    "    print 'there is a file'\n",
    "with open(\"NERtest.json\") as json_file:\n",
    "        json_dataNER = json_load_byteified(json_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
