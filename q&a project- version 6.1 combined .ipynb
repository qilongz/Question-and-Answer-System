{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math,numpy,json,re,nltk,csv,time,re,os.path,sys,ast,itertools,string\n",
    "from operator import add\n",
    "from math import sqrt\n",
    "from numpy import multiply\n",
    "from nltk import FreqDist, DictionaryProbDist\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as WNL\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict,OrderedDict\n",
    "from nltk import RegexpParser\n",
    "\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_load_byteified(file_handle):\n",
    "    return _byteify(json.load(file_handle, object_hook=_byteify),ignore_dicts=True)\n",
    "\n",
    "def _byteify(data, ignore_dicts = False):\n",
    "    # if this is a unicode string, return its string representation\n",
    "    if isinstance(data, unicode):\n",
    "        return data.encode('UTF-8')\n",
    "    # if this is a list of values, return list of byteified values\n",
    "    if isinstance(data, list):\n",
    "        return [ _byteify(item, ignore_dicts=True) for item in data ]\n",
    "    # if this is a dictionary, return dictionary of byteified keys and values\n",
    "    # but only if we haven't already byteified it\n",
    "    if isinstance(data, dict) and not ignore_dicts:\n",
    "        return {\n",
    "            _byteify(key, ignore_dicts=True): _byteify(value, ignore_dicts=True)\n",
    "            for key, value in data.iteritems()\n",
    "        }\n",
    "    # if it's anything else, return it in its original form\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful \n",
      "There are totally 40 documents in this dev dataset\n",
      "There are totally 42 documents in this test dataset\n",
      "There are totally 360 documents in this  train dataset\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_path = os.path.abspath('data/QA_test.json')\n",
    "dev_path = os.path.abspath('data/QA_dev.json')\n",
    "train_path = os.path.abspath('data/QA_train.json')\n",
    "\n",
    "\n",
    "#dataset_dev\n",
    "with open(dev_path) as f:\n",
    "    dataset_dev  = json_load_byteified(f)\n",
    "\n",
    "#dataset_test \n",
    "with open(test_path) as f:\n",
    "    dataset_test = json_load_byteified(f)\n",
    "\n",
    "#dataset_train \n",
    "with open(train_path) as f:\n",
    "    dataset_train = json_load_byteified(f)\n",
    "print \"Import Successful \"\n",
    "print \"There are totally\", len(dataset_dev),'documents in this dev dataset'\n",
    "print \"There are totally\", len(dataset_test),'documents in this test dataset'\n",
    "print \"There are totally\", len(dataset_train),'documents in this  train dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-0f54ce579e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-2ab29f8c63c5>\u001b[0m in \u001b[0;36mmy_tokenizer\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+(?:[-./]\\w+)?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 'in' and 'not in' operations are much faster over sets that lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "a = dataset_dev[0]['sentences']\n",
    "for i in a:\n",
    "    my_tokenizer(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build tf-idf model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    terms = set()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+(?:[-./]\\w+)?')\n",
    "    for token in tokenizer.tokenize(doc):\n",
    "        tok = token.encode('utf-8')\n",
    "        if tok not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(tok.lower()))\n",
    "    return list(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tf_idf_Model:\n",
    "    def __init__(self, collection):\n",
    "        self.vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, use_idf=True,norm='l2',stop_words=None, tokenizer=my_tokenizer)\n",
    "        self.collection_matrix = self.vectorizer.fit_transform(document_collections)\n",
    "        feature_array = self.vectorizer.get_feature_names()\n",
    "        self.features = dict()\n",
    "        for index in range(len(feature_array)):\n",
    "            term = feature_array[index]\n",
    "            self.features[term] = index\n",
    "\n",
    "    def predict(self, queryX,limit=3):\n",
    "        predictions = [self.inverted_index_score(i,limit) for i in  queryX]\n",
    "        return predictions\n",
    "\n",
    "    def inverted_index_score(self, query_sent,limit=3):\n",
    "        \"\"\"\n",
    "        now we implement inverted index to handle query\n",
    "        \n",
    "        :param query_sent: \n",
    "        :return: \n",
    "        \n",
    "        \"\"\"\n",
    "        query_words = my_tokenizer(query_sent)\n",
    "        score = defaultdict(float)\n",
    "\n",
    "        for w in query_words:\n",
    "            try:\n",
    "                col_i = self.features[w]\n",
    "                inverted_ix = self.collection_matrix[:, col_i]\n",
    "                for doc_i in range(inverted_ix.shape[0]):\n",
    "                    score[doc_i] += inverted_ix[doc_i, 0]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        index_score = sorted(score.items(), key=lambda (k, v): v, reverse=True)\n",
    "\n",
    "        if index_score:\n",
    "            doc_indexs = [i[0] for i in index_score[:limit]]\n",
    "            return doc_indexs\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25_Model(object): \n",
    "    def __init__(self, document_collection, k1=1.5, b=0.75, k3=0.0, EPS=0.25, tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.document_collection_length = len(document_collection) \n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length \n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_freq = FreqDist(self.document_corpus)\n",
    "        self.tf = [] \n",
    "        self.df = defaultdict(int) \n",
    "        self.bm25_idf = defaultdict(float) \n",
    "        self.average_idf = -1 \n",
    "        self.k1 = k1 \n",
    "        self.k3 = k3 \n",
    "        self.EPSILON = EPS \n",
    "        self.b = b \n",
    "        self.inverted_index = defaultdict(list) \n",
    "        self.initialize() \n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection): \n",
    "            doc_term_freq = FreqDist(document) \n",
    "            self.tf.append(doc_term_freq) \n",
    "            \n",
    "            for word, freq in doc_term_freq.items(): \n",
    "                self.df[word] += 1 \n",
    "                self.inverted_index[word].append(index) \n",
    "        for word, freq in self.df.items(): \n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5) \n",
    "            \n",
    "            self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys()) \n",
    "    \n",
    "    def predict(self, queryX, limit=1): \n",
    "        q_prediction = [] \n",
    "        for query in queryX: \n",
    "            answers = self.bm25_get_most_relevant(query)[:limit] \n",
    "            if len(answers) == 0:\n",
    "                q_prediction.append([]) \n",
    "            else:\n",
    "                q_prediction.append([i[0] for i in answers]) \n",
    "        return q_prediction \n",
    "\n",
    "    def bm25_get_most_relevant(self, query): \n",
    "        query_tks = self.tokenizer(query) \n",
    "        scores = defaultdict(float) \n",
    "        for q_token in query_tks: \n",
    "            for doc_index in self.inverted_index[q_token]: \n",
    "                idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf \n",
    "                top = self.tf[doc_index][q_token] * (self.k1 + 1) \n",
    "                below = self.tf[doc_index][q_token] + self.k1 * (1 - self.b + self.b * self.document_collection_length / self.avg_doc_length) \n",
    "                frq_q_t = self.corpus_freq[q_token]\n",
    "                scores[doc_index] += idf * top / below *(self.k3 +1)*frq_q_t/(self.k3+frq_q_t)\n",
    "        prels = scores.items() \n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True) \n",
    "        return sorted_socres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LM_Model:\n",
    "    def __init__(self,documents,a = 0.5 ,tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer  \n",
    "        self.alpha = a\n",
    "        self.document_collection = [self.tokenizer(doc) for doc in documents] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_term_prob = {}\n",
    "        self.corpus_term_freq = FreqDist(self.document_corpus)\n",
    "        self.vocabulary = self.corpus_term_freq.keys()\n",
    "        self.lmp = []\n",
    "        self.initialize()\n",
    "    def initialize(self): \n",
    "        document_freq = [FreqDist(doc) for doc in self.document_collection]\n",
    "        length_corpus = len(self.document_corpus)\n",
    "        for term,occurs in self.corpus_term_freq.items():\n",
    "            self.corpus_term_prob[term] = float(occurs)/float(length_corpus)\n",
    "        for sent_freq in document_freq:\n",
    "            tempDict = {}\n",
    "            for term in self.vocabulary:\n",
    "                upper = sent_freq.get(term,0) + self.alpha*self.corpus_term_prob.get(term,0)\n",
    "                below = self.corpus_term_freq.get(term,0) + self.alpha\n",
    "                tempDict[term] = float(upper)/float(below)\n",
    "            self.lmp.append(tempDict)\n",
    "    def get_lm_socres(self,Query):\n",
    "        doc_socres = []\n",
    "        Query = my_tokenizer(Query)\n",
    "        for doc_prob in  self.lmp:\n",
    "            term_score = []\n",
    "            for term in Query:\n",
    "                if term in self.vocabulary:\n",
    "                    term_score.append(doc_prob[term])\n",
    "            query_score = numpy.product(term_score)\n",
    "            doc_socres.append(query_score)\n",
    "        sorted_score = sorted(list(enumerate(doc_socres)), key=lambda (k,v): v, reverse=True)\n",
    "        doc_indexs = [i for i in sorted_score]\n",
    "        return doc_indexs\n",
    "    def predict(self,questions,limit = 3):\n",
    "        predictions = [] \n",
    "        for query in questions: \n",
    "            answers = self.get_lm_socres(query)[:limit] \n",
    "            predictions.append([i[0] for i in answers]) \n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Various Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_csv(csv_name,model_name,dataset,limit=1):\n",
    "    csv_file = open(csv_name, mode='w',)\n",
    "    fieldnames = ['document_ID', 'question_ID','question','prediction_ID','prediction_sentence']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, )\n",
    "    writer.writeheader()\n",
    "\n",
    "    ddi = 0\n",
    "    for document in dataset:\n",
    "        evaluate_row = []\n",
    "        questions = [i['question'] for i in document['qa']]\n",
    "        model = document[model_name]\n",
    "        predictions = model.predict(questions,limit)\n",
    "        quest_index = 0\n",
    "        for pred_index in predictions:\n",
    "            drow = dict()\n",
    "            drow['question_ID'] = quest_index\n",
    "            drow['prediction_ID'] = pred_index\n",
    "            evaluate_row.append(drow)\n",
    "            quest_index += 1\n",
    "        doc_sents = document['sentences']\n",
    "        for r in evaluate_row:\n",
    "            r['document_ID'] = ddi\n",
    "            r['question'] = questions[r['question_ID']].encode('utf-8')\n",
    "            if len(r['prediction_ID']) != 0:\n",
    "                r['prediction_sentence'] = doc_sents[r['prediction_ID'][0]].encode('utf-8')\n",
    "            else:\n",
    "                print 'error prediction',ddi,r['question_ID'],r['question']\n",
    "            writer.writerow(r)\n",
    "        ddi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error prediction 11 317 Who was the runner up?\n",
      "error prediction 11 520 Who was the runner up?\n",
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 22 94 What is an Etsudiantinas? \n",
      "error prediction 25 245 When was Chanakya alive?\n",
      "----------------------------------------------------\n",
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 25 245 When was Chanakya alive?\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#build model for each document collaction\n",
    "for document in dataset_dev:\n",
    "    document_collections = document['sentences']\n",
    "    document['tf_idf_model'] = tf_idf_Model(document_collections)\n",
    "    document['bm25_model'] = BM25_Model(document_collections)\n",
    "    document['lm_model'] = LM_Model(document_collections)\n",
    "print 'The Following are errors made by different models,doucnment Index,Query Index, Query'\n",
    "write_csv('data/tf_idf_dev_predictions.csv','tf_idf_model',dataset_dev,1)\n",
    "print '----------------------------------------------------'\n",
    "write_csv('data/bm25_dev_predictions.csv','bm25_model',dataset_dev,1)\n",
    "print '----------------------------------------------------'\n",
    "write_csv('data/lm_dev_predictions.csv','lm_model',dataset_dev,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import different result and compare predictions from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_prediction_accuracy(filename,devdata = False):\n",
    "    preds = []\n",
    "    with open(filename) as csvfile:\n",
    "        readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            tempDict = {}\n",
    "            tempDict['DocID'] = int(row['document_ID'])\n",
    "            tempDict['Predictions'] = ast.literal_eval(row['prediction_ID'])\n",
    "            tempDict['QuestionIndex'] = int(row['question_ID'])\n",
    "            tempDict['Question'] = row['question']\n",
    "            preds.append(tempDict)\n",
    "    count = 0\n",
    "    bingo = 0\n",
    "    for d in preds:\n",
    "        guess = d['Predictions'] \n",
    "        doc_i = d['DocID']\n",
    "        qus_i = d['QuestionIndex']\n",
    "        act_i = dataset_dev[doc_i]['qa'][qus_i]['answer_sentence']\n",
    "        if act_i in guess:\n",
    "            bingo += 1\n",
    "        count += 1\n",
    "    print \"Model correctness results :\",float(bingo)/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model correctness results : 0.589152782701\n",
      "Model correctness results : 0.651305683564\n",
      "Model correctness results : 0.643270707787\n"
     ]
    }
   ],
   "source": [
    "check_prediction_accuracy('data/tf_idf_dev_predictions.csv')\n",
    "check_prediction_accuracy('data/bm25_dev_predictions.csv')\n",
    "check_prediction_accuracy('data/lm_dev_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose BM25  Model and predict sentence  for test set\n",
    "* in this case BM25 will be used for sentence retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build models for test data set\n",
    "for document in dataset_test:\n",
    "    document_collections = document['sentences']\n",
    "    document['bm25_model'] = tf_idf_Model(document_collections)\n",
    "# write to a CSV file for test data predictions\n",
    "write_csv('data/bm25_dev_predictions.csv','bm25_model',dataset_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time is  3073.45918512\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time() - t0\n",
    "print 'Running time is ',t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-74283195b055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mNERtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/data/english.all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/data/stanford-ner.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/QA_test.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "path_to_jar = 'data/stanford-parser.jar'\n",
    "path_to_models_jar = 'data/stanford-parser-3.7.0-models.jar'\n",
    "NERtagger = StanfordNERTagger(cwd+'/data/english.all.3class.distsim.crf.ser.gz',cwd+'/data/stanford-ner.jar')\n",
    "\n",
    "with open(\"data/QA_test.json\") as json_file:\n",
    "    json_data_test = json_load_byteified(json_file)\n",
    "print 'Test dataset import success'\n",
    "\n",
    "if not os.path.isfile(\"NERtest.json\"):    \n",
    "    start = time.time()\n",
    "    progressT = len(json_data_test)    \n",
    "    listOfDocument=[]\n",
    "    i=0\n",
    "    for jd in json_data_test:\n",
    "        aList=[]        \n",
    "        aList.append(NERtagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x['question']))) for x in jd['qa']]))\n",
    "        #remove the below file if running on test set\n",
    "        aList.append(NERtagger.tag_sents([word_tokenize(re.sub(',', '',re.sub('[^a-zA-Z0-9-_*., ]', ' ',x))) for x in jd['sentences']]))\n",
    "        listOfDocument.append(aList)\n",
    "        i+=1\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"%d%%\" % (i*100/progressT))\n",
    "        sys.stdout.flush()    \n",
    "    for document in range(0,len(listOfDocument)):\n",
    "        #change [2] to [1] if test set\n",
    "        for sentence in range(0,len(listOfDocument[document][1])):\n",
    "            for word in range(0,len(listOfDocument[document][1][sentence])):   \n",
    "                listOfDocument[document][1][sentence][word]= \\\n",
    "                (listOfDocument[document][1][sentence][word][0],\\\n",
    "                 listOfDocument[document][1][sentence][word][1] \\\n",
    "                 if not listOfDocument[document][1][sentence][word][0].isdigit() else u'NUMBER')\n",
    "    with open('NERtest.json', 'w') as outfile:\n",
    "        json.dump(listOfDocument, outfile)\n",
    "    end = time.time()\n",
    "    print '\\nTime spending:',end - start    \n",
    "else:    \n",
    "    print 'NER file is alrady exist'\n",
    "with open(\"NERtest.json\") as json_file:\n",
    "        json_dataNER = json_load_byteified(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Answer rank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Ranking Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class answer_rank():\n",
    "    def __init__(self,json_data,json_dataOrg,json_dataPOS,json_dataPOSOrg):\n",
    "        self.json_data = json_data\n",
    "        self.json_dataOrg = json_dataOrg\n",
    "        self.json_dataPOS = json_dataPOS\n",
    "        self.json_dataPOSOrg = json_dataPOSOrg\n",
    "        self.dictDoc = {}\n",
    "        self.dictDocOrg = {}\n",
    "        self.initialize()\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "\n",
    "        for document in range(len(self.json_data)):\n",
    "            for thing in range(len(self.json_data[document])):\n",
    "                for sentence in range(len(self.json_data[document][thing])):\n",
    "                    for word in range(len(self.json_data[document][thing][sentence])):\n",
    "                        self.json_data[document][thing][sentence][word][1] = 'U'\\\n",
    "                        if word!=0 and self.json_data[document][thing][sentence][word][0][0].isupper()\\\n",
    "                        and self.json_data[document][thing][sentence][word][1]=='O'\\\n",
    "                        else self.json_data[document][thing][sentence][word][1]\n",
    "                        \n",
    "                        self.json_data[document][thing][sentence][word][0] = self.json_data[document][thing][sentence][word][0].lower()\n",
    "        \n",
    "                        if self.json_data[document][thing][sentence][word][0] \\\n",
    "                        in ['one','two','three','four','five','six','seven','eight','nine','ten','zero']\\\n",
    "                        or self.isfloat(self.json_data[document][thing][sentence][word][0]):\n",
    "                            self.json_data[document][thing][sentence][word][1] = 'NUMBER'\n",
    "        for document in range(len(self.json_dataPOS)):\n",
    "            for thing in range(len(self.json_dataPOS[document])):\n",
    "                for sentence in range(len(self.json_dataPOS[document][thing])):\n",
    "                    for word in range(len(self.json_dataPOS[document][thing][sentence])):\n",
    "                        self.json_dataPOS[document][thing][sentence][word][0] = self.json_dataPOS[document][thing][sentence][word][0].lower()\n",
    "                        if self.have_number(self.json_dataPOS[document][thing][sentence][word][0]):\n",
    "                            self.json_dataPOS[document][thing][sentence][word][1] = 'CD'\n",
    "        print 'NER json file import successful'\n",
    "        \n",
    "    \n",
    "    def have_number(self,s):\n",
    "        return any(i.isdigit() for i in s)\n",
    "\n",
    "    def isfloat(self,value):\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def readCSV(self,filename,devdata = False):\n",
    "        csv_file = filename\n",
    "        i=0\n",
    "        answerSecondFilter = []\n",
    "        dictDocOrg={}\n",
    "        t_i = 1\n",
    "        if devdata:\n",
    "            t_i = 2            \n",
    "        with open(csv_file, 'rb') as csvfile:\n",
    "            readCSV = csv.DictReader(csvfile, delimiter=',')\n",
    "            for row in readCSV:\n",
    "                document_i = int(row['document_ID'])\n",
    "                question_i = int(row['question_ID'])\n",
    "                filteredlistOfNERSentence = []\n",
    "                question_type = []\n",
    "                predictionList = ast.literal_eval(row['prediction_ID'])\n",
    "                prediction_index = predictionList[0] if len(predictionList)!=0 else 1\n",
    "                question_type = self.detectQuestion(document_i,question_i,prediction_index,t_i)         \n",
    "                self.dictDoc[document_i,question_i]=prediction_index,question_type                                              \n",
    "\n",
    "\n",
    "\n",
    "    def detectQuestion(self,i, j, k, l):\n",
    "        openclassword=[]\n",
    "        kindOfAnswer = []\n",
    "        questionPOS = self.json_dataPOS[i][0][j]\n",
    "        questionNER = self.json_data[i][0][j]        \n",
    "        answerPOS = self.json_dataPOS[i][l][k]\n",
    "        answerNER = self.json_data[i][l][k]\n",
    "        specialcommand=[]\n",
    "        #print questionNER, answerPOS\n",
    "        originalWithout = [x[0] for x in questionPOS]\n",
    "        originalWithoutA = [x[0] for x in answerPOS]\n",
    "        getIndexOfWH = [y for y,x in enumerate(questionPOS) if 'W' in x[1]]\n",
    "        #if wh\n",
    "        if len(getIndexOfWH) != 0:\n",
    "            getPOS = ''.join([x[1][0] for x in questionPOS])        \n",
    "\n",
    "            #not at last word\n",
    "            if getIndexOfWH[0]+1!=len(questionPOS):            \n",
    "                searchWordAfterWh1 = re.search('W(.*?)V', getPOS, re.IGNORECASE)\n",
    "                if searchWordAfterWh1:\n",
    "                    if len(searchWordAfterWh1.group(1))!=0:\n",
    "                        openclassword.append([searchWordAfterWh1.start()+len(searchWordAfterWh1.group(1))])                    \n",
    "                    else:\n",
    "                        openclassword.append([])\n",
    "                else:\n",
    "                    searchWordAfterWh1 = re.search('W(.*?)', getPOS, re.IGNORECASE)\n",
    "                    if searchWordAfterWh1:\n",
    "                        openclassword.append([searchWordAfterWh1.start()+len(searchWordAfterWh1.group(0))-1])\n",
    "                    else:\n",
    "                        openclassword.append([])\n",
    "                frontPart = range(0,searchWordAfterWh1.start()) if searchWordAfterWh1 else range(0,len(getPOS)/2)\n",
    "                backPart = range(searchWordAfterWh1.start()+len(searchWordAfterWh1.group(0)),len(getPOS)) if searchWordAfterWh1 else range(len(getPOS)/2,len(getPOS))                      \n",
    "                openclassword.append(frontPart+backPart)      \n",
    "                #print openclassword\n",
    "            else:\n",
    "                openclassword=[[],[y for y,x in enumerate(questionPOS)][:-1]]                        \n",
    "            #remove Stopwords        \n",
    "            openclassword[1] = [x for x in openclassword[1] if questionPOS[x][0] not in stopwords]\n",
    "            if len(openclassword[0])!=0:            \n",
    "                numberIndicator = ['year','length','percentage', 'many','much']            \n",
    "                for x in numberIndicator: \n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['NUMBER'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                            specialcommand.append(x)\n",
    "                personIndicator = ['name']\n",
    "                for x in personIndicator:\n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['PERSON','ORGANIZATION'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                placeIndicator = ['location','place','country','city','area']\n",
    "                for x in placeIndicator:\n",
    "                    for y in originalWithout[getIndexOfWH[0]:openclassword[0][0]+2]:\n",
    "                        if x in y:\n",
    "                            kindOfAnswer = ['LOCATION'] if len(kindOfAnswer)==0 else kindOfAnswer\n",
    "                            specialcommand=['location']\n",
    "                if len(kindOfAnswer)==0:    \n",
    "                    kindOfAnswer = ['O','U']\n",
    "            else:\n",
    "                numberIndicator = ['when']            \n",
    "                personIndicator = ['who','whom','whose','name']\n",
    "                placeIndicator = ['where','located']\n",
    "                if originalWithout[getIndexOfWH[0]] in numberIndicator:\n",
    "                    kindOfAnswer = ['NUMBER']\n",
    "                    specialcommand = ['year']\n",
    "                elif originalWithout[getIndexOfWH[0]] in personIndicator:\n",
    "                    kindOfAnswer = ['PERSON','ORGANIZATION']\n",
    "                elif originalWithout[getIndexOfWH[0]] in placeIndicator:\n",
    "                    kindOfAnswer = ['LOCATION']\n",
    "                    specialcommand=['location']\n",
    "                else:\n",
    "                    if len([y for y in openclassword[1] if questionPOS[y][0] in personIndicator])!=0:\n",
    "                        kindOfAnswer = ['PERSON','ORGANIZATION']\n",
    "                    elif len([y for y in openclassword[1] if questionPOS[y][0] in placeIndicator])!=0:\n",
    "                        kindOfAnswer = ['LOCATION']\n",
    "                        specialcommand=['location']\n",
    "                    else: \n",
    "                        kindOfAnswer = ['O','U']\n",
    "        else:\n",
    "            openclassword = [[],[x for x,y in enumerate(originalWithout) if y not in stopwords]]\n",
    "            kindOfAnswer = ['O','U']\n",
    "        #determine whether it requires number entity\n",
    "        newList1=[]\n",
    "        newList2=[]\n",
    "        for x in range(len(openclassword[0])):\n",
    "            if originalWithout[openclassword[0][x]] in originalWithoutA:            \n",
    "                newList1.extend([f for f,h in enumerate(originalWithoutA) if originalWithout[openclassword[0][x]] == h])        \n",
    "        for x in range(len(openclassword[1])):\n",
    "            if originalWithout[openclassword[1][x]] in originalWithoutA:\n",
    "                newList2.extend([f for f,h in enumerate(originalWithoutA) if originalWithout[openclassword[1][x]] == h])\n",
    "        openclassword[0]=newList1\n",
    "        openclassword[1]=newList2\n",
    "        return openclassword, kindOfAnswer,specialcommand     \n",
    "\n",
    "    def createNP(self,answerToReturn,answerPOS,specialcommand,q,j):\n",
    "        newAnswer = answerPOS[answerToReturn[1]][0]\n",
    "        if (newAnswer.isdigit() and 'year' not in specialcommand):\n",
    "            newAnswer = \"{:,}\".format(int(answerPOS[answerToReturn[1]][0]))\n",
    "        for i in range(answerToReturn[1],0,-1):\n",
    "            if answerToReturn[1] != 0:\n",
    "                if answerPOS[i][1] =='NNP':\n",
    "                    if 'NNP' in answerPOS[i-1][1] or (('DT' in answerPOS[i-1][1] or 'IN' in answerPOS[i-1][1]) \\\n",
    "                                                      and answerPOS[i-1][0] !='at' \\\n",
    "                                                      and 'location' in specialcommand):\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NN':\n",
    "                    if 'JJ' in answerPOS[i-1][1] or 'DT' in answerPOS[i-1][1] \\\n",
    "                    or 'CD' in answerPOS[i-1][1] or answerPOS[i-1][1] =='NN':\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='JJ':\n",
    "                    if answerPOS[i-1][1] =='JJ':\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NNS':\n",
    "                    if 'JJ' in answerPOS[i-1][1]:\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                elif 'DT' in answerPOS[i][1] and 'location' not in specialcommand:\n",
    "                    if 'TO' in answerPOS[i-1][1]:\n",
    "                        newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if answerPOS[i][1] !='CD' and answerPOS[i][1] !='RB':\n",
    "                        if 'NN' in answerPOS[i-1][1] or 'JJ' in answerPOS[i-1][1] or 'RB' in answerPOS[i-1][1]:\n",
    "                            newAnswer = answerPOS[i-1][0]+ \" \" +newAnswer\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        for i in range(answerToReturn[1],len(answerPOS)-1):\n",
    "            if answerToReturn[1] != len(answerPOS)-1:\n",
    "                if answerPOS[i][1] =='NNP':                \n",
    "                    if 'NNP' in answerPOS[i+1][1] or 'CC' in answerPOS[i+1][1] or 'IN' in answerPOS[i+1][1] or 'TO' in answerPOS[i+1][1] or ('NN' in answerPOS[i+1][1] and 'location' in specialcommand):                                        \n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='CC':\n",
    "                    if 'NNP' in answerPOS[i+1][1] or 'NN' == answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='CD':\n",
    "                    if 'CD' in answerPOS[i+1][1]:                 \n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:                    \n",
    "                        break\n",
    "                elif answerPOS[i][1] =='TO' or answerPOS[i][1] =='DT':\n",
    "                    if 'NN' in answerPOS[i+1][1] or 'RB' in answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break \n",
    "                elif answerPOS[i][1] =='IN' and 'location' not in specialcommand:\n",
    "                    if answerPOS[i+1][1] =='DT' or answerPOS[i+1][1] =='NNP' or answerPOS[i+1][1] =='TO' or answerPOS[i+1][1] =='CD':\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='JJ':\n",
    "                    if 'NNS' in answerPOS[i+1][1] or 'NN' == answerPOS[i+1][1] or answerPOS[i+1][1] =='JJ':\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NN':\n",
    "                    if 'NN' in answerPOS[i+1][1] or 'JJ' in answerPOS[i+1][1] or 'IN' in answerPOS[i+1][1] or 'CC' in answerPOS[i+1][1]:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                elif answerPOS[i][1] =='NNS':\n",
    "                    if 'IN' in answerPOS[i+1][1] and 'ORG' in specialcommand:\n",
    "                        newAnswer = newAnswer+' '+answerPOS[i+1][0]\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        buffernewAnswer1 = newAnswer.split()\n",
    "        if buffernewAnswer1[-1]=='along' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-6]\n",
    "        if buffernewAnswer1[-1]=='and' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if buffernewAnswer1[-1]=='for' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if buffernewAnswer1[-1]=='but' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-4]\n",
    "        if buffernewAnswer1[-1]=='in' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='as' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='at' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='on' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='to' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-3]\n",
    "        if buffernewAnswer1[-1]=='while' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-6]\n",
    "        if buffernewAnswer1[-1]=='despite' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-8]\n",
    "        if buffernewAnswer1[-1]=='because' and len(buffernewAnswer1)>1:\n",
    "            newAnswer=newAnswer[:-8]\n",
    "        if buffernewAnswer1[-1].isdigit() and len(buffernewAnswer1[-1])==4 and 'year' in specialcommand:\n",
    "            newAnswer=buffernewAnswer1[-1]\n",
    "        if buffernewAnswer1[-1].isdigit() and 'location' in specialcommand:\n",
    "            newAnswer=buffernewAnswer1[-1]\n",
    "        if len(specialcommand)!=0 and 'percentage' in specialcommand[0]:\n",
    "            newAnswer+='%'\n",
    "        if 'location' in specialcommand:\n",
    "            bufferNewAnswer=[]        \n",
    "            for x in newAnswer.split():\n",
    "                for y in self.json_dataOrg[q]['sentences'][j].split():\n",
    "                    if x in y:\n",
    "                        bufferNewAnswer.append(y)\n",
    "                        break\n",
    "            newAnswer=' '.join(bufferNewAnswer)\n",
    "            newAnswer = newAnswer[:-1] if (len(newAnswer)>1 and not (newAnswer[-1].isalnum() or newAnswer[-1]=='%')) else newAnswer\n",
    "        newAnswer = newAnswer.replace(',','-COMMA-')\n",
    "        return newAnswer   \n",
    "\n",
    "    \n",
    "    def returnAnswer(self,i,j,l):\n",
    "        answerListPOS = self.json_dataPOS[i][l][self.dictDoc[i,j][0]]    \n",
    "        answerListNER = self.json_data[i][l][self.dictDoc[i,j][0]]    \n",
    "        question_type = self.dictDoc[i,j][1][1]\n",
    "        #print answerListPOS,question_type\n",
    "        question_typeLocationInAnswer = [x for x,y in enumerate(answerListNER) \\\n",
    "                                         if ('CD' in answerListPOS[answerListNER.index(y)][1] \\\n",
    "                                             or 'NN' in answerListPOS[answerListNER.index(y)][1]) \\\n",
    "                                         and y[1] in question_type and y[0] not in stopwords \\\n",
    "                                         and x not in self.dictDoc[i,j][1][0][0] \\\n",
    "                                         and x not in self.dictDoc[i,j][1][0][1]]\n",
    "        #print question_typeLocationInAnswer\n",
    "        if len(question_typeLocationInAnswer) ==0:\n",
    "            question_typeLocationInAnswer = [x for x,y in enumerate(answerListNER) \\\n",
    "                                             if ('CD' in answerListPOS[answerListNER.index(y)][1] \\\n",
    "                                                 or 'NN' in answerListPOS[answerListNER.index(y)][1]) \\\n",
    "                                             and y[0] not in stopwords and x not in self.dictDoc[i,j][1][0][0] \\\n",
    "                                             and x not in self.dictDoc[i,j][1][0][1]]\n",
    "        scoreList = 0\n",
    "        if len(self.dictDoc[i,j][1][0][0])!=0:\n",
    "            maxScore = sys.maxint        \n",
    "            for x in question_typeLocationInAnswer:\n",
    "                score = sum([math.fabs(z-x) for z in self.dictDoc[i,j][1][0][0]])\n",
    "                if maxScore>score:\n",
    "                    maxScore=score\n",
    "                    scoreList = x\n",
    "        else:\n",
    "            maxScore = sys.maxint\n",
    "            scoreList = 0\n",
    "            for x in question_typeLocationInAnswer:\n",
    "                score = sum([math.fabs(z-x) for z in self.dictDoc[i,j][1][0][1]])\n",
    "                if maxScore>score:\n",
    "                    maxScore=score\n",
    "                    scoreList = x\n",
    "                    \n",
    "        answerToReturn = (self.json_dataPOSOrg[i][l][self.dictDoc[i,j][0]][scoreList][0],scoreList)\n",
    "        answerPOS = self.json_dataPOSOrg[i][l][self.dictDoc[i,j][0]]\n",
    "        specialcommand = self.dictDoc[i,j][1][2]\n",
    "        document_index = i\n",
    "        question_index = self.dictDoc[i,j][0]\n",
    "        \n",
    "        answer = self.createNP(answerToReturn,answerPOS,specialcommand,document_index,question_index)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    def writeToFile(self,filename,devdata = False):\n",
    "        t_i = 1\n",
    "        if devdata:\n",
    "            t_i = 2\n",
    "        with open(filename, mode='wb',) as csv_file:\n",
    "            if devdata :\n",
    "                fieldnames = ['document_id','question_id','answer_predict',\"answer_actual\",'tag','sentence','predict','question_type','question']\n",
    "            else:\n",
    "                fieldnames = ['id','answer']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames,delimiter=',')\n",
    "            writer.writeheader()\n",
    "            k = 0\n",
    "            doc_size=len(self.json_data)\n",
    "            for i in range(0, doc_size):\n",
    "                for j in range(0,len(self.json_data[i][0])):\n",
    "                    k+=1            \n",
    "                    dictToCSV={}\n",
    "                    if devdata:\n",
    "                        dictToCSV['document_id'] = i\n",
    "                        dictToCSV['question_id'] = j\n",
    "                        try:\n",
    "                            dictToCSV['answer_predict']= self.returnAnswer(i,j,t_i)\n",
    "                        except:\n",
    "                            print '\\nErrors on dev set return answers',i,j,devdata\n",
    "                        dictToCSV['answer_actual'] = self.json_dataOrg[i]['qa'][j]['answer']\n",
    "                        dictToCSV['tag'] =self.dictDoc[i,j]\n",
    "                        dictToCSV['question_type'] = self.dictDoc[i,j][1][1]\n",
    "                        dictToCSV['question'] = self.json_dataOrg[i]['qa'][j]['question']\n",
    "                        dictToCSV['sentence'] = self.json_dataOrg[i]['qa'][j]['answer_sentence']\n",
    "                        dictToCSV['predict'] = self.dictDoc[i,j][0]\n",
    "                    else:\n",
    "                        dictToCSV={}\n",
    "                        dictToCSV['id'] = k\n",
    "                        try:\n",
    "                            dictToCSV['answer'] = self.returnAnswer(i,j,t_i)\n",
    "                        except:\n",
    "                            print '\\nError on test return answers',i,j,t_i\n",
    "                    writer.writerow(dictToCSV)    \n",
    "                    csv_file.flush()\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write(str(k))\n",
    "                sys.stdout.flush()\n",
    "        csv_file.close()\n",
    "        print '\\nsuccess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detemineWhich(aList):\n",
    "    for x in aList:\n",
    "        if x[1]!='O':\n",
    "            return x[1]\n",
    "        else:\n",
    "            if len([d for d in x[0] if d.isdigit()])!=0 :\n",
    "                return 'NUMBER'\n",
    "    return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70159\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "with open('data/NERtrain.json') as json_file:\n",
    "    json_data_train = json_load_byteified(json_file)\n",
    "with open('data/QA_train.json') as json_file:\n",
    "    json_dataOrg_train = json_load_byteified(json_file)\n",
    "with open('train_question_tag.csv', mode='wb',) as csv_file:\n",
    "    fieldnames = ['question_type','question']\n",
    "\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames,delimiter=',')\n",
    "    writer.writeheader()\n",
    "    k = 0\n",
    "    doc_size=len(json_data_train)\n",
    "    for i in range(0, doc_size):\n",
    "        for j in range(0,len(json_data_train[i][1])):\n",
    "            k+=1            \n",
    "            dictToCSV={}\n",
    "            dictToCSV['question'] = json_dataOrg_train[i]['qa'][j]['question']\n",
    "            dictToCSV['question_type'] = detemineWhich(json_data_train[i][1][j])\n",
    "            writer.writerow(dictToCSV)    \n",
    "            csv_file.flush()\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(str(k))\n",
    "        sys.stdout.flush()\n",
    "    csv_file.close()\n",
    "    print '\\nsuccess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/NERdev.json') as json_file:\n",
    "    json_data = json_load_byteified(json_file)    \n",
    "with open('data/POSdev.json') as json_file:\n",
    "    json_dataPOS = json_load_byteified(json_file)\n",
    "with open('data/QA_dev.json') as json_file:\n",
    "    json_dataOrg = json_load_byteified(json_file)\n",
    "with open('data/POSdev.json') as json_file:\n",
    "    json_dataPOSOrg = json_load_byteified(json_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/NERtest.json') as json_file:\n",
    "    json_data_test = json_load_byteified(json_file)    \n",
    "with open('data/POStest.json') as json_file:\n",
    "    json_dataPOS_test = json_load_byteified(json_file)\n",
    "with open('data/QA_test.json') as json_file:\n",
    "    json_dataOrg_test = json_load_byteified(json_file)\n",
    "with open('data/POStest.json') as json_file:\n",
    "    json_dataPOSOrg_test = json_load_byteified(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER json file import successful\n",
      "8463\n",
      "success\n",
      "NER json file import successful\n",
      "8974\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "qa_dev = answer_rank(json_data,json_dataOrg,json_dataPOS,json_dataPOSOrg)\n",
    "qa_dev.readCSV(\"data/bm25_dev_predictions.csv\",True)\n",
    "qa_dev.writeToFile(\"data/bm25_dev_result.csv\",True)\n",
    "\n",
    "qa_test = answer_rank(json_data_test,json_dataOrg_test,json_dataPOS_test,json_dataPOSOrg_test)\n",
    "qa_test.readCSV(\"data/bm25_test_predictions.csv\",devdata = False)\n",
    "qa_test.writeToFile(\"data/bm25_test_result.csv\",devdata =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time() - t0\n",
    "print 'Running time is ',t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 1)** Sentence Retrival:<br >\n",
    "> **Errors Found**: Errors on prediciting sentences.so there are few sentence that do not have any predictions <br>\n",
    "\n",
    "> **Reasons**: from tf-idf model there are 6 sentences which is can not produce any relevent sentences to them, after tokenzie those query.For example 'who was then runner up' , the query will left token 'runner' in this query. but the document corpus dost exit. therefore base on current algorithm,  tf-idf model wont predict any sentencs secore back<br> \n",
    "However, there are antoher 3 sentences also occur errors prediciton.in this case the query 'What is an Etsudiantinas?' after tokeniz and select word from corpus. the result is zero , beacuse the only common word exit is term 'what', base on this word the word will also give zero results <br>\n",
    "\n",
    ">**Imporovement**: there are two possible solutions when those query can not extract relevant sentences. first one is change model to language model. after we smooth langurage model. the wont occur zero probabbblity to predict sentences. there fore there will least some sentences will be predict. second solutions is to use semantic ways. in term of semantic, this allow models to choose different similarity word to re-score sentences, simple means when if there words in query dosent exist in corpus we substitute the key words to re-score it.\n",
    "    \n",
    "**Task 2)** Entity Retrival: <br >\n",
    "> **Errors Found**: NER provide a entity extraction rules to parse entity from sentenes which can not extract right entity or assign an wrong entity <br>\n",
    "\n",
    ">**Reasons:** In this project, NER will provide 5 types of entity 'PERSON','NUMBER','ORGANZIATON','LOCATION','OTHER'many of the query. There are alot of date numbers for example 4000 NER will automatic consider as other rather than number. Some really unusall terms cant not be classify by NER. For Example , the term phrase 'Emilio Aguinaldo and Apolinario Mabini' is extreamlly unsuall too seen as \"PERSON\",but this term is actually belong to 'PERSON'. in term '0.914' the punctuation will also affect the entity extractions,moreover if the term collaps between entiries, a example of 'Christian Dior' seems should be 'PERSON' , but it will allocate to 'ORGANZIATION'.\n",
    "\n",
    "> **Improvemnt** Some entity can be process by rule based. coutiounsly implemanting by human design rule to preprocessing the rules and filter out  from entity 'OTHER', but it comes ineffiency and time consuming for human filter out entity<br>\n",
    "\n",
    "**Task 3)** Answer Ranking: <br >\n",
    "> **Errors Found** on reuturning answers the errors occurs when for the focus extraction,ususally rule base ia hard to produce the right part of answer sentene even if right found <br>\n",
    "\n",
    ">**Resons** Afterwards sentence retrival, base on the rule design, for example if three 'PERSON' tags were found in answer sentences, rule will only filter out the one closet to part based on position distance\n",
    "\n",
    "> **Improvement ** : Compares the rule<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancement for basic Q&A systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Infrared', 'JJ'], ['thermal-imaging', 'NN'], ['cameras', 'NNS'], ['are', 'VBP'], ['used', 'VBN'], ['to', 'TO'], ['detect', 'VB'], ['heat', 'NN'], ['loss', 'NN'], ['in', 'IN'], ['insulated', 'VBN'], ['systems', 'NNS'], ['to', 'TO'], ['observe', 'VB'], ['changing', 'VBG'], ['blood', 'NN'], ['flow', 'NN'], ['in', 'IN'], ['the', 'DT'], ['skin', 'NN'], ['and', 'CC'], ['to', 'TO'], ['detect', 'VB'], ['overheating', 'NN'], ['of', 'IN'], ['electrical', 'JJ'], ['apparatus', 'NN'], ['.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Define your custom tagged data. \n",
    "t = json_dataPOSOrg[0][2][3]\n",
    "print t\n",
    "tags = [(i[0],i[1]) for i in t]\n",
    "\n",
    "\n",
    "# Define your custom grammar (modified to be a valid regex).\n",
    "grammar = \"\"\" CHUNK: <W.*>{<N.*>+}<V.*>\"\"\"\n",
    "\n",
    "# Create an instance of your custom parser.\n",
    "custom_tag_parser = RegexpParser(grammar)\n",
    "\n",
    "# Parse!\n",
    "tree =  custom_tag_parser.parse(tags)\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == 'CHUNK': \n",
    "        print( subtree[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkTop1(doc_i,q_i,top3):\n",
    "    right_i = top3[0] \n",
    "    for index in top3:\n",
    "        t = json_dataPOSOrg[doc_i][0][q_i]\n",
    "        tags = [(i[0],i[1]) for i in t]\n",
    "        tree = custom_tag_parser.parse(tags)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'CHUNK': \n",
    "                focus =  subtree[0][0]\n",
    "                if checkExist(doc_i,index,focus):\n",
    "                    right_i = index\n",
    "                    break\n",
    "    return right_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkExist(doc_i,sent_i,focus):\n",
    "    sents = [x[0] for x in json_dataPOSOrg[doc_i][2][sent_i]]\n",
    "    if focus in sents and focus != 'year':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Superviesd Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Enhancement - Re-Build BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemma_words(word):\n",
    "    lemmas = []\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            lemmas.append(lemma.name().lower())\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25_Semantic_Model(object): \n",
    "    def __init__(self, document_collection, k1=1.5, b=0.75, k3=0.0, EPS=0.25, tokenizer=my_tokenizer): \n",
    "        self.tokenizer = tokenizer \n",
    "        self.document_collection_length = len(document_collection) \n",
    "        self.avg_doc_length = sum(map(lambda x: float(len(x)), document_collection)) / self.document_collection_length \n",
    "        self.document_collection = [self.tokenizer(doc) for doc in document_collection] \n",
    "        self.document_corpus = list(itertools.chain.from_iterable(self.document_collection))\n",
    "        self.corpus_freq = FreqDist(self.document_corpus)\n",
    "        self.tf = [] \n",
    "        self.df = defaultdict(int) \n",
    "        self.bm25_idf = defaultdict(float) \n",
    "        self.average_idf = -1 \n",
    "        self.k1 = k1 \n",
    "        self.k3 = k3 \n",
    "        self.EPSILON = EPS \n",
    "        self.b = b \n",
    "        self.inverted_index = defaultdict(list) \n",
    "        self.initialize() \n",
    "    def initialize(self):\n",
    "        for index, document in enumerate(self.document_collection): \n",
    "            doc_term_freq = FreqDist(document) \n",
    "            self.tf.append(doc_term_freq) \n",
    "            for word, freq in doc_term_freq.items(): \n",
    "                self.df[word] += 1 \n",
    "                self.inverted_index[word].append(index) \n",
    "        for word, freq in self.df.items(): \n",
    "            self.bm25_idf[word] = math.log(self.document_collection_length - freq + 0.5) - math.log(freq + 0.5) \n",
    "            \n",
    "            self.average_idf = sum(map(lambda k: float(self.bm25_idf[k]), self.bm25_idf.keys())) / len(self.bm25_idf.keys()) \n",
    "    \n",
    "    def predict(self, queryX, limit=1): \n",
    "        q_prediction = [] \n",
    "        for query in queryX: \n",
    "            answers = self.bm25_get_most_relevant(query)[:limit] \n",
    "            if len(answers) == 0:\n",
    "                q_prediction.append([]) \n",
    "            else:\n",
    "                q_prediction.append([i[0] for i in answers]) \n",
    "        return q_prediction \n",
    "\n",
    "    def bm25_get_most_relevant(self, query): \n",
    "        query_tks = self.tokenizer(query) \n",
    "        scores = defaultdict(float)\n",
    "        new_query_tks = []\n",
    "        for i in query_tks:\n",
    "            if i not in self.inverted_index.keys():\n",
    "                new_query_tks.extend(lemma_words(i))\n",
    "            else:\n",
    "                new_query_tks.append(i)\n",
    "        new_query_tks = list(set(new_query_tks))\n",
    "        for q_token in new_query_tks: \n",
    "            for doc_index in self.inverted_index[q_token]: \n",
    "                idf = self.bm25_idf[q_token] if self.bm25_idf[q_token] >= 0 else self.EPSILON * self.average_idf \n",
    "                top = self.tf[doc_index][q_token] * (self.k1 + 1) \n",
    "                below = self.tf[doc_index][q_token] + self.k1 * (1 - self.b + self.b * self.document_collection_length / self.avg_doc_length) \n",
    "                frq_q_t = self.corpus_freq[q_token]\n",
    "                scores[doc_index] += idf * top / below *(self.k3 +1)*frq_q_t/(self.k3+frq_q_t)\n",
    "        prels = scores.items() \n",
    "        sorted_socres = sorted(prels, key=lambda (k, v): v, reverse=True) \n",
    "        return sorted_socres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error prediction 11 783 Where did it open?\n",
      "error prediction 18 182 What did the actof of milno do?\n",
      "error prediction 25 245 When was Chanakya alive?\n",
      "Model correctness results : 0.645988420182\n"
     ]
    }
   ],
   "source": [
    "for document in dataset_dev:\n",
    "    document_collections_sents = document['sentences']\n",
    "    document['lemma_model'] = BM25_Semantic_Model(document_collections_sents)\n",
    "write_csv('data/lemma_dev_predictions.csv','lemma_model',dataset_dev,1)\n",
    "check_prediction_accuracy('data/lemma_dev_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = dataset_dev[0]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
